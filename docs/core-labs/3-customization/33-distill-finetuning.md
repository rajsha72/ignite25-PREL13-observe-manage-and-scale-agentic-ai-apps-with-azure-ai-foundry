# Distillation Fine-tuning

Learn to use model distillation to create cost-efficient models that maintain quality while reducing operational costs.

## Description

Explore model distillation by using a larger, more capable model's outputs to train a smaller, faster, and more cost-effective model.

## Learning Objectives

- Understand model distillation concepts
- Generate training data from larger models
- Fine-tune smaller models with distilled knowledge
- Compare performance and cost tradeoffs

## Instructions

**üìì Notebook:** [`labs/3-customization/33-distill-finetuning.ipynb`](https://github.com/microsoft/ignite25-PDY123-learn-how-to-observe-manage-and-scale-agentic-ai-apps-using-azure/blob/main/labs/3-customization/33-distill-finetuning.ipynb)

## Copilot Prompts

```
Explain how model distillation works and when to use it
```

```
Show me how to generate training data from GPT-4 outputs for distilling to GPT-3.5
```

```
Help me compare the cost and quality tradeoffs between different model sizes
```

## Related Resources

- üìò [Model Distillation Overview](https://learn.microsoft.com/azure/ai-studio/concepts/model-distillation)

---

[‚Üê Previous: Custom Grader](32-custom-grader.md){ .md-button }
[Next Lab: Evaluation Metrics ‚Üí](../4-evaluation/){ .md-button .md-button--primary }
