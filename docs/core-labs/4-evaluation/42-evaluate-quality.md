# Evaluate Quality

Assess response quality using groundedness, relevance, coherence, and fluency metrics.

## Description

Dive deep into quality evaluation metrics to measure how well your agent provides accurate, relevant, and well-structured responses.

## Learning Objectives

- Measure groundedness (factual accuracy)
- Evaluate relevance to user queries
- Assess coherence and fluency

## Instructions

**üìì Notebook:** [`labs/4-evaluation/42-evaluate-quality.ipynb`](https://github.com/microsoft/ignite25-PDY123-learn-how-to-observe-manage-and-scale-agentic-ai-apps-using-azure/blob/main/labs/4-evaluation/42-evaluate-quality.ipynb)

## Copilot Prompts

```
Explain the difference between groundedness and relevance metrics
```

```
Show me how to measure coherence in AI responses
```

## Related Resources

- üìò [Quality Metrics Documentation](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in#generation-quality-metrics)

---

[‚Üê Previous: First Evaluation](41-first-evaluation.md){ .md-button }
[Next: Evaluate Safety ‚Üí](43-evaluate-safety.md){ .md-button .md-button--primary }
