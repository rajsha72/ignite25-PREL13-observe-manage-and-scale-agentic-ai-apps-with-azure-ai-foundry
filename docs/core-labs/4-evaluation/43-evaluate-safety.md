# Evaluate Safety

Implement content safety evaluations to ensure your agent meets responsible AI standards.

## Description

Learn to evaluate and enforce safety guardrails for hate speech, violence, sexual content, and self-harm using Azure's content safety evaluators.

## Learning Objectives

- Evaluate content safety metrics
- Configure safety thresholds
- Understand responsible AI requirements

## Instructions

**ğŸ““ Notebook:** [`labs/4-evaluation/43-evaluate-safety.ipynb`](https://github.com/microsoft/ignite25-PDY123-learn-how-to-observe-manage-and-scale-agentic-ai-apps-using-azure/blob/main/labs/4-evaluation/43-evaluate-safety.ipynb)

## Copilot Prompts

```
Show me how to evaluate content safety using Azure AI
```

```
Explain the different safety categories (hate, violence, sexual, self-harm)
```

## Related Resources

- ğŸ“˜ [Safety Metrics Documentation](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in#risk-and-safety-metrics)

---

[â† Previous: Evaluate Quality](42-evaluate-quality.md){ .md-button }
[Next: Evaluate Agents â†’](44-evaluate-agents.md){ .md-button .md-button--primary }
