# 4.3 Be Cost-Effective With Distillation

!!! quote "BY THE END OF THIS LAB YOU SHOULD HAVE"

    - [ ] Used evaluation data to identify the best teacher and student models
    - [ ] Created training datasets from high-quality responses
    - [ ] Fine-tuned a smaller model using knowledge distillation
    - [ ] Deployed and evaluated the distilled model against larger models
    - [ ] Analyzed performance improvements in cost and efficiency

---

In this lab, we'll explore model distillation - a technique that transfers knowledge from a large, capable "teacher" model to a smaller, more efficient "student" model. While our fine-tuned model provides the right tone and style, it may be using a large, expensive model for a relatively narrow task. Through distillation, we can achieve comparable accuracy with better efficiency by trading general-purpose capabilities for task-specific intelligence, resulting in lower costs and faster response times.

## Step 1: Check Environment Setup

First, let's ensure our environment is properly configured with the necessary Azure OpenAI credentials.

```python
import os

openai_key = os.getenv("AZURE_OPENAI_API_KEY")
openai_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
model_name = "gpt-4.1"
api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2025-02-01-preview")

if not openai_key or not openai_endpoint:
    print("Error: Missing AZURE_OPENAI_KEY or AZURE_OPENAI_ENDPOINT environment variable.")

print("Using Model:", model_name)
print("Using API Version:", api_version)
```

Next, let's create an Azure OpenAI client and generate a unique tracking ID for this notebook run:

```python
from openai import AzureOpenAI

client = AzureOpenAI(
    azure_endpoint=os.environ.get("AZURE_OPENAI_ENDPOINT"),
    api_key=os.environ.get("AZURE_OPENAI_API_KEY"),
    api_version="2025-04-01-preview",
)

import uuid
UNIQUE_ENOUGH_KEY = str(uuid.uuid4()).split("-")[0]
```

!!! note
    If you encounter any errors, verify that your environment variables are set correctly in your `.env` file.

---

## Step 2: Finding the Teacher and Student Models

### 2.1 Load Evaluation Data

In the previous notebook, we evaluated multiple models against our custom grader. Now we'll load that data to identify which model performs best as our "teacher":

```python
import json

# Load exported data from grader notebook
with open('../data/distillation_export.json', 'r') as f:
    export_data = json.load(f)

# Reconstruct the baseline runs
baseline_eval_id = export_data['baseline_eval_id']
baseline_runs = []

for i, run_id in enumerate(export_data['baseline_runs_ids']):
    run = client.evals.runs.retrieve(eval_id=baseline_eval_id, run_id=run_id)
    run.model = export_data['baseline_runs_models'][i]
    baseline_runs.append(run)

# Get other variables
baseline_eval = client.evals.retrieve(baseline_eval_id)
qa_validation = export_data['qa_validation']
GRADER_MODEL = export_data['GRADER_MODEL']
GRADER_PROMPT = export_data['GRADER_PROMPT'] 
SYSTEM_PROMPT = export_data['SYSTEM_PROMPT']

print(f"‚úÖ Loaded {len(baseline_runs)} baseline runs from grader notebook")
print(f"‚úÖ Loaded {len(qa_validation)} validation Q&A pairs")
```

### 2.2 Select Teacher and Student Models

Next, we'll analyze the performance metrics of each model to determine which should be our "teacher" (the model with the best quality outputs) and which should be our "student" (the smaller model that we'll fine-tune):

```python
# Define relative model sizes (larger index = smaller model)
SIZE_INDEX = {
    "o3": 1,
    "o3-mini": 2,
    "o4-mini": 3,
    "gpt-4.1": 4,
    "gpt-4.1-nano": 6,
    "gpt-4o-mini": 5
}

# Set a quality threshold for "high-quality" responses
CUTOFF = 4.0

# Collect high-quality responses from each model
HIGH_SCORES = {}
for model in SIZE_INDEX:
    HIGH_SCORES[model] = []

# Find all high-scoring outputs across models
for run in baseline_runs:
    if not hasattr(run, 'model') or not run.model:
        continue
        
    model = run.model
    if model not in HIGH_SCORES:
        HIGH_SCORES[model] = []
        
    # Get all scores for this model
    scores = []
    for result in client.evals.runs.list_results(eval_id=baseline_eval_id, run_id=run.id).data:
        if hasattr(result, 'score') and result.score is not None:
            scores.append(result.score)
            # If score is high, save this as a potential training example
            if result.score >= CUTOFF:
                row = result.record
                if 'messages' not in row:
                    row['messages'] = [
                        {"role": "system", "content": SYSTEM_PROMPT},
                        {"role": "user", "content": row['item']['question']},
                        {"role": "assistant", "content": row['item']['answer']}
                    ]
                HIGH_SCORES[model].append(row)
```

We'll calculate metrics to help us select the best models:

```python
# Calculate composite metrics
model_metrics = []
for run in baseline_runs:
    if not hasattr(run, 'model') or not run.model:
        continue
    
    model = run.model
    if model not in SIZE_INDEX:
        continue
        
    scores = []
    for result in client.evals.runs.list_results(eval_id=baseline_eval_id, run_id=run.id).data:
        if hasattr(result, 'score') and result.score is not None:
            scores.append(result.score)
            
    if not scores:
        continue
        
    n = len(scores)
    mean_score = sum(scores) / n
    p90 = sorted(scores)[int(n * 0.9)] if n > 0 else 0
    high_scores = len([s for s in scores if s >= CUTOFF])
    coverage = high_scores / n if n > 0 else 0
    
    # Our composite metric balances average quality with consistency
    composite = (mean_score * 0.5) + (p90 * 0.5) + (coverage * 2)
    
    model_metrics.append({
        'model': model,
        'mean': mean_score,
        'p90': p90,
        'high_scores': high_scores,
        'coverage': coverage,
        'composite': composite,
        'n': n
    })

# Sort by composite score (descending)
model_metrics.sort(key=lambda x: -x['composite'])

# Select the teacher model (highest composite score)
TEACHER_MODEL = model_metrics[0]['model'] if model_metrics else "gpt-4.1"

# Select the student model (smallest with high coverage)
eligible_students = [(m['model'], m['composite']) 
                     for m in model_metrics 
                     if m['model'] != TEACHER_MODEL 
                     and m['model'] in SIZE_INDEX 
                     and SIZE_INDEX[m['model']] > SIZE_INDEX[TEACHER_MODEL] 
                     and m['coverage'] >= 0.4]

if eligible_students:
    eligible_students.sort(key=lambda t: (SIZE_INDEX[t[0]], -t[1]))
    STUDENT_MODEL = eligible_students[-1][0]
else:
    STUDENT_MODEL = "gpt-4.1-nano"  # Default to nano if no good candidates

print(f"\nSelected Teacher: {TEACHER_MODEL}")
print(f"Selected Student: {STUDENT_MODEL} (high samples: {len(HIGH_SCORES[STUDENT_MODEL])})")
```

!!! tip
    The teacher model should have the highest quality outputs, while the student model should be as small as possible while still having reasonable performance.

---

## Step 3: Preparing Training Data

### 3.1 Create Training Datasets

Using the high-quality responses from our teacher model, we'll create training and validation datasets for fine-tuning:

```python
# Create training/validation datasets from teacher model's high-quality responses
training_filename = f"zava-tone-training-{UNIQUE_ENOUGH_KEY}.jsonl"
validation_filename = f"zava-tone-validation-{UNIQUE_ENOUGH_KEY}.jsonl"

# Make an 80/20 split for training/validation
split_at = int(len(HIGH_SCORES[TEACHER_MODEL]) * 0.80)
training_data = HIGH_SCORES[TEACHER_MODEL][:split_at]
validation_data = HIGH_SCORES[TEACHER_MODEL][split_at:]
print(f"Split into {len(training_data)} training / {len(validation_data)} validation rows.")

# Create and upload the training data
with open(training_filename, "w") as f:
    for message in training_data:
        json.dump(message, f)
        f.write("\n")
with open(training_filename, "rb") as f:
    training_file = client.files.create(file=f, purpose="fine-tune")
    training_file = client.files.wait_for_processing(training_file.id)
print(f"üèãÔ∏è‚Äç‚ôÇÔ∏è Created training file:\n{training_file.to_json(indent=2)}")

# Create and upload the validation data
with open(validation_filename, "w") as f:
    for message in validation_data:
        json.dump(message, f)
        f.write("\n")
with open(validation_filename, "rb") as f:
    validation_file = client.files.create(file=f, purpose="fine-tune")
    validation_file = client.files.wait_for_processing(validation_file.id)
print(f"üìã Created validation file:\n{validation_file.to_json(indent=2)}")
```

### 3.2 Start Fine-Tuning the Student Model

Now we'll start the fine-tuning job to distill knowledge from the teacher model to the student model:

```python
# Submit the fine-tuning job
SUFFIX = f"{TEACHER_MODEL}-zava-tone-{UNIQUE_ENOUGH_KEY}".replace(".", "")

# Use optimized hyperparameters for faster convergence
job = client.fine_tuning.jobs.create(
    model=STUDENT_MODEL,
    suffix=SUFFIX,
    training_file=training_file.id,
    validation_file=validation_file.id,
    extra_body={ "trainingType": "globalstandard" },
    hyperparameters={ 
        "learning_rate_multiplier": 2.0,  # Higher learning rate for faster training
        "n_epochs": 3,                    # Reduced epochs (default is often overkill)
        "batch_size": 16,                 # Larger batch for stability
    }
)
print(f"üë®‚Äçüî¨ Created fine-tuning job:\n{job.to_json(indent=2)}")
```

### 3.3 Monitor Fine-Tuning Progress

We'll monitor the progress of our fine-tuning job, with optimized early stopping to save time:

```python
# Monitor the fine-tuning job with early stopping capability
from IPython.display import clear_output
import time

start_time = time.time()
last_loss = None
patience_counter = 0
PATIENCE = 3  # Stop if no improvement for 3 checks

status = job.status
while status not in ["succeeded", "failed", "cancelled"]:
    time.sleep(10)
    job = client.fine_tuning.jobs.retrieve(job.id)
    status = job.status
    
    # Monitor training metrics for early stopping
    try:
        events = client.fine_tuning.jobs.list_events(job.id, limit=5)
        if events.data:
            latest_event = events.data[0]
            if hasattr(latest_event, 'data') and 'train_loss' in latest_event.data:
                current_loss = latest_event.data['train_loss']
                if last_loss and current_loss >= last_loss:
                    patience_counter += 1
                    if patience_counter >= PATIENCE:
                        print(f"‚ö†Ô∏è Early stopping: Loss plateau detected")
                else:
                    patience_counter = 0
                last_loss = current_loss
                print(f"üìà Latest train_loss: {current_loss:.4f}")
    except Exception as e:
        pass  # Continue without early stopping if events unavailable
    
    clear_output(wait=True)
    print(f"üë®‚Äçüî¨ Job {job.id}: {status}")
    print("‚è±Ô∏è Elapsed time: {} minutes {} seconds".format(int((time.time() - start_time) // 60), int((time.time() - start_time) % 60)))

print(f"üèÅ Fine-tuning finished!")
```

---

## Step 4: Deploying and Evaluating the Distilled Model

### 4.1 Deploy the Fine-Tuned Model

After fine-tuning completes, we'll deploy our model for testing:

```python
# Deploy the fine-tuned model using Developer Tier to keep costs low
from azure.identity import DefaultAzureCredential
from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient

cogsvc_client = CognitiveServicesManagementClient(
    credential=DefaultAzureCredential(),
    subscription_id=os.environ.get("AZURE_SUBSCRIPTION_ID")
)

# Define our deployment configuration
DEPLOYMENT_NAME = f"zava-tone-distilled-{SUFFIX}"
DEPLOYMENT = {
    "properties": {
        "model": { 
            "format": "OpenAI", 
            "name": job.fine_tuned_model, 
            "version": "1" 
        },
    },
    "sku": { 
        "capacity": 250, 
        "name": "DeveloperTier" 
    },
}

# Submit the deployment request
deployment = cogsvc_client.deployments.begin_create_or_update(
    resource_group_name=os.environ.get("AZURE_RESOURCE_GROUP"),
    account_name=os.environ.get("AZURE_AI_FOUNDRY_NAME"),
    deployment_name=DEPLOYMENT_NAME,
    deployment=DEPLOYMENT,
)
print(f"üõ≥Ô∏è Submitted deployment {deployment}")
```

### 4.2 Wait for Deployment to Complete

We'll monitor the deployment until it's ready:

```python
# Wait for deployment to complete
start_time = time.time()
status = deployment.status()

while status not in ["Succeeded", "Failed"]:
    deployment.wait(5)
    status = deployment.status()
    clear_output(wait=True)
    print(f"üõ≥Ô∏è Provisioning {DEPLOYMENT_NAME}: {status}")
    print("‚è±Ô∏èElapsed time: {} minutes {} seconds".format(int((time.time() - start_time) // 60), int((time.time() - start_time) % 60)))

print(f"üèÅ Provisioning finished!")
```

### 4.3 Prepare Validation Data for Final Evaluation

Now we'll upload our validation dataset for the final evaluation:

```python
# Upload validation data for final evaluation
filename = f"./zava-tone-posttraining-{UNIQUE_ENOUGH_KEY}.jsonl"

with open(filename, "w") as f:
    for row in qa_validation:
        json.dump(row, f)
        f.write("\n")

posttraining_file = None
with open(filename, "rb") as f:
    posttraining_file = client.files.create(purpose="evals", file=f)
    posttraining_file = client.files.wait_for_processing(posttraining_file.id)
```

### 4.4 Set Up Final Evaluation

We'll set up an evaluation to compare our distilled model against both the original student model and the teacher model:

```python
# Define the models to compare in our final evaluation
POST_EVAL_MODELS = [
    DEPLOYMENT_NAME, # distilled model
    "gpt-4.1-nano",  # original student
    "gpt-4.1",       # control (teacher or another large model)
]

# Create the evaluation
posttraining_eval = client.evals.create(
    name=f"zava-tone-posttraining-{UNIQUE_ENOUGH_KEY}",
    data_source_config=DATA_SOURCE,
    testing_criteria=TESTING_CRITERIA
)

# Create runs for each model
postraining_runs = []
for model in POST_EVAL_MODELS:
    RUN_DATA_SOURCE = {
        "type": "completions",
        "model": model,
        "source": { "type": "file_id", "id": posttraining_file.id },
        "input_messages": {
            "type": "template",
            "template": [
                { 
                    "type": "message", 
                    "role": "system", 
                    "content": { "type": "input_text", "text": SYSTEM_PROMPT },
                },
                { 
                    "type": "message", 
                    "role": "user", 
                    "content": { "type": "input_text", "text": "{{item.question}}" },
                },
            ],
        },
        "sampling_params": { "max_completions_tokens": 100 },
    }
    
    run = client.evals.runs.create(
        name=f"{model}-{UNIQUE_ENOUGH_KEY}", 
        eval_id=posttraining_eval.id,
        data_source=RUN_DATA_SOURCE, 
    )
    print(f"üèÉ‚Äç‚û°Ô∏è Created run {run.id} for eval {posttraining_eval.id}")
    postraining_runs.append(run)
```

### 4.5 Wait for Evaluation Results

We'll wait for all evaluation runs to complete:

```python
# Wait for all evaluation runs to complete
start_time = time.time()

while any([r.status not in ["completed", "failed"] for r in postraining_runs]):
    time.sleep(10)
    clear_output(wait=True)

    for i in range(len(postraining_runs)):
        postraining_runs[i] = client.evals.runs.retrieve(eval_id=posttraining_eval.id, run_id=postraining_runs[i].id)
        print(f"üèÉ‚Äç‚û°Ô∏è Run {postraining_runs[i].name}: {postraining_runs[i].status}")
    
    now = time.time()
    print("‚è±Ô∏è Elapsed time: {} minutes {} seconds".format(int((now - start_time) // 60), int((now - start_time) % 60)))

print(f"üèÅ All {len(postraining_runs)} runs completed!")
```

---

## Step 5: Interpreting the Results

### 5.1 Visualize Evaluation Results

Finally, we'll visualize the results to see how our distilled model compares to the original models:

```python
# Import the visualization utility
import importlib.util
spec = importlib.util.spec_from_file_location("eval_utils", "../3-Custom-Politeness-Evaluator/eval_utils.py")
eval_utils = importlib.util.module_from_spec(spec)
spec.loader.exec_module(eval_utils)
display_evaluation_summary = eval_utils.display_evaluation_summary

# View results of post-training evaluation
display_evaluation_summary(client, [posttraining_eval.id], x_range=(1, 10))

# Compare pre and post distillation results
display_evaluation_summary(client, [baseline_eval.id, posttraining_eval.id], x_range=(1, 10))
```

!!! tip
    When interpreting the results, look for how close the distilled model's performance is to the teacher model. A successful distillation should show the smaller model performing significantly better than its original version and approaching the performance of the larger model.

---

## Step 6: Teardown

Once you've completed this lab, don't forget to clean up your resources:

- Delete the fine-tuned model deployment
- Delete any temporary files created during the lab
- If you're completely done with the Azure AI project, consider deleting the resource group

---

## Summary

In this lab, you've learned how to:

1. Use evaluation data to select the best teacher and student models for distillation
2. Create training datasets from high-quality model responses
3. Fine-tune a smaller model using knowledge distillation techniques
4. Deploy and evaluate the distilled model against the original models
5. Analyze the performance improvements in terms of cost and efficiency

The distillation process has allowed you to:

- ü§ë Minimize per-token costs by using a smaller model
- üèéÔ∏è Improve performance (reduce latency) while maintaining response quality
- üéØ Create a task-specific model optimized for your particular use case

All of this was accomplished without manually creating training data - instead, we used evaluations to identify high-quality outputs from a larger model and used those to train our smaller model.

---

<div style="display: flex; align-items: center; justify-content: left; padding: 5px; height: 40px; background: linear-gradient(90deg, #7873f5 0%, #ff6ec4 100%); border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.12); font-size: 1.5em; font-weight: bold; color: #fff;">
    Next: Wrap Up
</div>