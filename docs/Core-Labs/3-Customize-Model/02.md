# 4.2 Create Custom Politeness Evaluator

!!! quote "BY THE END OF THIS LAB YOU SHOULD HAVE"

    - [ ] Built a custom evaluator (grader) for measuring response quality
    - [ ] Tested baseline model performance against your evaluation criteria
    - [ ] Analyzed evaluation results to identify the best model for distillation
    - [ ] Prepared data for the distillation process

---

In this lab, we'll explore how to create a custom evaluator that grades AI responses based on politeness, helpfulness, and factual accuracy. This evaluator will help us measure how well different models follow the Zava tone guidelines and identify which models are best for our distillation process. By the end, you'll understand how to create objective measurements for subjective qualities like "politeness" in AI responses.

## Step 1: Check Environment Setup

Before we begin creating our evaluator, let's ensure our environment is correctly configured with the necessary Azure OpenAI credentials.

```python
import os

openai_key = os.getenv("AZURE_OPENAI_API_KEY")
openai_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
model_name = "gpt-4.1"
api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2025-02-01-preview")

if not openai_key or not openai_endpoint:
    print("Error: Missing AZURE_OPENAI_KEY or AZURE_OPENAI_ENDPOINT environment variable.")

print("Using Model:", model_name)
print("Using API Version:", api_version)
```

Next, create an Azure OpenAI Client and generate a unique ID to track this notebook run:

```python
from openai import AzureOpenAI

client = AzureOpenAI(
    azure_endpoint=os.environ.get("AZURE_OPENAI_ENDPOINT"),
    api_key=os.environ.get("AZURE_OPENAI_API_KEY"),
    api_version="2025-04-01-preview",
)

import uuid
UNIQUE_ENOUGH_KEY = str(uuid.uuid4()).split("-")[0]
```

!!! note
    If you encounter any errors, verify that your environment variables are set correctly in your `.env` file.

---

## Step 2: Building Our Grader

Distillation involves transferring knowledge from a "teacher" model to a "student" model. To evaluate this process, we need a way to measure response quality before and after distillation. We'll create a "grader" - a model that evaluates responses against our specific criteria for tone and style.

### 2.1 Curate Baseline Responses

First, we'll review our baseline data - examples of "good" responses that match our desired style:

```python
import pandas as pd

# Read the JSONL file and display sample data
baseline_jsonl_df = pd.read_json("../data/distill_sft_baseline.jsonl", lines=True)
pd.set_option('display.max_colwidth', None)
pd.set_option('display.width', 200)
pd.DataFrame(
    [
        {"question": row["item"]["question"], "answer": row["item"]["answer"]}
        for _, row in baseline_jsonl_df.head(5).iterrows()
    ]
)
```

This will display the first 5 examples from our baseline dataset, showing the question-answer pairs that represent our ideal response style.

### 2.2 Upload Baseline Data To Cloud

Now we'll upload our baseline data to Azure OpenAI for use in evaluations:

```python
# Create a cloud-hosted file with baseline data (purpose="evals")
grader_eval_file = None
with open("../data/distill_sft_baseline.jsonl", "rb") as f:
    grader_eval_file = client.files.create(purpose="evals", file=f)
    grader_eval_file = client.files.wait_for_processing(grader_eval_file.id)

print(f"Created eval file:\n{grader_eval_file.to_json(indent=2)}")
```

!!! note
    The "evals" purpose indicates this file will be used for evaluation jobs. The file must be in JSONL format.

### 2.3 Create a Helpfulness Evaluator

Our grader will evaluate responses based on three metrics: politeness, helpfulness, and factual accuracy. We'll use a capable reasoning model (o3) for our grader:

```python
# We'll use the most capable reasoning model as our grader
GRADER_MODEL = "o3"
```

Now we'll define the prompt that tells our grader how to evaluate responses:

```python
GRADER_PROMPT = """
You are an expert in assessing polite and helpful customer service responses

You'll be given a conversation in the form of a question and answer. 

## Scoring Criteria

Judge the answer by using three metrics to compute a final score.

### Metric 1: Is the answer polite?
Give this a score in the range of 1 to 5 where:
- 1 means the answer was rude, disrespectful or dismissive
- 3 means the answer was neutral, neither polite nor rude
- 5 means the answer had an emoji, a warm, respectful tone; acknowledges the user or context; optional greeting; no blame or dismissiveness.

### Metric 2: Is the answer helpful?
Give this a score in the range of 1 to 5 where:
- 1 means the response did not end with an offer to help further
- 3 means the response ended with a generic offer to help
- 5 means the response directly answers the question, provides concrete next steps/resources, ends with a specific offer to help related to the user's need.

### Metric 3: Is the answer informative?
Give this a score of 0 or 1 where:
- 0 means the answer did not mention any specific product or product-related fact
- 1 means the answer mentions a specific product/feature/policy/setting or concrete fact/step clearly relevant.

### Final Score
The final score you must decide should be based on a weighted blend of Metric 1 and
Metric 2 using the formula: `(Metric 1 + Metric 2) * (Metric 3)`

This means that if Metric 3 is zero, the final score must be zero.
"""
```

!!! tip
    This detailed rubric ensures the model grades responses consistently based on our specific criteria for Zava's tone.

### 2.4 Put the Grader Together

Next, we'll define the structure of our evaluation, including the user prompt pattern and testing criteria:

```python
# Define how user prompts will be formatted for evaluation
USER_PROMPT = """
Q: {{item.question}}
A: {{item.answer}}
"""

# Create input messages for our evaluator
INPUT = [
    {
        "type": "message",
        "role": "system",
        "content": { "type": "input_text", "text": GRADER_PROMPT }
    },
    {
        "type": "message",
        "role": "user",
        "content": { "type": "input_text", "text": USER_PROMPT }
    }
]

# Define the data schema and testing criteria
SCHEMA = {
    "type": "object",
    "properties": {
        "question": { "type": "string" },
        "answer": { "type": "string" },
    }
}
DATA_SOURCE = {
    "item_schema": SCHEMA,
    "include_sample_schema": False,
    "type": "custom",
}

# Set up the testing criteria
TESTING_CRITERIA = {
    "name": "Zava Tone Grader",
    "type": "score_model",
    "model": GRADER_MODEL,
    "input": INPUT,
    "range": [1.0, 10.0],    # Our grader scores in a range from 1 to 10
    "pass_threshold": 4.0,   # Let's say a 4 is "passing" for now.
}
```

### 2.5 Submit The Evaluation Task

Now we'll create our evaluation using the Azure OpenAI API:

```python
# Create the evaluation
grader_eval = client.evals.create(
    name=f"zava-tone-baseline-{UNIQUE_ENOUGH_KEY}",
    data_source_config=DATA_SOURCE,
    testing_criteria=[TESTING_CRITERIA],
)

print(f"‚öñÔ∏è Submitted grader evaluation {grader_eval.id}.")
```

### 2.6 Run The Evaluation Job

With the evaluation created, we'll run it against our baseline dataset:

```python
# Set up the data source for our evaluation run
RUN_DATA_SOURCE = {
    "type": "jsonl",
    "source": { "type": "file_id", "id": grader_eval_file.id }
}
grader_run = client.evals.runs.create(
    name=f"zava-tone-grader-{GRADER_MODEL}",
    eval_id=grader_eval.id,
    data_source=RUN_DATA_SOURCE,
)
print(f"üèÉ‚Äç‚û°Ô∏è Submitted run {grader_run.id} for {grader_eval.id}.")
```

### 2.7 Poll For Evaluation Results

The evaluation takes time to complete. We'll monitor its progress:

```python
# Wait for the evaluation to complete
from IPython.display import clear_output
import time

start_time = time.time()

grader_run = client.evals.runs.retrieve(eval_id=grader_eval.id, run_id=grader_run.id)
while grader_run.status not in ["completed", "failed"]:
    time.sleep(5)
    clear_output(wait=True)

    grader_run = client.evals.runs.retrieve(eval_id=grader_eval.id, run_id=grader_run.id)
    now = time.time()
    mins, secs = int((now - start_time) // 60), int((now - start_time) % 60)
    print(f"‚è±Ô∏è Elapsed time: {mins} minutes {secs} seconds")

print(f"üèÅ Run {grader_run.id}: {grader_run.status}!")
```

### 2.8 View & Analyze Results

After the evaluation completes, we'll visualize the results:

```python
# Visualize the evaluation results
from eval_utils import display_evaluation_summary

display_evaluation_summary(client, [grader_eval.id], x_range=(0, 10))
```

!!! note
    This visualization helps us verify that our grader rates our "gold standard" examples appropriately. If the results don't match our expectations, we might need to refine our grader's prompt.

---

## Step 3: Baseline Testing Our Candidate Models

Now that we have a working grader, we'll test various models to see which performs best with our criteria.

### 3.1 Curate Q&A Dataset

First, let's examine our Q&A dataset that we'll use for testing:

```python
# Review our Q&A dataset
qa_jsonl_df = pd.read_json("../data/distill_sft_qa.jsonl", lines=True)
pd.DataFrame(
    [
        {"question": row["item"]["question"], "answer": row["item"]["answer"]}
        for _, row in qa_jsonl_df.head(5).iterrows()
    ]
)
```

### 3.2 Split Training & Validation

We'll split our Q&A dataset into baseline (training) and validation sets:

```python
import json

# Load the Q&A data
qa = []
with open("../data/distill_sft_qa.jsonl", "r") as f:
    for line in f.readlines():
        qa.append(json.loads(line))

# Randomize and split the data 50/50
from random import shuffle
shuffle(qa)

split_at = int(len(qa) / 2)
qa_baseline = qa[:split_at]
qa_validation = qa[split_at:]

print(f"{len(qa_baseline)} pairs for baseline testing, {len(qa_validation)} for validation.")
```

### 3.3 Upload Baseline (Training) Set

Next, we'll save and upload our baseline dataset:

```python
# Save and upload the baseline dataset
filename = f"./zava-tone-baseline-{UNIQUE_ENOUGH_KEY}.jsonl"

with open(filename, "w") as f:
    for row in qa_baseline:
        json.dump(row, f)
        f.write("\n")

baseline_file = None
with open(filename, "rb") as f:
    baseline_file = client.files.create(purpose="evals", file=f)
    baseline_file = client.files.wait_for_processing(baseline_file.id)

print(f"Created baseline file:\n{baseline_file.to_json(indent=2)}")
```

### 3.4 Create The Baseline Evaluation

We'll set up an evaluation that tests multiple models with our grader:

```python
# Define a simple system prompt
SYSTEM_PROMPT = "You are Cora, a polite, factual and helpful assistant for Zava, a DIY hardware store."

# Define the models we want to test
BASE_MODELS = [
    "o3-mini",
    "o4-mini",
    "gpt-4.1",
    "gpt-4.1-nano",
    "gpt-4o-mini"
]

# Create the evaluation
baseline_eval = client.evals.create(
    name=f"zava-tone-baseline-{UNIQUE_ENOUGH_KEY}",
    data_source_config=DATA_SOURCE,
    testing_criteria=[TESTING_CRITERIA]
)
print(f"‚öñÔ∏è Created baseline eval {baseline_eval.id}")
```

### 3.5 Run Baseline Evaluation

Now we'll create and run evaluation jobs for each model:

```python
# Create runs for each model
baseline_runs = []
for model in BASE_MODELS:
    RUN_DATA_SOURCE = {
        "type": "completions",
        "model": model,
        "source": { "type": "file_id", "id": baseline_file.id },
        "input_messages": {
            "type": "template",
            "template": [
                { 
                    "type": "message", 
                    "role": "system", 
                    "content": { "type": "input_text", "text": SYSTEM_PROMPT },
                },
                { 
                    "type": "message", 
                    "role": "user", 
                    "content": { "type": "input_text", "text": "{{item.question}}" },
                },
            ],
        },
        "sampling_params": { "max_completions_tokens": 20_000 } if model.startswith("o") else { "max_completions_tokens": 100 },
    }
    run = client.evals.runs.create(
        name=f"{model}-{UNIQUE_ENOUGH_KEY}", 
        eval_id=baseline_eval.id,
        data_source=RUN_DATA_SOURCE, 
    )
    print(f"üèÉ‚Äç‚û°Ô∏è Created run {run.id} for eval {baseline_eval.id}")
    baseline_runs.append(run)
```

### 3.6 Poll For Evaluation Results

We'll wait for all our evaluation runs to complete:

```python
# Wait for all runs to complete
while any([r.status not in ["completed", "failed"] for r in baseline_runs]):
    time.sleep(10)
    clear_output(wait=True)

    for i in range(len(baseline_runs)):
        baseline_runs[i] = client.evals.runs.retrieve(eval_id=baseline_eval.id, run_id=baseline_runs[i].id)
        print(f"üèÉ‚Äç‚û°Ô∏è Run {baseline_runs[i].name}: {baseline_runs[i].status}")
    
    now = time.time()
    print("‚è±Ô∏è Elapsed time: {} minutes {} seconds".format(int((now - start_time) // 60), int((now - start_time) % 60)))

print(f"üèÅ All {len(baseline_runs)} runs completed!")
```

### 3.7 Visualize Results & Pick Model

Finally, we'll visualize the results to identify which models performed best:

```python
# Visualize evaluation results
display_evaluation_summary(client, [baseline_eval.id], x_range=(1, 10))
```

!!! tip
    Look for the model with the highest median score - this will be your "teacher" model for distillation. The model with the lowest score that's still reasonably sized could be your "student" model.

We'll also export the data for use in the distillation notebook:

```python
# Save evaluation data for the distillation notebook
export_data = {
    'baseline_eval_id': baseline_eval.id,
    'baseline_runs_ids': [run.id for run in baseline_runs],
    'baseline_runs_models': [run.model for run in baseline_runs],
    'qa_validation': qa_validation,
    'GRADER_MODEL': GRADER_MODEL,
    'GRADER_PROMPT': GRADER_PROMPT,
    'SYSTEM_PROMPT': SYSTEM_PROMPT,
    'UNIQUE_ENOUGH_KEY': UNIQUE_ENOUGH_KEY
}

# Save to a JSON file
with open('../data/distillation_export.json', 'w') as f:
    json.dump(export_data, f, indent=2)

print("‚úÖ Exported data for distillation notebook")
```

---

## Step 4: Teardown

Once you've completed this lab, don't forget to clean up your resources:

- Delete the resource group
- Purge any created resources

---

## Summary

In this lab, you've learned how to:

1. Create a custom evaluator (grader) that objectively measures subjective qualities like politeness and helpfulness
2. Test multiple models against your evaluation criteria
3. Analyze evaluation results to select the best models for distillation
4. Export data for use in the distillation process

This approach allows you to:
- Evaluate AI responses based on specific business requirements
- Identify which models perform best for your specific use case
- Prepare for model distillation to create more efficient AI assistants

---

<div style="display: flex; align-items: center; justify-content: left; padding: 5px; height: 40px; background: linear-gradient(90deg, #7873f5 0%, #ff6ec4 100%); border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.12); font-size: 1.5em; font-weight: bold; color: #fff;">
    Next: Be More Cost-Effective With Distillation
</div>