{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44644603",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(135deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); color: white; padding: 20px; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.2);\">\n",
    "    <h1 style=\"color: #FFF; text-shadow: 1px 1px 3px rgba(0,0,0,0.5);\">üí¨ | Step 3: Be More Cost-Effective With Distillation </h1>\n",
    "        <p style=\"font-size: 16px; line-height: 1.6;\">\n",
    "            We have the model customized to provide the right tone and style. But we are using a large (slow and expensive) model for a very narrow and focused task. Let's see how we can distill the knowledge to a smaller (faster and cheaper) model to get comparable accuracy with better efficiency - by trading in general-purpose LLMs for a narrower task-specific intelligence.\n",
    "        </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfd8483",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Check Environment Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56955b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make sure required environment variables are set\n",
    "import os\n",
    "\n",
    "openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "model_name = \"gpt-4.1\"\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2025-02-01-preview\")\n",
    "\n",
    "if not openai_key or not openai_endpoint:\n",
    "    print(\"Error: Missing AZURE_OPENAI_KEY or AZURE_OPENAI_ENDPOINT environment variable.\")\n",
    "\n",
    "print(\"Using Model:\", model_name)\n",
    "print(\"Using API Version:\", api_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465864fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then create an Azure OpenAI Client instance\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2025-04-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99ea17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And create a handy UUID we can use to track each run of this notebook\n",
    "import uuid\n",
    "UNIQUE_ENOUGH_KEY = str(uuid.uuid4()).split(\"-\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ce633f",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Building Our Grader\n",
    "\n",
    "Distillation is about having a \"teacher\" model transfer its learning to a \"student\" model, thereby allowing it to perform a _narrower_ task with comparable accuracy. To understand if our distillation had the desired effect, we typically _grade_ the student model before and after distillation - and show that the grades on that task improved significantly.\n",
    "\n",
    "To do this, we need a _grader_ - some model that is capable of \"reasoning\" on a given response to see if it meets our desired criteria for \"tone & style\". By using the same grader throughout, we can make sure we get a consistent outcome in terms of evaluations. To create an effective grader we take 3 steps:\n",
    "\n",
    "1. We curate a set of **baseline** questions and answers that are the gold standard for responses. A \"good\" grader would rank these very highly when asked.\n",
    "2. We define a set of **grading** criteria that the grader will use to come up with a score on the responses. These must be clear and consistent guidelines.\n",
    "3. We run the grader on that gold standard to see how our **assessment** rubric holds up. If the results look good, we have a grader ready to go - else we refine and retry.\n",
    "\n",
    "By the end of this step, we will have an effective grader ready to use!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec1f178",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2.1 Curate Baseline Responses\n",
    "\n",
    "In our case, we can simply take a subset of the training data from the previous step since we know it is the kind of \"good\" responses we want to see. We store it in a new file (`distill_sft_baseline.json`) that we can then upload to our Azure AI Foundry project, for use in a later step. Let's review the data in the file and upload it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c465496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the JSONL file into a DataFrame and print the first 5 rows in 2 columns\n",
    "baseline_jsonl_df = pd.read_json(\"33-distill_sft_baseline.jsonl\", lines=True)\n",
    "# Display all columns and set display width to show full text in the output\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 200)\n",
    "pd.DataFrame(\n",
    "    [\n",
    "        {\"question\": row[\"item\"][\"question\"], \"answer\": row[\"item\"][\"answer\"]}\n",
    "        for _, row in baseline_jsonl_df.head(5).iterrows()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c2e925",
   "metadata": {},
   "source": [
    "### 2.2 Upload Baseline Data To Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20078f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cloud-hosted file with baseline data for evaluations (purpose=\"evals\")\n",
    "# An \"evals\" file is a special type of file that can be used in evaluation jobs \n",
    "# - it has to be in JSONL format but its properties depend on how grader is setup\n",
    "grader_eval_file = None\n",
    "with open(\"./33-distill_baseline.jsonl\", \"rb\") as f:\n",
    "    grader_eval_file = client.files.create(purpose=\"evals\", file=f)\n",
    "    grader_eval_file = client.files.wait_for_processing(grader_eval_file.id)\n",
    "\n",
    "print(f\"Created eval file:\\n{grader_eval_file.to_json(indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7132b0c4",
   "metadata": {},
   "source": [
    "### 2.3 Create a Helpfulness Evaluator\n",
    "\n",
    "The Grader is just like a custom evaluator that takes an input (\"question\", \"answer\") and scores it for the desired metric (\"Helpfulness\") using a set of custom criteria that we define. This is an example of AI-Assisted evaluation (or LLM as a judge) where we use an LLM to execute a task (\"grader this\") where we describe the parameters of the task in the prompt template we provide. Let's see that in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39987c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the most capable reasoning model as our grader.\n",
    "GRADER_MODEL = \"o4-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eb2456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we create a custom Grader that will score the responses for \"Helpfulness\" on a scale of 0-10\n",
    "# using the custom rubric below - where 0 is \"not helpful at all\" and 10 is \"extremely helpful\".\n",
    "\n",
    "# We want it to grader our \"student\" responses later, based on the same criteria. So let's make sure we get our rubric right.\n",
    "# We want good answers to score high - and bad answers to be penalized heavily (get 0)\n",
    "\n",
    "GRADER_PROMPT = \"\"\"\n",
    "\n",
    "You are an expert in assessing polite and helpful customer service responses\n",
    "\n",
    "You'll be given a conversation in the form of a question and answer. \n",
    "\n",
    "## Scoring Criteria\n",
    "\n",
    "Judge the answer by using two metrics to compute a final score.\n",
    "\n",
    "### Metric 1: Is the answer polite?\n",
    "\n",
    "Give this a score in the range of 1 to 5 where:\n",
    "- 1 means the answer was rude, disrespectful or dismissive\n",
    "- 3 means the answer was neutral, neither polite nor rude\n",
    "- 5 means the answer had an emoji followed by a greeting or an acknowledgement of the user question\n",
    "\n",
    "### Metric 2: Is the answer helpful?\n",
    "\n",
    "Give this a score in the range of 1 to 5 where:\n",
    "- 1 means the response did not end with an offer to help further\n",
    "- 3 means the response ended with a generic offer to help\n",
    "- 5 means the response ended with an offer to help that was clearly related to the user's question\n",
    "\n",
    "### Metric 3: Is the answer informative?\n",
    "Give this a score of 0 or 1 where:\n",
    "- 0 means the answer did not mention any specific product or product-related fact\n",
    "- 1 means the answer mentioned a specific product or solution\n",
    "\n",
    "### Final Score\n",
    "The final score you must decide should be based on a weighted blend of Metric 1 and\n",
    "Metric 2 using the formula: `(Metric 1 + Metric 2) * (Metric 3)`\n",
    "\n",
    "This means that if Metric 3 is zero, the final score must be zero.\n",
    "\n",
    "## Response Structure\n",
    "Your response must be in a JSON format that can be loaded by Python's json.loads()\n",
    "function. It must resemble the following:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"steps\": [\n",
    "    { \n",
    "      \"description\": <one sentence describing your reasoning for Metric 1>\", \n",
    "      \"result\": <string representation of Metric 1's score> \n",
    "    },\n",
    "    { \n",
    "      \"description\": <one sentence describing your reasoning for Metric 2>\", \n",
    "      \"result\": <string representation of Metric 2's score> \n",
    "    }\n",
    "  ],\n",
    "  \"result\": <floating point value of the Final Score>\n",
    "}\n",
    "\n",
    "## General Guidance\n",
    "The questions will be about paint products and related topics. Deep research is not required. Use common sense to determine if the answer is polite, helpful and factual. The responses should be concise and to the point.\n",
    "\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baac3299",
   "metadata": {},
   "source": [
    "### 2.4 Put the Grader Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd7d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define our Evaluation for validating our choice in grader prompt and model.\n",
    "\n",
    "# The entire user prompt is data driven from the file. No generation is done using\n",
    "# a model in this case, just simple string substitution using this pattern. This\n",
    "# means we directly reference the two fields in our baseline.jsonl file.\n",
    "\n",
    "USER_PROMPT = \"\"\"\n",
    "Q: {{item.question}}\n",
    "A: {{item.answer}}\n",
    "\"\"\"\n",
    "\n",
    "INPUT = [\n",
    "    {\n",
    "        \"type\": \"message\",\n",
    "        \"role\": \"system\",\n",
    "        \"content\": { \"type\": \"input_text\", \"text\": GRADER_PROMPT }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"message\",\n",
    "        \"role\": \"user\",\n",
    "        \"content\": { \"type\": \"input_text\", \"text\": USER_PROMPT }\n",
    "    }\n",
    "]\n",
    "\n",
    "# We need to describe what our evaluation dataset looks like.\n",
    "SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"question\": { \"type\": \"string\" },\n",
    "        \"answer\": { \"type\": \"string\" },\n",
    "    }\n",
    "}\n",
    "DATA_SOURCE = {\n",
    "    \"item_schema\": SCHEMA,\n",
    "    \"include_sample_schema\": False,\n",
    "    \"type\": \"custom\",\n",
    "}\n",
    "\n",
    "# Lastly, we define test criteria that combines all the above.\n",
    "TESTING_CRITERIA = {\n",
    "    \"name\": \"Zava Tone Grader\",\n",
    "    \"type\": \"score_model\",\n",
    "    \"model\": GRADER_MODEL,\n",
    "    \"input\": INPUT,\n",
    "    \"range\": [1.0, 10.0],    # Our grader scores in a range from 1 to 10\n",
    "    \"pass_threshold\": 4.0,   # Let's say a 4 is \"passing\" for now.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd3f23b",
   "metadata": {},
   "source": [
    "### 2.5 Submit The Evaluation Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2a9254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've set up the parameters for our Eval, now we create it via the API.\n",
    "grader_eval = client.evals.create(\n",
    "    name=f\"zava-tone-baseline-{UNIQUE_ENOUGH_KEY}\",\n",
    "    data_source_config=DATA_SOURCE,\n",
    "    testing_criteria=[TESTING_CRITERIA],\n",
    ")\n",
    "\n",
    "print(f\"‚öñÔ∏è Submitted grader evaluation {grader_eval.id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cadff41",
   "metadata": {},
   "source": [
    "### 2.6 Run The Evaluation Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80e24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our evaluation needs a test run. This is where we let it know to use our\n",
    "# \"gold standard\" file (baseline.jsonl) to test our grader.\n",
    "\n",
    "RUN_DATA_SOURCE = {\n",
    "    \"type\": \"jsonl\",\n",
    "    \"source\": { \"type\": \"file_id\", \"id\": grader_eval_file.id }\n",
    "}\n",
    "grader_run = client.evals.runs.create(\n",
    "    name=f\"32-zava-tone-grader-{GRADER_MODEL}\",\n",
    "    eval_id=grader_eval.id,\n",
    "    data_source=RUN_DATA_SOURCE,\n",
    ")\n",
    "print(f\"üèÉ‚Äç‚û°Ô∏è Submitted run {grader_run.id} for {grader_eval.id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887047d3",
   "metadata": {},
   "source": [
    "### 2.7 Poll For Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f143b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An Eval Run takes time to complete. Let's actively wait for it to finish before continuing.\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "grader_run = client.evals.runs.retrieve(eval_id=grader_eval.id, run_id=grader_run.id)\n",
    "while grader_run.status not in [\"completed\", \"failed\"]:\n",
    "    time.sleep(5)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    grader_run = client.evals.runs.retrieve(eval_id=grader_eval.id, run_id=grader_run.id)\n",
    "    now = time.time()\n",
    "    mins, secs = int((now - start_time) // 60), int((now - start_time) % 60)\n",
    "    print(f\"‚è±Ô∏è Elapsed time: {mins} minutes {secs} seconds\")\n",
    "\n",
    "print(f\"üèÅ Run {grader_run.id}: {grader_run.status}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3407dd",
   "metadata": {},
   "source": [
    "### 2.8 View & Analyze Results\n",
    "\n",
    "You can see these on the Azure AI Foundry portal - or run a script to visualize them here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've got a handy script for rendering the results from an Evaluations Runs. Let's\n",
    "# eyeball this stuff. It uses the Evals API to retrieve the scores and plot them.\n",
    "from eval_utils import display_evaluation_summary\n",
    "\n",
    "display_evaluation_summary(client, [grader_eval.id], x_range=(0, 10))\n",
    "\n",
    "# We should see that our grader generally thinks our \"gold standard\" is pretty on-brand for Zava tone. \n",
    "# This is where we'd iterate on tuning the grader, making sure we\n",
    "# clearly capture features for it to score, etc. We're keeping it simple for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851b76fe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Baseline Testing Our Candidate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371fb56d",
   "metadata": {},
   "source": [
    "### 3.1 Curate Q&A Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea010600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the JSONL file into a DataFrame and print the first 5 rows in 2 columns\n",
    "qa_jsonl_df = pd.read_json(\"33-distill_sft_qa.jsonl\", lines=True)\n",
    "# Display all columns and set display width to show full text in the output\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 200)\n",
    "pd.DataFrame(\n",
    "    [\n",
    "        {\"question\": row[\"item\"][\"question\"], \"answer\": row[\"item\"][\"answer\"]}\n",
    "        for _, row in qa_jsonl_df.head(5).iterrows()\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc388ad7",
   "metadata": {},
   "source": [
    "### 3.2 Split Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12911fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "qa = []\n",
    "with open(\"33-distill_sft_qa.jsonl\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        qa.append(json.loads(line))\n",
    "\n",
    "print(f\"Number of Q/A pairs: {len(qa)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8726f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First we'll randomize it to maybe prove a point that this isn't totally staged üòú\n",
    "from random import shuffle\n",
    "shuffle(qa)\n",
    "\n",
    "# Now we split 50/50.\n",
    "split_at = int(len(qa) / 2)\n",
    "qa_baseline = qa[:split_at]\n",
    "qa_validation = qa[split_at:]\n",
    "\n",
    "# Check it.\n",
    "print(f\"{len(qa_baseline)} pairs for baseline testing, {len(qa_validation)} for validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5010febd",
   "metadata": {},
   "source": [
    "### 3.3 Upload baseline (training) set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d7187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll upload our baseline dataset and prepare our Evaluation. We need to save the data\n",
    "# to disk first for...reasons...because of the OpenAI SDK. That's fine.\n",
    "filename = f\"./32-zava-tone-baseline-{UNIQUE_ENOUGH_KEY}.jsonl\"\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    for row in qa_baseline:\n",
    "        json.dump(row, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "baseline_file = None\n",
    "with open(filename, \"rb\") as f:\n",
    "    baseline_file = client.files.create(purpose=\"evals\", file=f)\n",
    "    baseline_file = client.files.wait_for_processing(baseline_file.id)\n",
    "\n",
    "print(f\"Created baseline file:\\n{baseline_file.to_json(indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e269ad0",
   "metadata": {},
   "source": [
    "### 3.4 Create The Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf281e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll now build out the Evaluation details. In this case, we'll *generate* responses\n",
    "# using a base model, unlike before where we used the pre-canned results just to test\n",
    "# the grader.\n",
    "\n",
    "# We'll use a simple system prompt to show how distillation and fine-tuning let us\n",
    "# get away without overly complex prompt engineering.\n",
    "SYSTEM_PROMPT = \"You are Cora, a polite, factual and helpful assistant for Zava, a DIY hardware store.\"\n",
    "\n",
    "# We'll use a flee of base models as our baseline, including `o3-mini` (our grader).\n",
    "BASE_MODELS = [\n",
    "    \"o3-mini\",\n",
    "    \"o4-mini\",\n",
    "    \"gpt-4.1-mini\",\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4.1-nano\",\n",
    "]\n",
    "\n",
    "# The prompt we'll grade will look like this pattern. Similar to before, but now we're\n",
    "# going to use {{sample.output_text}} to substitute what the model under test generates.\n",
    "USER_PROMPT = \"\"\"\n",
    "Q: {{item.question}}\n",
    "A: {{sample.output_text}}\n",
    "\"\"\"\n",
    "\n",
    "# Input to our grader remains the same as before, but we reproduce it here for context.\n",
    "INPUT = [\n",
    "    {\n",
    "        \"type\": \"message\",\n",
    "        \"role\": \"system\",\n",
    "        \"content\": { \"type\": \"input_text\", \"text\": GRADER_PROMPT }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"message\",\n",
    "        \"role\": \"user\",\n",
    "        \"content\": { \"type\": \"input_text\", \"text\": USER_PROMPT }\n",
    "    }\n",
    "]\n",
    "\n",
    "# The schema and data source are similar, but with one major difference noted below.\n",
    "SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"question\": { \"type\": \"string\" },\n",
    "        \"answer\": { \"type\": \"string\" },\n",
    "    },\n",
    "}\n",
    "DATA_SOURCE = {\n",
    "    \"item_schema\": SCHEMA, \n",
    "    \"include_sample_schema\": True, # Note this change! Needed for data gen.\n",
    "    \"type\": \"custom\"\n",
    "}\n",
    "\n",
    "# Same testing criteria, reproduced again for context.\n",
    "TESTING_CRITERIA = {\n",
    "    \"name\": \"Zava Tone Grader\",\n",
    "    \"type\": \"score_model\",\n",
    "    \"model\": GRADER_MODEL,\n",
    "    \"input\": INPUT,\n",
    "    \"range\": [1.0, 10.0],\n",
    "    \"pass_threshold\": 4.0,\n",
    "}\n",
    "\n",
    "# We create one Evaluation for *all* our base models. Each model is tested in a\n",
    "# distinct Run that we'll define next.\n",
    "baseline_eval = client.evals.create(\n",
    "    name=f\"32-zava-tone-baseline-{UNIQUE_ENOUGH_KEY}\",\n",
    "    data_source_config=DATA_SOURCE,\n",
    "    testing_criteria=[TESTING_CRITERIA]\n",
    ")\n",
    "print(f\"‚öñÔ∏è Created baseline eval {baseline_eval.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14ac8b7",
   "metadata": {},
   "source": [
    "### 3.5 Run Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d93f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each run gets its own data source definition as it needs to specify a different\n",
    "# model deployment to use for generation. The template is the prompt template\n",
    "# sent to the model under test. It uses the simple Clippy system prompt and for\n",
    "# the user input, we use the \"question\" from the baseline Q&A data file.\n",
    "baseline_runs = []\n",
    "for model in BASE_MODELS:\n",
    "    RUN_DATA_SOURCE = {\n",
    "        \"type\": \"completions\",\n",
    "        \"model\": model,\n",
    "        \"source\": { \"type\": \"file_id\", \"id\": baseline_file.id },\n",
    "        \"input_messages\": {\n",
    "            \"type\": \"template\",\n",
    "            \"template\": [\n",
    "                { \n",
    "                    \"type\": \"message\", \n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": { \"type\": \"input_text\", \"text\": SYSTEM_PROMPT },\n",
    "                },\n",
    "                { \n",
    "                    \"type\": \"message\", \n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": { \"type\": \"input_text\", \"text\": \"{{item.question}}\" },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        \"sampling_params\": { \"max_completions_tokens\": 20_000 } if model.startswith(\"o\") else { \"max_completions_tokens\": 100 }, # XXX\n",
    "    }\n",
    "    run = client.evals.runs.create(\n",
    "        name=f\"{model}-{UNIQUE_ENOUGH_KEY}\", \n",
    "        eval_id=baseline_eval.id,\n",
    "        data_source=RUN_DATA_SOURCE, \n",
    "    )\n",
    "    print(f\"üèÉ‚Äç‚û°Ô∏è Created run {run.id} for eval {baseline_eval.id}\")\n",
    "    baseline_runs.append(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b4f158",
   "metadata": {},
   "source": [
    "### 3.6 Poll For Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b4956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have to wait for our half-dozen or so Runs to finish. Twiddle your thumbs a bit!\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while any([r.status not in [\"completed\", \"failed\"] for r in baseline_runs]):\n",
    "    time.sleep(10)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    for i in range(len(baseline_runs)):\n",
    "        baseline_runs[i] = client.evals.runs.retrieve(eval_id=baseline_eval.id, run_id=baseline_runs[i].id)\n",
    "        print(f\"üèÉ‚Äç‚û°Ô∏è Run {baseline_runs[i].name}: {baseline_runs[i].status}\")\n",
    "    \n",
    "    now = time.time()\n",
    "    print(\"‚è±Ô∏è Elapsed time: {} minutes {} seconds\".format(int((now - start_time) // 60), int((now - start_time) % 60)))\n",
    "\n",
    "print(f\"üèÅ All {len(baseline_runs)} runs completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39495ddb",
   "metadata": {},
   "source": [
    "### 3.7 Visualize Results & Pick Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77608b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize our evaluation and identify the best and worst performers.\n",
    "display_evaluation_summary(client, [baseline_eval.id], x_range=(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec0c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "# Save the evaluation objects and runs\n",
    "export_data = {\n",
    "    'baseline_eval_id': baseline_eval.id,\n",
    "    'baseline_runs_ids': [run.id for run in baseline_runs],\n",
    "    'baseline_runs_models': [run.model for run in baseline_runs],\n",
    "    'qa_validation': qa_validation,\n",
    "    'GRADER_MODEL': GRADER_MODEL,\n",
    "    'GRADER_PROMPT': GRADER_PROMPT,\n",
    "    'SYSTEM_PROMPT': SYSTEM_PROMPT,\n",
    "    'UNIQUE_ENOUGH_KEY': UNIQUE_ENOUGH_KEY\n",
    "}\n",
    "\n",
    "# Save to a JSON file\n",
    "with open('32-distillation_export.json', 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Exported data for distillation notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a6cbb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Teardown\n",
    "\n",
    "Don't forget to clean up - delete the resource group, purge resources!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e5aec1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Summary \n",
    "\n",
    "In this notebook we demonstrated **distillation** using Azure OpenAI *Evaluations*\n",
    "and *Fine Tuning* features.\n",
    "\n",
    "We used an objective of *adjusting the tone* of a model to meet our needs, in this\n",
    "case making its responses sarcastic, while preserving accuracy in results, and\n",
    "*distilled* the native capabilities of a state-of-the-art reasoning model (`o3`)\n",
    "into a much smaller, non-reasoning model (`4.1-nano`) to let our agent or app\n",
    "use the smallest model possible while:\n",
    "\n",
    "- ü§ë minimizing per-token costs\n",
    "- üèéÔ∏è improve performance (latency)\n",
    "\n",
    "We did all this:\n",
    "\n",
    "- without creating training data directly\n",
    "- without knowing the ideal student model\n",
    "- only by knowing how to define our Grader\n",
    "\n",
    "So to wrap it all up:\n",
    "\n",
    "1. We described the ideal state to our complex reasoning model in the form of\n",
    "   a few samples we feel are ideal.\n",
    "2. We described to the reasoning model (grader) how to judge those examples\n",
    "   to measure their quality.\n",
    "3. We let Evaluations and Fine Tuning do the rest!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
