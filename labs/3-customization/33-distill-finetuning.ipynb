{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44644603",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(135deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); color: white; padding: 20px; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.2);\">\n",
    "    <h1 style=\"color: #FFF; text-shadow: 1px 1px 3px rgba(0,0,0,0.5);\">üí¨ | Step 3: Be More Cost-Effective With Distillation </h1>\n",
    "        <p style=\"font-size: 16px; line-height: 1.6;\">\n",
    "            We have the model customized to provide the right tone and style. But we are using a large (slow and expensive) model for a very narrow and focused task. Let's see how we can distill the knowledge to a smaller (faster and cheaper) model to get comparable accuracy with better efficiency - by trading in general-purpose LLMs for a narrower task-specific intelligence.\n",
    "        </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfd8483",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Check Environment Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56955b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make sure required environment variables are set\n",
    "import os\n",
    "\n",
    "openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "model_name = \"gpt-4.1\"\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2025-02-01-preview\")\n",
    "\n",
    "if not openai_key or not openai_endpoint:\n",
    "    print(\"Error: Missing AZURE_OPENAI_KEY or AZURE_OPENAI_ENDPOINT environment variable.\")\n",
    "\n",
    "print(\"Using Model:\", model_name)\n",
    "print(\"Using API Version:\", api_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465864fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then create an Azure OpenAI Client instance\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2025-04-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99ea17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And create a handy UUID we can use to track each run of this notebook\n",
    "import uuid\n",
    "UNIQUE_ENOUGH_KEY = str(uuid.uuid4()).split(\"-\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5478eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Distillation From Teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c8a981",
   "metadata": {},
   "source": [
    "### 2.1 Find Our Winning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea22fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Load exported data from grader notebook\n",
    "with open('32-distillation_export.json', 'r') as f:\n",
    "    export_data = json.load(f)\n",
    "\n",
    "# Reconstruct the baseline runs\n",
    "baseline_eval_id = export_data['baseline_eval_id']\n",
    "baseline_runs = []\n",
    "\n",
    "for i, run_id in enumerate(export_data['baseline_runs_ids']):\n",
    "    run = client.evals.runs.retrieve(eval_id=baseline_eval_id, run_id=run_id)\n",
    "    run.model = export_data['baseline_runs_models'][i]  # Ensure model name is set\n",
    "    baseline_runs.append(run)\n",
    "\n",
    "# Get other variables\n",
    "baseline_eval = client.evals.retrieve(baseline_eval_id)\n",
    "qa_validation = export_data['qa_validation']\n",
    "GRADER_MODEL = export_data['GRADER_MODEL']\n",
    "GRADER_PROMPT = export_data['GRADER_PROMPT'] \n",
    "SYSTEM_PROMPT = export_data['SYSTEM_PROMPT']\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(baseline_runs)} baseline runs from grader notebook\")\n",
    "print(f\"‚úÖ Loaded {len(qa_validation)} validation Q&A pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2bb235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composite metric based teacher/student selection\n",
    "# Uses existing score_distribution + HIGH_SCORES already populated above\n",
    "\n",
    "import math\n",
    "from statistics import mean\n",
    "\n",
    "CUTOFF = 4.0\n",
    "HIGH_SCORES = {\n",
    "    \"gpt-4o\": [],\n",
    "    \"gpt-4.1\": [],\n",
    "    \"o3-mini\": [],\n",
    "    \"o4-mini\": [],\n",
    "    \"gpt-4o-mini\": [],\n",
    "    \"gpt-4.1-nano\": [],\n",
    "}\n",
    "\n",
    "# Let's find our responses that were Excellent (at or above CUTOFF). We'll collect them\n",
    "# and pre-format them into chat completions format to save time later.\n",
    "#\n",
    "# This part is honestly a bit tricky...we're extracting the prompts and responses for the\n",
    "# model under test and *not* the prompts to the grader, so we have to do surgery. üî™\n",
    "for run in baseline_runs:\n",
    "    pages = client.evals.runs.output_items.list(run.id, eval_id=baseline_eval.id).iter_pages()\n",
    "    for page in pages:\n",
    "        for item in page.data:\n",
    "            # We only used 1 grader. If you use multiple, you should look for which ones you want.\n",
    "            if not item.results:\n",
    "                continue\n",
    "            result = item.results[0]\n",
    "            # FIX: Use attribute access for score and sample\n",
    "            if result.score >= CUTOFF:\n",
    "                generated = result.sample[\"input\"][-1][\"content\"].strip().split(\"\\nA: \")\n",
    "                question = generated[0][3:] # drops the \"Q: \"\n",
    "                answer = generated[-1]\n",
    "                messages = [\n",
    "                    { \"role\": \"system\", \"content\": SYSTEM_PROMPT },\n",
    "                    { \"role\": \"user\", \"content\": question },\n",
    "                    { \"role\": \"assistant\", \"content\": answer },\n",
    "                ]\n",
    "                HIGH_SCORES[run.model].append({ \"messages\": messages })\n",
    "\n",
    "# 1. Collect raw score lists for each model (recompute if needed)\n",
    "score_distribution = {}\n",
    "for run in baseline_runs:\n",
    "    score_distribution.setdefault(run.model, [])\n",
    "    pages = client.evals.runs.output_items.list(run.id, eval_id=baseline_eval.id).iter_pages()\n",
    "    for page in pages:\n",
    "        for item in page.data:\n",
    "            if not item.results:\n",
    "                continue\n",
    "            result = item.results[0]\n",
    "            # FIX: Use attribute access for score\n",
    "            if hasattr(result, \"score\"):\n",
    "                score_distribution[run.model].append(result.score)\n",
    "\n",
    "# 2. Define model size ordering (left = larger / more capable)\n",
    "SIZE_ORDER = [\"o3-mini\", \"gpt-4.1\", \"o4-mini\", \"gpt-4o-mini\", \"gpt-4.1-nano\"]\n",
    "SIZE_INDEX = {m:i for i,m in enumerate(SIZE_ORDER)}\n",
    "\n",
    "# 3. Composite quality metric\n",
    "#    quality = 0.5*mean + 0.3*p90 + 0.2*coverage  (coverage scaled to 0-10)\n",
    "#    p90 stabilizes performance tail; coverage rewards consistency\n",
    "\n",
    "def composite_quality(scores, cutoff):\n",
    "    if not scores:\n",
    "        return -1\n",
    "    scores_sorted = sorted(scores)\n",
    "    p90 = scores_sorted[math.floor(0.9*(len(scores_sorted)-1))]\n",
    "    coverage = sum(1 for s in scores if s >= cutoff) / len(scores) if scores else 0.0\n",
    "    return 0.5*mean(scores) + 0.3*p90 + 0.2*(coverage*10)  # coverage scaled\n",
    "\n",
    "# 4. Pick teacher = argmax composite_quality (ties broken by larger model first)\n",
    "cutoff = CUTOFF  # reuse existing threshold\n",
    "model_metrics = []\n",
    "for model, scores in score_distribution.items():\n",
    "    cq = composite_quality(scores, cutoff)\n",
    "    model_metrics.append({\n",
    "        \"model\": model,\n",
    "        \"mean\": round(mean(scores),2) if scores else 0,\n",
    "        \"p90\": round(sorted(scores)[math.floor(0.9*(len(scores)-1))],2) if scores else 0,\n",
    "        \"coverage\": round(sum(1 for s in scores if s >= cutoff)/len(scores),2) if scores else 0,\n",
    "        \"high_scores\": len([s for s in scores if s >= cutoff]),\n",
    "        \"n\": len(scores),\n",
    "        \"composite\": round(cq,3),\n",
    "        \"size_index\": SIZE_INDEX.get(model, 999)\n",
    "    })\n",
    "\n",
    "# Sort for display\n",
    "model_metrics.sort(key=lambda d: d['composite'], reverse=True)\n",
    "TEACHER_MODEL = model_metrics[0]['model'] if model_metrics else None\n",
    "\n",
    "# 5. Student selection: choose smallest model with >= MIN_STUDENT_SAMPLES high scores\n",
    "MIN_STUDENT_SAMPLES = 10\n",
    "\n",
    "eligible_students = []\n",
    "for m in HIGH_SCORES:\n",
    "    if m == TEACHER_MODEL:\n",
    "        continue\n",
    "    if m not in SIZE_INDEX or TEACHER_MODEL not in SIZE_INDEX:\n",
    "        continue\n",
    "    if SIZE_INDEX[m] <= SIZE_INDEX[TEACHER_MODEL]:  # must be strictly smaller\n",
    "        continue\n",
    "    high_count = len(HIGH_SCORES[m])\n",
    "    if high_count >= MIN_STUDENT_SAMPLES:\n",
    "        eligible_students.append((m, high_count))\n",
    "\n",
    "if eligible_students:\n",
    "    # Prefer smallest (highest size index), tie-break by more high samples\n",
    "    eligible_students.sort(key=lambda t: (SIZE_INDEX[t[0]], -t[1]))\n",
    "    STUDENT_MODEL = eligible_students[-1][0]\n",
    "else:\n",
    "    # Relax: pick smallest model with ANY high scores, else default nano\n",
    "    relaxed = [(m, len(HIGH_SCORES[m])) for m in HIGH_SCORES \n",
    "               if m != TEACHER_MODEL and m in SIZE_INDEX and SIZE_INDEX[m] > SIZE_INDEX[TEACHER_MODEL] and len(HIGH_SCORES[m]) > 0]\n",
    "    if relaxed:\n",
    "        relaxed.sort(key=lambda t: (SIZE_INDEX[t[0]], -t[1]))\n",
    "        STUDENT_MODEL = relaxed[-1][0]\n",
    "    else:\n",
    "        STUDENT_MODEL = \"gpt-4.1-nano\"\n",
    "\n",
    "print(\"=== Model Composite Metrics ===\")\n",
    "for mm in model_metrics:\n",
    "    star = \"<- TEACHER\" if mm['model'] == TEACHER_MODEL else \"\"\n",
    "    print(f\"{mm['model']:12s} comp={mm['composite']:5.2f} mean={mm['mean']:4.2f} p90={mm['p90']:4.2f} cov={mm['coverage']:4.2f} hi={mm['high_scores']:3d}/{mm['n']:3d} {star}\")\n",
    "\n",
    "print(f\"\\nSelected Teacher: {TEACHER_MODEL}\")\n",
    "print(f\"Selected Student: {STUDENT_MODEL} (high samples: {len(HIGH_SCORES[STUDENT_MODEL])})\")\n",
    "\n",
    "# 6. (Optional) Raise cutoff adaptively if too many highs (all models >90% coverage)\n",
    "if all(mm['coverage'] > 0.9 for mm in model_metrics if mm['n'] > 0):\n",
    "    print(\"‚ö†Ô∏è Coverage very high across all models ‚Äî consider increasing CUTOFF to better separate quality.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e6fef4",
   "metadata": {},
   "source": [
    "### 2.2 Create Our Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38531244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we go any further, let's turn our collected excellent responses into our\n",
    "# training and validation fine-tuning datasets. Like before, we have to write these\n",
    "# to disk and then upload them via the Files API.\n",
    "training_filename = f\"zava-tone-training-{UNIQUE_ENOUGH_KEY}.jsonl\"\n",
    "validation_filename = f\"zava-tone-validation-{UNIQUE_ENOUGH_KEY}.jsonl\"\n",
    "\n",
    "# Make an 80/20 split to form our training/validation data.\n",
    "split_at = int(len(HIGH_SCORES[TEACHER_MODEL]) * 0.80)\n",
    "training_data = HIGH_SCORES[TEACHER_MODEL][:split_at]\n",
    "validation_data = HIGH_SCORES[TEACHER_MODEL][split_at:]\n",
    "print(f\"Split into {len(training_data)} training / {len(validation_data)} validation rows.\")\n",
    "\n",
    "# Create and upload the training data.\n",
    "with open(training_filename, \"w\") as f:\n",
    "    for message in training_data:\n",
    "        json.dump(message, f)\n",
    "        f.write(\"\\n\")\n",
    "with open(training_filename, \"rb\") as f:\n",
    "    training_file = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "    training_file = client.files.wait_for_processing(training_file.id)\n",
    "print(f\"üèãÔ∏è‚Äç‚ôÇÔ∏è Created training file:\\n{training_file.to_json(indent=2)}\")\n",
    "\n",
    "# Create and upload the validation data.\n",
    "with open(validation_filename, \"w\") as f:\n",
    "    for message in validation_data:\n",
    "        json.dump(message, f)\n",
    "        f.write(\"\\n\")\n",
    "with open(validation_filename, \"rb\") as f:\n",
    "    validation_file = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "    validation_file = client.files.wait_for_processing(validation_file.id)\n",
    "print(f\"üìã Created validation file:\\n{validation_file.to_json(indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d13a2fb",
   "metadata": {},
   "source": [
    "### 2.3 Training The Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c79880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we start training! Submit our fine-tuning job to teach 4.1-nano new tricks.\n",
    "## NN-TOFIX: Check STUDENT MODEL name\n",
    "TEACHER_MODEL = TEACHER_MODEL\n",
    "STUDENT_MODEL = STUDENT_MODEL\n",
    "SUFFIX = f\"{TEACHER_MODEL}-zava-tone-{UNIQUE_ENOUGH_KEY}\".replace(\".\", \"\") # '.' is a reserved character üòú\n",
    "\n",
    "# OPTIMIZATION 4: Optimized hyperparameters for faster convergence\n",
    "job = client.fine_tuning.jobs.create(\n",
    "    model=STUDENT_MODEL,\n",
    "    suffix=SUFFIX,\n",
    "    training_file=training_file.id,\n",
    "    validation_file=validation_file.id,\n",
    "    extra_body={ \"trainingType\": \"globalstandard\" },\n",
    ")\n",
    "print(f\"üë®‚Äçüî¨ Created fine-tuning job:\\n{job.to_json(indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71fc867",
   "metadata": {},
   "source": [
    "### 2.4 Wait For Fine Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd907ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZATION 1: Add early stopping and progress monitoring to speed up training\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "last_loss = None\n",
    "patience_counter = 0\n",
    "PATIENCE = 3  # Stop if no improvement for 3 checks\n",
    "\n",
    "status = job.status\n",
    "while status not in [\"succeeded\", \"failed\", \"cancelled\"]:\n",
    "    time.sleep(10)\n",
    "    job = client.fine_tuning.jobs.retrieve(job.id)\n",
    "    status = job.status\n",
    "    \n",
    "    # OPTIMIZATION 2: Monitor training metrics for early stopping\n",
    "    try:\n",
    "        events = client.fine_tuning.jobs.list_events(job.id, limit=5)\n",
    "        if events.data:\n",
    "            latest_event = events.data[0]\n",
    "            if hasattr(latest_event, 'data') and 'train_loss' in latest_event.data:\n",
    "                current_loss = latest_event.data['train_loss']\n",
    "                if last_loss and current_loss >= last_loss:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= PATIENCE:\n",
    "                        print(f\"‚ö†Ô∏è Early stopping: Loss plateau detected\")\n",
    "                        # Could implement job cancellation here if needed\n",
    "                else:\n",
    "                    patience_counter = 0\n",
    "                last_loss = current_loss\n",
    "                print(f\"üìà Latest train_loss: {current_loss:.4f}\")\n",
    "    except Exception as e:\n",
    "        pass  # Continue without early stopping if events unavailable\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(f\"üë®‚Äçüî¨ Job {job.id}: {status}\")\n",
    "    print(\"‚è±Ô∏è Elapsed time: {} minutes {} seconds\".format(int((time.time() - start_time) // 60), int((time.time() - start_time) % 60)))\n",
    "\n",
    "if status == \"succeeded\":\n",
    "    print(f\"üèÅ Fine-tuning finished!\")\n",
    "else:\n",
    "    raise RuntimeError(f\"Fine-tuning job did not complete successfully (status={status})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d108c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Judging The Distilled Student"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224127cf",
   "metadata": {},
   "source": [
    "### 3.1 Deploy Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca89054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to deploy our fine-tuned model. We'll use Developer Tier to keep\n",
    "# costs under control for evaluation.\n",
    "\n",
    "# We can't do this with the OpenAI SDK, so we need to reach for the Azure SDK.\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient\n",
    "\n",
    "cogsvc_client = CognitiveServicesManagementClient(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    subscription_id=os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\n",
    ")\n",
    "\n",
    "# Define our Deployment. Note the use of SKU for specificy capacity and\n",
    "# the name of the deployment tier.\n",
    "DEPLOYMENT_NAME = f\"zava-tone-distilled-{SUFFIX}\"\n",
    "DEPLOYMENT = {\n",
    "    \"properties\": {\n",
    "        \"model\": { \n",
    "            \"format\": \"OpenAI\", \n",
    "            \"name\": job.fine_tuned_model, \n",
    "            \"version\": \"1\" \n",
    "        },\n",
    "    },\n",
    "    \"sku\": { \n",
    "        \"capacity\": 250, \n",
    "        \"name\": \"DeveloperTier\" \n",
    "    },\n",
    "}\n",
    "\n",
    "# Submit the request for provisioning. This may take a few minutes, so we\n",
    "# poll for updates. If it already exists, this should return quickly. Since\n",
    "# we're deploying a 4.1-nano model, this should only take 3-5 minutes tops.\n",
    "deployment = cogsvc_client.deployments.begin_create_or_update(\n",
    "    resource_group_name=os.environ.get(\"AZURE_RESOURCE_GROUP\"),\n",
    "    account_name=os.environ.get(\"AZURE_AI_FOUNDRY_NAME\"),\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    deployment=DEPLOYMENT,\n",
    ")\n",
    "print(f\"üõ≥Ô∏è Submitted deployment {deployment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d950f0",
   "metadata": {},
   "source": [
    "### 3.2 Wait Till Deployment Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88230a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for our deployment to finish provisioning.\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "status = deployment.status()\n",
    "\n",
    "while status not in [\"Succeeded\", \"Failed\"]:\n",
    "    deployment.wait(5)\n",
    "    status = deployment.status()\n",
    "    clear_output(wait=True)\n",
    "    print(f\"üõ≥Ô∏è Provisioning {DEPLOYMENT_NAME}: {status}\")\n",
    "    print(\"‚è±Ô∏èElapsed time: {} minutes {} seconds\".format(int((time.time() - start_time) // 60), int((time.time() - start_time) % 60)))\n",
    "\n",
    "print(f\"üèÅ Provisioning finished!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b83dd8a",
   "metadata": {},
   "source": [
    "### 3.3 Upload Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b687d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll upload our post-training validation dataset and prepare our final Evaluation.\n",
    "# We need to save the data to disk first, again for...reasons.\n",
    "filename = f\"./zava-tone-posttraining-{UNIQUE_ENOUGH_KEY}.jsonl\"\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    for row in qa_validation:\n",
    "        json.dump(row, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "posttraining_file = None\n",
    "with open(filename, \"rb\") as f:\n",
    "    posttraining_file = client.files.create(purpose=\"evals\", file=f)\n",
    "    posttraining_file = client.files.wait_for_processing(posttraining_file.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f216611d",
   "metadata": {},
   "source": [
    "### 3.4 Evaluate Student vs. Peers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05976b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define our Evaluation for validating our choice in grader prompt and model.\n",
    "\n",
    "# The entire user prompt is data driven from the file. No generation is done using\n",
    "# a model in this case, just simple string substitution using this pattern. This\n",
    "# means we directly reference the two fields in our baseline.jsonl file.\n",
    "\n",
    "USER_PROMPT = \"\"\"\n",
    "Q: {{item.question}}\n",
    "A: {{item.answer}}\n",
    "\"\"\"\n",
    "\n",
    "INPUT = [\n",
    "    {\n",
    "        \"type\": \"message\",\n",
    "        \"role\": \"system\",\n",
    "        \"content\": { \"type\": \"input_text\", \"text\": GRADER_PROMPT }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"message\",\n",
    "        \"role\": \"user\",\n",
    "        \"content\": { \"type\": \"input_text\", \"text\": USER_PROMPT }\n",
    "    }\n",
    "]\n",
    "\n",
    "# We need to describe what our evaluation dataset looks like.\n",
    "SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"question\": { \"type\": \"string\" },\n",
    "        \"answer\": { \"type\": \"string\" },\n",
    "    }\n",
    "}\n",
    "DATA_SOURCE = {\n",
    "    \"item_schema\": SCHEMA,\n",
    "    \"include_sample_schema\": False,\n",
    "    \"type\": \"custom\",\n",
    "}\n",
    "\n",
    "# Lastly, we define test criteria that combines all the above.\n",
    "TESTING_CRITERIA = {\n",
    "    \"name\": \"Zava Tone Grader\",\n",
    "    \"type\": \"score_model\",\n",
    "    \"model\": GRADER_MODEL,\n",
    "    \"input\": INPUT,\n",
    "    \"range\": [1.0, 10.0],    # Our grader scores in a range from 1 to 10\n",
    "    \"pass_threshold\": 4.0,   # Let's say a 4 is \"passing\" for now.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ac102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create a final Eval using our post-training dataset that doesn't overlap with the\n",
    "# original distillation and training dataset. This lets us judge our new model based on\n",
    "# data it hasn't seen before. We'll also through in one of our better performing base\n",
    "# models as a control.\n",
    "POST_EVAL_MODELS = [\n",
    "    DEPLOYMENT_NAME,# distilled\n",
    "    \"gpt-4.1-nano\", # student\n",
    "    \"gpt-4.1\",      # control\n",
    "]\n",
    "\n",
    "# SCHEMA, GRADER_MODEL, and INPUT are re-used from our previous Evaluation definition,\n",
    "# but let's restate the source and testing criteria again.\n",
    "DATA_SOURCE = {\n",
    "    \"item_schema\": SCHEMA, \n",
    "    \"include_sample_schema\": True, # Note this change! Needed for data gen.\n",
    "    \"type\": \"custom\"\n",
    "}\n",
    "# OPTIMIZATION 3: Multi-grader validation to reduce bias\n",
    "TESTING_CRITERIA = [\n",
    "    {\n",
    "        \"name\": \"Zava Tone Grader (Primary)\",\n",
    "        \"type\": \"score_model\", \n",
    "        \"model\": GRADER_MODEL,\n",
    "        \"input\": INPUT,\n",
    "        \"range\": [1.0, 10.0],\n",
    "        \"pass_threshold\": 4.0,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Consistency Check (Secondary)\",\n",
    "        \"type\": \"score_model\",\n",
    "        \"model\": \"gpt-4o\",  # Different model for cross-validation\n",
    "        \"input\": INPUT,\n",
    "        \"range\": [1.0, 10.0], \n",
    "        \"pass_threshold\": 4.0,\n",
    "    }\n",
    "]\n",
    "posttraining_eval = client.evals.create(\n",
    "    name=f\"zava-tone-posttrain-evaluation-{UNIQUE_ENOUGH_KEY}\",\n",
    "    data_source_config=DATA_SOURCE,\n",
    "    testing_criteria=TESTING_CRITERIA  # Now using list of criteria\n",
    ")\n",
    "print(f\"Created eval {posttraining_eval.id}\")\n",
    "\n",
    "# Now add our runs.\n",
    "postraining_runs = []\n",
    "for model in POST_EVAL_MODELS:\n",
    "    RUN_DATA_SOURCE = {\n",
    "        \"type\": \"completions\",\n",
    "        \"model\": model,\n",
    "        \"source\": { \"type\": \"file_id\", \"id\": posttraining_file.id },\n",
    "        \"input_messages\": {\n",
    "            \"type\": \"template\",\n",
    "            \"template\": [\n",
    "                { \n",
    "                    \"type\": \"message\", \n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": { \"type\": \"input_text\", \"text\": SYSTEM_PROMPT },\n",
    "                },\n",
    "                { \n",
    "                    \"type\": \"message\", \n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": { \"type\": \"input_text\", \"text\": \"{{item.question}}\" },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        \"sampling_params\": { \"max_completions_tokens\": 100 }, # XXX again, note the purposeful typo\n",
    "    }\n",
    "    run = client.evals.runs.create(\n",
    "        name=f\"{model}-{UNIQUE_ENOUGH_KEY}\", \n",
    "        eval_id=posttraining_eval.id,\n",
    "        data_source=RUN_DATA_SOURCE, \n",
    "    )\n",
    "    print(f\"üèÉ‚Äç‚û°Ô∏è Created run {run.id} for {posttraining_eval.id}\")\n",
    "    postraining_runs.append(run)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1a5347",
   "metadata": {},
   "source": [
    "### 3.5 Wait For Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf13c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we wait for our runs to finish.\n",
    "start_time = time.time()\n",
    "\n",
    "while any([r.status not in [\"completed\", \"failed\"] for r in postraining_runs]):\n",
    "    time.sleep(10)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    for i in range(len(postraining_runs)):\n",
    "        postraining_runs[i] = client.evals.runs.retrieve(eval_id=posttraining_eval.id, run_id=postraining_runs[i].id)\n",
    "        print(f\"üèÉ‚Äç‚û°Ô∏è Run {postraining_runs[i].name}: {postraining_runs[i].status}\")\n",
    "    \n",
    "    now = time.time()\n",
    "    print(\"‚è±Ô∏è Elapsed time: {} minutes {} seconds\".format(int((now - start_time) // 60), int((now - start_time) % 60)))\n",
    "\n",
    "print(f\"üèÅ All {len(postraining_runs)} runs completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f20554b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Interpret The Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"eval_utils\", \"eval_utils.py\")\n",
    "eval_utils = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(eval_utils)\n",
    "display_evaluation_summary = eval_utils.display_evaluation_summary\n",
    "\n",
    "display_evaluation_summary(client, [posttraining_eval.id], x_range=(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1ebaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now let's put it all together.\n",
    "# Let's visualize our post-training evaluation. Fingers crossed!\n",
    "display_evaluation_summary(client, [baseline_eval.id, posttraining_eval.id], x_range=(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a6cbb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Teardown\n",
    "\n",
    "Don't forget to clean up - delete the resource group, purge resources!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e5aec1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Summary \n",
    "\n",
    "In this notebook we demonstrated **distillation** using Azure OpenAI *Evaluations*\n",
    "and *Fine Tuning* features.\n",
    "\n",
    "We used an objective of *adjusting the tone* of a model to meet our needs, in this\n",
    "case making its responses sarcastic, while preserving accuracy in results, and\n",
    "*distilled* the native capabilities of a state-of-the-art reasoning model (`o3-mini`)\n",
    "into a much smaller, non-reasoning model (`4.1-nano`) to let our agent or app\n",
    "use the smallest model possible while:\n",
    "\n",
    "- ü§ë minimizing per-token costs\n",
    "- üèéÔ∏è improve performance (latency)\n",
    "\n",
    "We did all this:\n",
    "\n",
    "- without creating training data directly\n",
    "- without knowing the ideal student model\n",
    "- only by knowing how to define our Grader\n",
    "\n",
    "So to wrap it all up:\n",
    "\n",
    "1. We described the ideal state to our complex reasoning model in the form of\n",
    "   a few samples we feel are ideal.\n",
    "2. We described to the reasoning model (grader) how to judge those examples\n",
    "   to measure their quality.\n",
    "3. We let Evaluations and Fine Tuning do the rest!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197dc0d6",
   "metadata": {},
   "source": [
    "### üéØ Success Criteria:\n",
    "- Function tests 4 different reasoning types\n",
    "- Returns score 0.0-1.0 for generalization retention  \n",
    "- Can identify when model only gives Zava hardware responses\n",
    "- Uses ‚â§10 lines of core implementation logic\n",
    "\n",
    "**Ready to code? Or need a hint about the approach?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
