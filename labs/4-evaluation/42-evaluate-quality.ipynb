{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a57b6e53",
   "metadata": {},
   "source": [
    "# Lab 02: Explore Built-in Quality Evaluators\n",
    "\n",
    "By the end of this lab, you will know:\n",
    "\n",
    "1. What AI-Assisted evaluation workflows are, and how to run them.\n",
    "1. The built-in quality evaluators available in Azure AI Foundry\n",
    "1. How to run a quality evaluator with a test prompt (to understand usage)\n",
    "1. How to run a composite quality evaluator (with multiple evaluators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5c2dc7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Create Model Configuration Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdc24cd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Authenticate with Azure\n",
    "\n",
    "To use the Azure AI evaluation SDK, you need to authenticate with Azure. The SDK uses the Azure Identity library to handle authentication. In this lab, we will use the `DefaultAzureCredential` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fff88947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"@odata.context\": \"https://graph.microsoft.com/v1.0/$metadata#users/$entity\",\n",
      "  \"businessPhones\": [],\n",
      "  \"displayName\": \"ninarasi (BAMI)\",\n",
      "  \"givenName\": null,\n",
      "  \"id\": \"87416935-d1b4-47d6-a7a9-afe45d916ece\",\n",
      "  \"jobTitle\": null,\n",
      "  \"mail\": \"ninarasi@caai2601.onmicrosoft.com\",\n",
      "  \"mobilePhone\": null,\n",
      "  \"officeLocation\": null,\n",
      "  \"preferredLanguage\": null,\n",
      "  \"surname\": null,\n",
      "  \"userPrincipalName\": \"ninarasi@caai2601.onmicrosoft.com\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Verify that you are authenticated\n",
    "!az ad signed-in-user show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c917b7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<azure.identity._credentials.default.DefaultAzureCredential object at 0x785a9d45d040>\n"
     ]
    }
   ],
   "source": [
    "# Generate a default credential\n",
    "from azure.identity import DefaultAzureCredential\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Check: credential created\n",
    "from pprint import pprint\n",
    "pprint(credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77126cd6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Create the Azure AI Project Configuration\n",
    "\n",
    "We need to create the Azure AI Project configuration that will be used to upload evaluation results to the Azure AI Foundry portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7427de64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure AI Project Configuration:\n",
      "{'project_name': 'proj-4v6jkpb4qesje',\n",
      " 'resource_group_name': 'rg-nitya-ignite-PREL13',\n",
      " 'subscription_id': '7a880728-70d3-49d0-adde-4250716cfd94'}\n",
      "\n",
      "Azure AI Project URL: https://aoai-4v6jkpb4qesje.services.ai.azure.com/api/projects/proj-4v6jkpb4qesje\n"
     ]
    }
   ],
   "source": [
    "# Get Azure AI project configuration from environment variables\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "subscription_id = os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\n",
    "resource_group_name = os.environ.get(\"AZURE_RESOURCE_GROUP\")\n",
    "project_name = os.environ.get(\"AZURE_AI_PROJECT_NAME\")\n",
    "azure_ai_foundry_name = os.environ.get(\"AZURE_AI_FOUNDRY_NAME\")\n",
    "\n",
    "# Create the azure_ai_project dictionary (used by some evaluators)\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": subscription_id,\n",
    "    \"resource_group_name\": resource_group_name,\n",
    "    \"project_name\": project_name,\n",
    "}\n",
    "\n",
    "# Create the azure_ai_project_url (used by ContentSafetyEvaluator)\n",
    "azure_ai_project_url = f\"https://{azure_ai_foundry_name}.services.ai.azure.com/api/projects/{project_name}\"\n",
    "\n",
    "print(\"Azure AI Project Configuration:\")\n",
    "pprint(azure_ai_project)\n",
    "print(f\"\\nAzure AI Project URL: {azure_ai_project_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration for AI-assisted evaluators\n",
    "# in Foundry projects\n",
    "\n",
    "import os\n",
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a518b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Basic Quality Evaluators\n",
    "\n",
    "Scores are typically numerical, generated using a Likert scale (1 to 5) with higher scores indicating better quality. The _threshold_ sets the cutoff for a \"pass/fail\" rating on that evaluator, helping you get a quick sense of where the primary issues lie.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9514f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coherence': 4.0,\n",
       " 'gpt_coherence': 4.0,\n",
       " 'coherence_reason': 'The response is coherent, directly addresses the question, and presents information in a logical and clear manner, though it is concise.',\n",
       " 'coherence_result': 'pass',\n",
       " 'coherence_threshold': 3}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.1 Coherence\n",
    "# ............\n",
    "# CoherenceEvaluator measures the logical and orderly presentation of ideas in a response, \n",
    "# allowing the reader to easily follow and understand the writer's train of thought. \n",
    "# A coherent response directly addresses the question with clear connections between \n",
    "# sentences and paragraphs, using appropriate transitions and a logical sequence of ideas. \n",
    "# Higher scores mean better coherence.\n",
    "\n",
    "from azure.ai.evaluation import CoherenceEvaluator\n",
    "\n",
    "coherence = CoherenceEvaluator(model_config=model_config, threshold=3)\n",
    "coherence(\n",
    "    query=\"Is Marie Curie is born in Paris?\", \n",
    "    response=\"No, Marie Curie is born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a139730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fluency': 3.0,\n",
       " 'gpt_fluency': 3.0,\n",
       " 'fluency_reason': 'The response communicates a clear idea with a minor grammatical error and limited vocabulary, fitting the definition of Competent Fluency.',\n",
       " 'fluency_result': 'pass',\n",
       " 'fluency_threshold': 3}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.2 Fluency\n",
    "# ..........\n",
    "# FluencyEvaluator measures the effectiveness and clarity of written communication, \n",
    "# focusing on grammatical accuracy, vocabulary range, sentence complexity, coherence, \n",
    "# and overall readability. It assesses how smoothly ideas are conveyed and how easily \n",
    "# the reader can understand the text.\n",
    "\n",
    "from azure.ai.evaluation import FluencyEvaluator\n",
    "\n",
    "fluency = FluencyEvaluator(model_config=model_config, threshold=3)\n",
    "fluency(\n",
    "    response=\"No, Marie Curie is born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a17578",
   "metadata": {},
   "source": [
    "## 5. Composite QA Evaluator\n",
    "QAEvaluator measures comprehensively various aspects in a question-answering scenario - including Relevance, Groundedness, Fluency, Coherence, Similarity, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ea81a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_score': 0.631578947368421,\n",
       " 'f1_result': 'pass',\n",
       " 'f1_threshold': 3,\n",
       " 'similarity': 5.0,\n",
       " 'gpt_similarity': 5.0,\n",
       " 'similarity_result': 'pass',\n",
       " 'similarity_threshold': 3,\n",
       " 'groundedness': 3.0,\n",
       " 'gpt_groundedness': 3.0,\n",
       " 'groundedness_reason': 'The response gives a location (Warsaw) not found in the context, so it is not grounded in the provided information and includes unsupported details.',\n",
       " 'groundedness_result': 'pass',\n",
       " 'groundedness_threshold': 3,\n",
       " 'coherence': 4.0,\n",
       " 'gpt_coherence': 4.0,\n",
       " 'coherence_reason': 'The response is coherent, logically organized, and clearly answers the question with appropriate context and flow.',\n",
       " 'coherence_result': 'pass',\n",
       " 'coherence_threshold': 3,\n",
       " 'relevance': 4.0,\n",
       " 'gpt_relevance': 4.0,\n",
       " 'relevance_reason': 'The response directly answers the query by stating Marie Curie was born in Warsaw, and clarifies she was not born in Paris. It is accurate and sufficiently informative, though it does not add extra context or insight.',\n",
       " 'relevance_result': 'pass',\n",
       " 'relevance_threshold': 3,\n",
       " 'fluency': 3.0,\n",
       " 'gpt_fluency': 3.0,\n",
       " 'fluency_reason': 'The response is clear and correct, but it is simple and lacks complexity or advanced vocabulary.',\n",
       " 'fluency_result': 'pass',\n",
       " 'fluency_threshold': 3}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.3 QA Evaluation\n",
    "# .................\n",
    "# QAEvaluator measures comprehensively various aspects \n",
    "# in a question-answering scenario:\n",
    "# Relevance / Groundedness / Fluency / Coherence\n",
    "# Similarity / F1 score\n",
    "\n",
    "from azure.ai.evaluation import QAEvaluator\n",
    "\n",
    "qa_eval = QAEvaluator(model_config=model_config, threshold=3)\n",
    "qa_eval(\n",
    "    query=\"Where was Marie Curie born?\", \n",
    "    context=\"Background: 1. Marie Curie was a chemist. 2. Marie Curie was born on November 7, 1867. 3. Marie Curie is a French scientist.\",\n",
    "    response=\"According to wikipedia, Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie was born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd71b234",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6.  Retrieval Augmented Generation (RAG) Evaluators\n",
    "\n",
    "A retrieval-augmented generation (RAG) system tries to generate the most relevant answer consistent with grounding documents in response to a user's query.  This requires it to _retrieve_ documents that provide grounding context, and _generate_ responses that are relevance, consistent with grounding data, and complete.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a2c47f",
   "metadata": {},
   "source": [
    "### 6.1 Retrieval Evaluator\n",
    "RetrievalEvaluator measures the textual quality of retrieval results with an LLM without requiring ground truth. This metric focuses on how relevant the context chunks (encoded as a string) are to address a query and how the most relevant context chunks are surfaced at the top of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65955c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'retrieval': 5.0,\n",
       " 'gpt_retrieval': 5.0,\n",
       " 'retrieval_reason': 'The most relevant context chunk is at the top and directly answers the query. The retrieval is highly relevant and well ranked.',\n",
       " 'retrieval_result': 'pass',\n",
       " 'retrieval_threshold': 3}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.evaluation import RetrievalEvaluator\n",
    "\n",
    "retrieval = RetrievalEvaluator(model_config=model_config, threshold=3)\n",
    "retrieval(\n",
    "    query=\"Where was Marie Curie born?\", \n",
    "    context=\"Background: 1. Marie Curie was born in Warsaw. 2. Marie Curie was born on November 7, 1867. 3. Marie Curie is a French scientist. \",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c920eff3",
   "metadata": {},
   "source": [
    "\n",
    "### 6.2 Groundedness Evaluator \n",
    "GroundednessEvaluator measures how well the generated response aligns with the given context (grounding source) and doesn't fabricate content outside of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eda150a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'groundedness': 5.0,\n",
       " 'gpt_groundedness': 5.0,\n",
       " 'groundedness_reason': 'The response is fully accurate, complete, and directly grounded in the provided context.',\n",
       " 'groundedness_result': 'pass',\n",
       " 'groundedness_threshold': 3}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.evaluation import GroundednessEvaluator\n",
    "\n",
    "groundedness = GroundednessEvaluator(model_config=model_config, threshold=3)\n",
    "groundedness(\n",
    "    query=\"Is Marie Curie is born in Paris?\", \n",
    "    context=\"Background: 1. Marie Curie is born on November 7, 1867. 2. Marie Curie is born in Warsaw.\",\n",
    "    response=\"No, Marie Curie is born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f433c969",
   "metadata": {},
   "source": [
    "### 6.3 Relevance Evaluator\n",
    "\n",
    "RelevanceEvaluator measures how effectively a response addresses a query. It assesses the accuracy, completeness, and direct relevance of the response based solely on the given query. Higher scores mean better relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44116930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relevance': 4.0,\n",
       " 'gpt_relevance': 4.0,\n",
       " 'relevance_reason': 'The RESPONSE accurately answers the QUERY by stating that Marie Curie was born in Warsaw, which is correct and directly relevant to the question asked.',\n",
       " 'relevance_result': 'pass',\n",
       " 'relevance_threshold': 3}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"relevance\": 4.0,\n",
    "    \"gpt_relevance\": 4.0, \n",
    "    \"relevance_reason\": \"The RESPONSE accurately answers the QUERY by stating that Marie Curie was born in Warsaw, which is correct and directly relevant to the question asked.\",\n",
    "    \"relevance_result\": \"pass\", \n",
    "    \"relevance_threshold\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da91363c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coherence': 4.0,\n",
      " 'coherence_reason': 'The response is coherent, logically organized, and '\n",
      "                     'directly answers the question in a clear manner.',\n",
      " 'coherence_result': 'pass',\n",
      " 'coherence_threshold': 3,\n",
      " 'gpt_coherence': 4.0}\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import CoherenceEvaluator\n",
    "coherence_evaluator = CoherenceEvaluator(model_config)\n",
    "\n",
    "result = coherence_evaluator(\n",
    "    query=\"What is the capital of Japan?\",\n",
    "    response=\"The capital of Japan is Tokyo.\"\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2179ed8f",
   "metadata": {},
   "source": [
    "### 6.4 Response Completeness Evaluator\n",
    "\n",
    "ResponseCompletenessEvaluator that captures the recall aspect of response alignment with the expected response. This is complementary to GroundednessEvaluator which captures the precision aspect of response alignment with the grounding source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25a3b14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class ResponseCompletenessEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'response_completeness': 1,\n",
       " 'response_completeness_result': 'fail',\n",
       " 'response_completeness_threshold': 3,\n",
       " 'response_completeness_reason': \"The response completely misses the key information from the ground truth and does not include any relevant statements about the CEO's compensation package.\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.evaluation import ResponseCompletenessEvaluator\n",
    "\n",
    "response_completeness = ResponseCompletenessEvaluator(model_config=model_config, threshold=3)\n",
    "response_completeness(\n",
    "    response=\"Based on the retrieved documents, the shareholder meeting discussed the operational efficiency of the company and financing options.\",\n",
    "    ground_truth=\"The shareholder meeting discussed the compensation package of the company CEO.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56667c74",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Textual Similarity Evaluators\n",
    "\n",
    "\n",
    "These evaluators compare how closely the textual response generated by your AI system matches the response you would expect, typically called the \"ground truth\".\n",
    "- The SimilarityEvaluator uses an \"LLM-as-Judge\" (AI-assisted evaluation) approach to score the metric.\n",
    "- The F1 Score, BLEU, GLEU, ROUGE and METEOR evaluators (NLP-based) use a mathematical approach to score the metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1872f9",
   "metadata": {},
   "source": [
    "### 7.1 Similarity Evaluator\n",
    "SimilarityEvaluator measures the degrees of semantic similarity between the generated text and its ground truth with respect to a query. Compared to other text-similarity metrics that require ground truths, this metric focuses on semantics of a response (instead of simple overlap in tokens or n-grams) and also considers the broader context of a query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe271d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'similarity': 5.0,\n",
       " 'gpt_similarity': 5.0,\n",
       " 'similarity_result': 'pass',\n",
       " 'similarity_threshold': 3}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.evaluation import SimilarityEvaluator\n",
    "\n",
    "similarity = SimilarityEvaluator(model_config=model_config, threshold=3)\n",
    "similarity(\n",
    "    query=\"Is Marie Curie is born in Paris?\", \n",
    "    response=\"According to wikipedia, Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie was born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1e7260",
   "metadata": {},
   "source": [
    "### 7.2 F1 Score\n",
    "F1ScoreEvaluator measures the similarity by shared tokens between the generated text and the ground truth, focusing on both precision and recall. The F1-score computes the ratio of the number of shared words between the model generation and the ground truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "588f292f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_score': 0.631578947368421, 'f1_result': 'pass', 'f1_threshold': 0.5}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.evaluation import F1ScoreEvaluator\n",
    "\n",
    "f1_score = F1ScoreEvaluator(threshold=0.5)\n",
    "f1_score(\n",
    "    response=\"According to wikipedia, Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie was born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16081012",
   "metadata": {},
   "source": [
    "### 7.3 BLEU Score\n",
    "BleuScoreEvaluator computes the BLEU (Bilingual Evaluation Understudy) score commonly used in natural language processing (NLP) and machine translation. It measures how closely the generated text matches the reference text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4580ed7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu_score': 0.1550967560878879,\n",
       " 'bleu_result': 'fail',\n",
       " 'bleu_threshold': 0.3}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.evaluation import BleuScoreEvaluator\n",
    "\n",
    "bleu_score = BleuScoreEvaluator(threshold=0.3)\n",
    "bleu_score(\n",
    "    response=\"According to wikipedia, Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie was born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd0a01",
   "metadata": {},
   "source": [
    "### 7.4 GLEU Score\n",
    "\n",
    "GleuScoreEvaluator computes the GLEU (Google-BLEU) score. It measures the similarity by shared n-grams between the generated text and ground truth, similar to the BLEU score, focusing on both precision and recall. But it addresses the drawbacks of the BLEU score using a per-sentence reward objective. The numerical score is a 0-1 float and a higher score is better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7159326d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gleu_score': 0.25925925925925924,\n",
       " 'gleu_result': 'pass',\n",
       " 'gleu_threshold': 0.2}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.evaluation import GleuScoreEvaluator\n",
    "\n",
    "\n",
    "gleu_score = GleuScoreEvaluator(threshold=0.2)\n",
    "gleu_score(\n",
    "    response=\"According to wikipedia, Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie was born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4477569f",
   "metadata": {},
   "source": [
    "### 7.5 ROUGE Score\n",
    "RougeScoreEvaluator computes the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores, a set of metrics used to evaluate automatic summarization and machine translation. It measures the overlap between generated text and reference summaries. The numerical score is a 0-1 float and a higher score is better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a12b716a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge_precision': 0.46153846153846156,\n",
       " 'rouge_recall': 1.0,\n",
       " 'rouge_f1_score': 0.631578947368421,\n",
       " 'rouge_precision_result': 'fail',\n",
       " 'rouge_recall_result': 'pass',\n",
       " 'rouge_f1_score_result': 'pass',\n",
       " 'rouge_precision_threshold': 0.6,\n",
       " 'rouge_recall_threshold': 0.5,\n",
       " 'rouge_f1_score_threshold': 0.55}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.evaluation import RougeScoreEvaluator, RougeType\n",
    "\n",
    "rouge = RougeScoreEvaluator(rouge_type=RougeType.ROUGE_L, precision_threshold=0.6, recall_threshold=0.5, f1_score_threshold=0.55) \n",
    "rouge(\n",
    "    response=\"According to wikipedia, Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie was born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62c3d9d",
   "metadata": {},
   "source": [
    "### 7.6 METEOR Score\n",
    "MeteorScoreEvaluator measures the similarity by shared n-grams between the generated text and the ground truth, similar to the BLEU score, focusing on precision and recall. But it addresses limitations of other metrics like the BLEU score by considering synonyms, stemming, and paraphrasing for content alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e971963d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meteor_score': 0.8621140763997908,\n",
       " 'meteor_result': 'fail',\n",
       " 'meteor_threshold': 0.9}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.evaluation import MeteorScoreEvaluator\n",
    "\n",
    "meteor_score = MeteorScoreEvaluator(threshold=0.9)\n",
    "meteor_score(\n",
    "    response=\"According to wikipedia, Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie was born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d223752",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Explore Custom Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9843c29",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 8.1 Code-Based Evaluator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5679169b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "{'score': False}\n",
      "{'score': True}\n"
     ]
    }
   ],
   "source": [
    "# Custom evaluator as a function to calculate response length\n",
    "def response_length(response, **kwargs):\n",
    "    return len(response)\n",
    "\n",
    "# Custom class based evaluator to check for blocked words\n",
    "class BlocklistEvaluator:\n",
    "    def __init__(self, blocklist):\n",
    "        self._blocklist = blocklist\n",
    "\n",
    "    def __call__(self, *, answer: str, **kwargs):\n",
    "        contains_block_word = any(word in answer for word in self._blocklist)\n",
    "        return {\"score\": contains_block_word}\n",
    "\n",
    "blocklist_evaluator = BlocklistEvaluator(blocklist=[\"bad\", \"worst\", \"terrible\"])\n",
    "\n",
    "# Test custom evaluator 1\n",
    "result = response_length(\"The capital of Japan is Tokyo.\")\n",
    "print(result)\n",
    "\n",
    "# Test custom evaluator 2\n",
    "result = blocklist_evaluator(answer=\"The capital of Japan is Tokyo.\")\n",
    "print(result)\n",
    "\n",
    "# Test custom evaluator 3\n",
    "result = blocklist_evaluator(answer=\"This is a bad idea.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0202d615",
   "metadata": {},
   "source": [
    "### 8.2 Prompt-Based Evaluator\n",
    "To build your own prompt-based large language model evaluator or AI-assisted annotator, you can create a custom evaluator based on a prompt template. [Learn more here](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-evaluators/custom-evaluators). You can also create a custom grader using the Azure OpenAI grader with custom prompts. [See example here](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/evaluation/azure-ai-evaluation/samples/aoai_score_model_grader_sample.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2107224d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Run Multiple Evaluators\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a70075c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    ContentSafetyEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    GroundednessEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    SimilarityEvaluator,\n",
    ")\n",
    "\n",
    "# Create evaluators\n",
    "# ContentSafetyEvaluator requires the azure_ai_project_url (string) format\n",
    "content_safety_evaluator = ContentSafetyEvaluator(azure_ai_project=azure_ai_project_url, credential=credential)\n",
    "\n",
    "# Other evaluators use the model_config\n",
    "relevance_evaluator = RelevanceEvaluator(model_config)\n",
    "coherence_evaluator = CoherenceEvaluator(model_config)\n",
    "groundedness_evaluator = GroundednessEvaluator(model_config)\n",
    "fluency_evaluator = FluencyEvaluator(model_config)\n",
    "similarity_evaluator = SimilarityEvaluator(model_config)\n",
    "\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"42-evaluate-quality.jsonl\",\n",
    "    evaluators={\n",
    "        \"content_safety\": content_safety_evaluator,\n",
    "        \"coherence\": coherence_evaluator,\n",
    "        \"relevance\": relevance_evaluator,\n",
    "        \"groundedness\": groundedness_evaluator,\n",
    "        \"fluency\": fluency_evaluator,\n",
    "        \"similarity\": similarity_evaluator,\n",
    "    },\n",
    "    evaluation_name=\"42-evaluate-quality\",\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"content_safety\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            }\n",
    "        },\n",
    "        \"coherence\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            }\n",
    "        },\n",
    "        \"groundedness\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.ground_truth}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        },\n",
    "        \"relevance\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        },\n",
    "        \"fluency\": {\n",
    "            \"column_mapping\": {\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        },\n",
    "        \"similarity\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\"\n",
    "            } \n",
    "        },\n",
    "    },\n",
    "\n",
    "    # Specify the azure_ai_project_url (string format) to push results to portal\n",
    "    azure_ai_project = azure_ai_project_url,\n",
    "    \n",
    "    # Specify the output path to push results also to local file\n",
    "    output_path=\"./42-evaluate-quality.results.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde8ca4",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### 9.1 View Results Online\n",
    "\n",
    "Just as before, you can now view the results of the multi-evaluator run using the Evaluation tab in the Azure AI Foundry Studio. Here is what you should see:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ce3da",
   "metadata": {},
   "source": [
    "#### Quality Evaluation\n",
    "\n",
    "![Quality](./../docs/img/screenshots/lab-02-portal-quality.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e146493",
   "metadata": {},
   "source": [
    "### 9.2 View Results Locally\n",
    "\n",
    "Just like before, you can see the results of the multi-evaluator run stored in a local json file at `02-quality-evaluators.results.json`. \n",
    "- Open the file in the VS Code editor\n",
    "- Right click and select \"Format Document\" to read the results better\n",
    "- Observe the metrics collected for each row - these are the multiple evaluators running on the same sample prompt/response pair\n",
    "- Scroll down to the end of the file to see the summary of the evaluation run - get a sense of the overall metrics for the run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686630d4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ | Congratulations!\n",
    "\n",
    "You have successfully completed the second lab in this module and got hands-on experience with a core subset of the the built-in quality evaluators. You also got a sense of how to create and run a custom evaluator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dceed0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
