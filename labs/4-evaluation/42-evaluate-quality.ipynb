{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a57b6e53",
   "metadata": {},
   "source": [
    "# üõçÔ∏è | Cora-For-Zava: Explore Quality Evaluators\n",
    "\n",
    "Welcome! This notebook helps you dive deep into quality evaluators for measuring AI response quality.\n",
    "\n",
    "## üõí Our Zava Scenario\n",
    "**Cora** is a customer service chatbot for **Zava** - a fictitious retailer of home improvement goods for DIY enthusiasts. Cora must provide accurate, relevant, and coherent responses about hardware and home improvement products to Zava customers. This notebook helps you assess response quality using specialized evaluators for groundedness, relevance, coherence, fluency, and similarity‚Äîensuring Cora meets the high standards expected in a production retail environment.\n",
    "\n",
    "## üéØ What You'll Build\n",
    "\n",
    "By the end of this notebook, you'll have:\n",
    "- ‚úÖ Learned what AI-assisted evaluation workflows are\n",
    "- ‚úÖ Explored the built-in quality evaluators in Azure AI Foundry\n",
    "- ‚úÖ Run individual quality evaluators with test prompts\n",
    "- ‚úÖ Used composite evaluators to assess multiple quality metrics\n",
    "- ‚úÖ Analyzed quality evaluation results\n",
    "\n",
    "## üí° What You'll Learn\n",
    "\n",
    "- What AI-assisted evaluation workflows are and how to run them\n",
    "- The built-in quality evaluators available in Azure AI Foundry\n",
    "- How to run quality evaluators on test data\n",
    "- How to use composite evaluators for comprehensive quality assessment\n",
    "- How to interpret quality metrics for your AI application\n",
    "\n",
    "Ready to evaluate quality? Let's get started! üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdc24cd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Authenticate with Azure\n",
    "\n",
    "To use the Azure AI evaluation SDK, you need to authenticate with Azure. The SDK uses the Azure Identity library to handle authentication. In this lab, we will use the `DefaultAzureCredential` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff88947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that you are authenticated\n",
    "!az ad signed-in-user show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c917b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a default credential\n",
    "from azure.identity import DefaultAzureCredential\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Check: credential created\n",
    "from pprint import pprint\n",
    "pprint(credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77126cd6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Create the Azure AI Project Configuration\n",
    "\n",
    "We need to create the Azure AI Project configuration that will be used to upload evaluation results to the Azure AI Foundry portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Azure AI project configuration from environment variables\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "subscription_id = os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\n",
    "resource_group_name = os.environ.get(\"AZURE_RESOURCE_GROUP\")\n",
    "project_name = os.environ.get(\"AZURE_AI_PROJECT_NAME\")\n",
    "azure_ai_foundry_name = os.environ.get(\"AZURE_AI_FOUNDRY_NAME\")\n",
    "\n",
    "# Create the azure_ai_project dictionary (used by some evaluators)\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": subscription_id,\n",
    "    \"resource_group_name\": resource_group_name,\n",
    "    \"project_name\": project_name,\n",
    "}\n",
    "\n",
    "# Create the azure_ai_project_url (used by ContentSafetyEvaluator)\n",
    "azure_ai_project_url = f\"https://{azure_ai_foundry_name}.services.ai.azure.com/api/projects/{project_name}\"\n",
    "\n",
    "print(\"Azure AI Project Configuration: Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration for AI-assisted evaluators\n",
    "# in Foundry projects\n",
    "\n",
    "import os\n",
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a518b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Basic Quality Evaluators\n",
    "\n",
    "Scores are typically numerical, generated using a Likert scale (1 to 5) with higher scores indicating better quality. The _threshold_ sets the cutoff for a \"pass/fail\" rating on that evaluator, helping you get a quick sense of where the primary issues lie.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9514f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Coherence\n",
    "# ............\n",
    "# CoherenceEvaluator measures the logical and orderly presentation of ideas in a response, \n",
    "# allowing the reader to easily follow and understand the writer's train of thought. \n",
    "# A coherent response directly addresses the question with clear connections between \n",
    "# sentences and paragraphs, using appropriate transitions and a logical sequence of ideas. \n",
    "# Higher scores mean better coherence.\n",
    "\n",
    "from azure.ai.evaluation import CoherenceEvaluator\n",
    "\n",
    "coherence = CoherenceEvaluator(model_config=model_config, threshold=3)\n",
    "coherence(\n",
    "    query=\"Do you sell hammers?\", \n",
    "    response=\"Yes, we carry hammers in our tool section. They're located in aisle 7.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a139730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Fluency\n",
    "# ..........\n",
    "# FluencyEvaluator measures the effectiveness and clarity of written communication, \n",
    "# focusing on grammatical accuracy, vocabulary range, sentence complexity, coherence, \n",
    "# and overall readability. It assesses how smoothly ideas are conveyed and how easily \n",
    "# the reader can understand the text.\n",
    "\n",
    "from azure.ai.evaluation import FluencyEvaluator\n",
    "\n",
    "fluency = FluencyEvaluator(model_config=model_config, threshold=3)\n",
    "fluency(\n",
    "    query=\"Do you have paint brushes?\",\n",
    "    response=\"We have paint brushes available in various sizes for your project needs.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a17578",
   "metadata": {},
   "source": [
    "## 5. Composite QA Evaluator\n",
    "QAEvaluator measures comprehensively various aspects in a question-answering scenario - including Relevance, Groundedness, Fluency, Coherence, Similarity, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea81a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 QA Evaluation\n",
    "# .................\n",
    "# QAEvaluator measures comprehensively various aspects \n",
    "# in a question-answering scenario:\n",
    "# Relevance / Groundedness / Fluency / Coherence\n",
    "# Similarity / F1 score\n",
    "\n",
    "from azure.ai.evaluation import QAEvaluator\n",
    "\n",
    "qa_eval = QAEvaluator(model_config=model_config, threshold=3)\n",
    "qa_eval(\n",
    "    query=\"What type of drill should I buy for woodworking?\", \n",
    "    context=\"Product Info: 1. We carry cordless drills ideal for woodworking. 2. Popular brands include DeWalt and Makita. 3. Prices range from $89 to $249.\",\n",
    "    response=\"For woodworking, I recommend our cordless drills. We have DeWalt and Makita models starting at $89.\",\n",
    "    ground_truth=\"Cordless drills from DeWalt or Makita are best for woodworking projects.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd71b234",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6.  Retrieval Augmented Generation (RAG) Evaluators\n",
    "\n",
    "A retrieval-augmented generation (RAG) system tries to generate the most relevant answer consistent with grounding documents in response to a user's query.  This requires it to _retrieve_ documents that provide grounding context, and _generate_ responses that are relevance, consistent with grounding data, and complete.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a2c47f",
   "metadata": {},
   "source": [
    "### 6.1 Retrieval Evaluator\n",
    "RetrievalEvaluator measures the textual quality of retrieval results with an LLM without requiring ground truth. This metric focuses on how relevant the context chunks (encoded as a string) are to address a query and how the most relevant context chunks are surfaced at the top of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65955c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import RetrievalEvaluator\n",
    "\n",
    "retrieval = RetrievalEvaluator(model_config=model_config, threshold=3)\n",
    "retrieval(\n",
    "    query=\"What paint finish works best for bathrooms?\", \n",
    "    context=\"Product Guide: 1. Semi-gloss paint is ideal for high-moisture areas. 2. Satin finish provides good moisture resistance. 3. Matte finish is not recommended for bathrooms.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c920eff3",
   "metadata": {},
   "source": [
    "\n",
    "### 6.2 Groundedness Evaluator \n",
    "GroundednessEvaluator measures how well the generated response aligns with the given context (grounding source) and doesn't fabricate content outside of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import GroundednessEvaluator\n",
    "\n",
    "groundedness = GroundednessEvaluator(model_config=model_config, threshold=3)\n",
    "groundedness(\n",
    "    query=\"Is this paint safe for children's rooms?\", \n",
    "    context=\"Product Details: 1. Our Interior Paint is low-VOC certified. 2. Safe for indoor use including nurseries. 3. Meets EPA safety standards.\",\n",
    "    response=\"Yes, this paint is low-VOC and safe for children's rooms.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f433c969",
   "metadata": {},
   "source": [
    "### 6.3 Relevance Evaluator\n",
    "\n",
    "RelevanceEvaluator measures how effectively a response addresses a query. It assesses the accuracy, completeness, and direct relevance of the response based solely on the given query. Higher scores mean better relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44116930",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"relevance\": 5.0,\n",
    "    \"gpt_relevance\": 5.0, \n",
    "    \"relevance_reason\": \"The response directly answers the query by confirming the paint is low-VOC and safe for children's rooms, which precisely addresses the customer's question about safety.\",\n",
    "    \"relevance_result\": \"pass\", \n",
    "    \"relevance_threshold\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da91363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import CoherenceEvaluator\n",
    "coherence_evaluator = CoherenceEvaluator(model_config)\n",
    "\n",
    "result = coherence_evaluator(\n",
    "    query=\"Which screwdriver set is best for DIY projects?\",\n",
    "    response=\"Our 20-piece precision screwdriver set is perfect for DIY work and includes both Phillips and flathead types.\"\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2179ed8f",
   "metadata": {},
   "source": [
    "### 6.4 Response Completeness Evaluator\n",
    "\n",
    "ResponseCompletenessEvaluator that captures the recall aspect of response alignment with the expected response. This is complementary to GroundednessEvaluator which captures the precision aspect of response alignment with the grounding source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a3b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import ResponseCompletenessEvaluator\n",
    "\n",
    "response_completeness = ResponseCompletenessEvaluator(model_config=model_config, threshold=3)\n",
    "response_completeness(\n",
    "    response=\"Our cordless drill has a 20V battery and weighs 4.5 pounds.\",\n",
    "    ground_truth=\"The drill features a 20V lithium battery, weighs 4.5 pounds, and includes a 2-year warranty.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56667c74",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Textual Similarity Evaluators\n",
    "\n",
    "\n",
    "These evaluators compare how closely the textual response generated by your AI system matches the response you would expect, typically called the \"ground truth\".\n",
    "- The SimilarityEvaluator uses an \"LLM-as-Judge\" (AI-assisted evaluation) approach to score the metric.\n",
    "- The F1 Score, BLEU, GLEU, ROUGE and METEOR evaluators (NLP-based) use a mathematical approach to score the metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1872f9",
   "metadata": {},
   "source": [
    "### 7.1 Similarity Evaluator\n",
    "SimilarityEvaluator measures the degrees of semantic similarity between the generated text and its ground truth with respect to a query. Compared to other text-similarity metrics that require ground truths, this metric focuses on semantics of a response (instead of simple overlap in tokens or n-grams) and also considers the broader context of a query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe271d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import SimilarityEvaluator\n",
    "\n",
    "similarity = SimilarityEvaluator(model_config=model_config, threshold=3)\n",
    "similarity(\n",
    "    query=\"Do you have exterior paint?\", \n",
    "    response=\"Yes, we stock weather-resistant exterior paint in multiple colors.\",\n",
    "    ground_truth=\"We carry exterior paint suitable for outdoor use.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1e7260",
   "metadata": {},
   "source": [
    "### 7.2 F1 Score\n",
    "F1ScoreEvaluator measures the similarity by shared tokens between the generated text and the ground truth, focusing on both precision and recall. The F1-score computes the ratio of the number of shared words between the model generation and the ground truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588f292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import F1ScoreEvaluator\n",
    "\n",
    "f1_score = F1ScoreEvaluator(threshold=0.5)\n",
    "f1_score(\n",
    "    response=\"We stock weather-resistant exterior paint in various colors.\",\n",
    "    ground_truth=\"We carry exterior paint suitable for outdoor use.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16081012",
   "metadata": {},
   "source": [
    "### 7.3 BLEU Score\n",
    "BleuScoreEvaluator computes the BLEU (Bilingual Evaluation Understudy) score commonly used in natural language processing (NLP) and machine translation. It measures how closely the generated text matches the reference text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4580ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import BleuScoreEvaluator\n",
    "\n",
    "bleu_score = BleuScoreEvaluator(threshold=0.3)\n",
    "bleu_score(\n",
    "    response=\"We stock weather-resistant exterior paint in various colors.\",\n",
    "    ground_truth=\"We carry exterior paint suitable for outdoor use.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd0a01",
   "metadata": {},
   "source": [
    "### 7.4 GLEU Score\n",
    "\n",
    "GleuScoreEvaluator computes the GLEU (Google-BLEU) score. It measures the similarity by shared n-grams between the generated text and ground truth, similar to the BLEU score, focusing on both precision and recall. But it addresses the drawbacks of the BLEU score using a per-sentence reward objective. The numerical score is a 0-1 float and a higher score is better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7159326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import GleuScoreEvaluator\n",
    "\n",
    "\n",
    "gleu_score = GleuScoreEvaluator(threshold=0.2)\n",
    "gleu_score(\n",
    "    response=\"We stock weather-resistant exterior paint in various colors.\",\n",
    "    ground_truth=\"We carry exterior paint suitable for outdoor use.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4477569f",
   "metadata": {},
   "source": [
    "### 7.5 ROUGE Score\n",
    "RougeScoreEvaluator computes the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores, a set of metrics used to evaluate automatic summarization and machine translation. It measures the overlap between generated text and reference summaries. The numerical score is a 0-1 float and a higher score is better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12b716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import RougeScoreEvaluator, RougeType\n",
    "\n",
    "rouge = RougeScoreEvaluator(rouge_type=RougeType.ROUGE_L, precision_threshold=0.6, recall_threshold=0.5, f1_score_threshold=0.55) \n",
    "rouge(\n",
    "    response=\"We stock weather-resistant exterior paint in various colors.\",\n",
    "    ground_truth=\"We carry exterior paint suitable for outdoor use.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62c3d9d",
   "metadata": {},
   "source": [
    "### 7.6 METEOR Score\n",
    "MeteorScoreEvaluator measures the similarity by shared n-grams between the generated text and the ground truth, similar to the BLEU score, focusing on precision and recall. But it addresses limitations of other metrics like the BLEU score by considering synonyms, stemming, and paraphrasing for content alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e971963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import MeteorScoreEvaluator\n",
    "\n",
    "meteor_score = MeteorScoreEvaluator(threshold=0.9)\n",
    "meteor_score(\n",
    "    response=\"We stock weather-resistant exterior paint in various colors.\",\n",
    "    ground_truth=\"We carry exterior paint suitable for outdoor use.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d223752",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Explore Custom Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9843c29",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 8.1 Code-Based Evaluator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5679169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom evaluator as a function to calculate response length\n",
    "def response_length(response, **kwargs):\n",
    "    return len(response)\n",
    "\n",
    "# Custom class based evaluator to check for blocked words\n",
    "class BlocklistEvaluator:\n",
    "    def __init__(self, blocklist):\n",
    "        self._blocklist = blocklist\n",
    "\n",
    "    def __call__(self, *, answer: str, **kwargs):\n",
    "        contains_block_word = any(word in answer for word in self._blocklist)\n",
    "        return {\"score\": contains_block_word}\n",
    "\n",
    "blocklist_evaluator = BlocklistEvaluator(blocklist=[\"unavailable\", \"discontinued\", \"out of stock\"])\n",
    "\n",
    "# Test custom evaluator 1\n",
    "result = response_length(\"Our cordless drill is perfect for home projects.\")\n",
    "print(result)\n",
    "\n",
    "# Test custom evaluator 2\n",
    "result = blocklist_evaluator(answer=\"This drill is available in our store.\")\n",
    "print(result)\n",
    "\n",
    "# Test custom evaluator 3\n",
    "result = blocklist_evaluator(answer=\"Sorry, that item is currently out of stock.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0202d615",
   "metadata": {},
   "source": [
    "### 8.2 Prompt-Based Evaluator\n",
    "To build your own prompt-based large language model evaluator or AI-assisted annotator, you can create a custom evaluator based on a prompt template. [Learn more here](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-evaluators/custom-evaluators). You can also create a custom grader using the Azure OpenAI grader with custom prompts. [See example here](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/evaluation/azure-ai-evaluation/samples/aoai_score_model_grader_sample.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2107224d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Run Multiple Evaluators\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70075c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    ContentSafetyEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    GroundednessEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    SimilarityEvaluator,\n",
    ")\n",
    "\n",
    "# Create evaluators\n",
    "# ContentSafetyEvaluator requires the azure_ai_project_url (string) format\n",
    "content_safety_evaluator = ContentSafetyEvaluator(azure_ai_project=azure_ai_project_url, credential=credential)\n",
    "\n",
    "# Other evaluators use the model_config\n",
    "relevance_evaluator = RelevanceEvaluator(model_config)\n",
    "coherence_evaluator = CoherenceEvaluator(model_config)\n",
    "groundedness_evaluator = GroundednessEvaluator(model_config)\n",
    "fluency_evaluator = FluencyEvaluator(model_config)\n",
    "similarity_evaluator = SimilarityEvaluator(model_config)\n",
    "\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"42-evaluate-quality.jsonl\",\n",
    "    evaluators={\n",
    "        \"content_safety\": content_safety_evaluator,\n",
    "        \"coherence\": coherence_evaluator,\n",
    "        \"relevance\": relevance_evaluator,\n",
    "        \"groundedness\": groundedness_evaluator,\n",
    "        \"fluency\": fluency_evaluator,\n",
    "        \"similarity\": similarity_evaluator,\n",
    "    },\n",
    "    evaluation_name=\"42-evaluate-quality\",\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"content_safety\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            }\n",
    "        },\n",
    "        \"coherence\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            }\n",
    "        },\n",
    "        \"groundedness\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.ground_truth}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        },\n",
    "        \"relevance\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        },\n",
    "        \"fluency\": {\n",
    "            \"column_mapping\": {\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        },\n",
    "        \"similarity\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\"\n",
    "            } \n",
    "        },\n",
    "    },\n",
    "\n",
    "    # Specify the azure_ai_project_url (string format) to push results to portal\n",
    "    azure_ai_project = azure_ai_project_url,\n",
    "    \n",
    "    # Specify the output path to push results also to local file\n",
    "    output_path=\"./42-evaluate-quality.results.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde8ca4",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### 9.1 View Results Online\n",
    "\n",
    "Just as before, you can now view the results of the multi-evaluator run using the Evaluation tab in the Azure AI Foundry Studio. Here is what you should see:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ce3da",
   "metadata": {},
   "source": [
    "#### Quality Evaluation\n",
    "\n",
    "![Quality](./../docs/img/screenshots/lab-02-portal-quality.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e146493",
   "metadata": {},
   "source": [
    "### 9.2 View Results Locally\n",
    "\n",
    "Just like before, you can see the results of the multi-evaluator run stored in a local json file at `02-quality-evaluators.results.json`. \n",
    "- Open the file in the VS Code editor\n",
    "- Right click and select \"Format Document\" to read the results better\n",
    "- Observe the metrics collected for each row - these are the multiple evaluators running on the same sample prompt/response pair\n",
    "- Scroll down to the end of the file to see the summary of the evaluation run - get a sense of the overall metrics for the run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686630d4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ | Congratulations!\n",
    "\n",
    "You have successfully completed the second lab in this module and got hands-on experience with a core subset of the the built-in quality evaluators. You also got a sense of how to create and run a custom evaluator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
