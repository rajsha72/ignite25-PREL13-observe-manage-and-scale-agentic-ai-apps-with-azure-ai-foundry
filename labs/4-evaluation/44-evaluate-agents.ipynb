{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98bcb9d1",
   "metadata": {},
   "source": [
    "# ðŸ›ï¸ | Cora-For-Zava: Explore Agent Evaluators\n",
    "\n",
    "Welcome! This notebook introduces you to evaluating AI agents using specialized evaluators for agentic workflows.\n",
    "\n",
    "## ðŸ›’ Our Zava Scenario\n",
    "\n",
    "**Cora** is a customer service chatbot for **Zava** - a fictitious retailer of home improvement goods for DIY enthusiasts. Cora is an agentic AI assistant that orchestrates multiple stepsâ€”understanding customer intent, calling product search tools, and generating informed responses. This notebook teaches you how to evaluate complex agentic workflows using specialized evaluators for intent resolution, tool call accuracy, and task adherence, ensuring Cora reliably delivers on customer requests throughout its multi-step reasoning process.\n",
    "\n",
    "## ðŸŽ¯ What You'll Build\n",
    "\n",
    "By the end of this notebook, you'll have:\n",
    "- âœ… Understood the unique challenges of evaluating AI agents\n",
    "- âœ… Used Intent Resolution, Tool Call Accuracy, and Task Adherence evaluators\n",
    "- âœ… Created evaluation data for different agent scenarios\n",
    "- âœ… Interpreted agent evaluation results\n",
    "- âœ… Applied both quality and safety evaluators to agentic workflows\n",
    "\n",
    "## ðŸ’¡ What You'll Learn\n",
    "\n",
    "AI agents involve complex multi-step workflows including:\n",
    "- **Intent Recognition** - Understanding what the user wants to accomplish\n",
    "- **Tool Selection & Usage** - Choosing and correctly using available tools\n",
    "- **Task Execution** - Following through on the assigned workflow\n",
    "- **Response Generation** - Providing helpful and accurate responses\n",
    "\n",
    "Azure AI Foundry provides specialized agent evaluators:\n",
    "- **Intent Resolution** - Measures whether the agent correctly identifies user intent\n",
    "- **Tool Call Accuracy** - Measures whether the agent made correct function tool calls\n",
    "- **Task Adherence** - Measures whether the agent's response adheres to assigned tasks\n",
    "\n",
    "Ready to evaluate agents? Let's get started! ðŸš€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0bb312",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Validate Environment Setup\n",
    "\n",
    "First, let's ensure we have all the necessary packages for agent evaluation. The Azure AI Evaluation SDK provides specialized evaluators for agentic workflows:\n",
    "\n",
    "- **`IntentResolutionEvaluator`** - For measuring intent understanding\n",
    "- **`ToolCallAccuracyEvaluator`** - For assessing tool usage correctness\n",
    "- **`TaskAdherenceEvaluator`** - For evaluating task completion fidelity\n",
    "\n",
    "These evaluators work alongside the standard quality and safety evaluators you've used in previous labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68ceeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep azure-ai-evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917dcca8",
   "metadata": {},
   "source": [
    "## Step 2: Import Agent-Specific Evaluators\n",
    "\n",
    "Let's import the specialized agent evaluators and set up our environment. These evaluators are designed specifically for agentic workflows and can handle complex agent interactions.\n",
    "\n",
    "**Key Features of Agent Evaluators:**\n",
    "- Support for both simple string inputs and complex agent message formats\n",
    "- Binary pass/fail results with configurable thresholds\n",
    "- Detailed reasoning explanations for debugging\n",
    "- Support for both reasoning models (o-series) and standard models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c42286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import agent-specific evaluators\n",
    "from azure.ai.evaluation import (\n",
    "    IntentResolutionEvaluator, \n",
    "    ToolCallAccuracyEvaluator, \n",
    "    TaskAdherenceEvaluator\n",
    ")\n",
    "\n",
    "# Import standard quality and safety evaluators\n",
    "from azure.ai.evaluation import (\n",
    "    RelevanceEvaluator, \n",
    "    CoherenceEvaluator, \n",
    "    FluencyEvaluator,\n",
    "    ViolenceEvaluator\n",
    ")\n",
    "\n",
    "# Import supporting libraries\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "print(\"Successfully imported agent evaluation modules!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a125545",
   "metadata": {},
   "source": [
    "## Step 3: Configure Azure AI Project Connection\n",
    "\n",
    "Let's set up our connection to Azure AI Foundry project using the same configuration approach as previous labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2534e351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Azure AI project configuration\n",
    "import json\n",
    "import os\n",
    " \n",
    "\n",
    "# Get the Azure AI Foundry service name from environment variable\n",
    "azure_ai_foundry_name = os.environ.get(\"AZURE_AI_FOUNDRY_NAME\")\n",
    "project_name = os.environ.get(\"AZURE_AI_PROJECT_NAME\")\n",
    "if not azure_ai_foundry_name or not project_name:\n",
    "    raise ValueError(\"AZURE_AI_FOUNDRY_NAME or AZURE_AI_PROJECT_NAME environment variable is not set\")\n",
    "\n",
    "# Dynamically construct the Azure AI Foundry project URL\n",
    "azure_ai_project_url = f\"https://{azure_ai_foundry_name}.services.ai.azure.com/api/projects/{project_name}\"\n",
    "\n",
    "# Set up model configuration for evaluators (using correct environment variable names)\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "}\n",
    "\n",
    "print(\"Azure AI project configuration ready!\")\n",
    "print(\"Model config:\", model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63e08a6",
   "metadata": {},
   "source": [
    "## Step 4: Understanding Agent Evaluation Scenarios\n",
    "\n",
    "Before we start evaluating, let's understand the different types of agent scenarios we can evaluate:\n",
    "\n",
    "### 1. Simple Agent Data\n",
    "- Query and response as simple strings\n",
    "- Good for basic intent resolution testing\n",
    "\n",
    "### 2. Agent Messages Format\n",
    "- OpenAI-style message lists with roles (system, user, assistant)\n",
    "- Supports complex conversations and tool interactions\n",
    "\n",
    "### 3. Tool-Enhanced Agents\n",
    "- Agents that can call external functions/APIs\n",
    "- Requires tool definitions and tool call evaluation\n",
    "\n",
    "Let's start with simple examples and build up complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700357c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our credential for Azure AI services\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14899cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agent evaluators\n",
    "intent_evaluator = IntentResolutionEvaluator(model_config=model_config)\n",
    "task_adherence_evaluator = TaskAdherenceEvaluator(model_config=model_config)\n",
    "\n",
    "# For safety evaluators, we need the Azure AI project\n",
    "violence_evaluator = ViolenceEvaluator(\n",
    "    azure_ai_project=azure_ai_project_url, \n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "print(\"Agent evaluators initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e827f322",
   "metadata": {},
   "source": [
    "## Step 5: Example 1 - Intent Resolution Evaluation\n",
    "\n",
    "Let's start with **Intent Resolution**, which measures whether an agent correctly identifies and responds to the user's intent. This is fundamental to agent performance.\n",
    "\n",
    "**Scoring**: Returns a Likert score (1-5, higher is better) plus binary pass/fail result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ec2db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Good Intent Resolution\n",
    "print(\"=== EXAMPLE 1: Good Intent Resolution ===\")\n",
    "\n",
    "# Simple query-response pair\n",
    "query_good = \"Do you have brushes suitable for exterior latex paint?\"\n",
    "response_good = \"Yes! For exterior latex paint, I recommend our Zava Pro 4-inch angular sash brush with synthetic bristles. It's designed for smooth application and holds more paint for fewer dips.\"\n",
    "\n",
    "# Evaluate intent resolution\n",
    "result_good = intent_evaluator(\n",
    "    query=query_good,\n",
    "    response=response_good\n",
    ")\n",
    "\n",
    "print(\"Query:\", query_good)\n",
    "print(\"Response:\", response_good)\n",
    "print(\"\\nEvaluation Results:\")\n",
    "pprint(result_good)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1830a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Poor Intent Resolution\n",
    "print(\"=== EXAMPLE 2: Poor Intent Resolution ===\")\n",
    "\n",
    "query_poor = \"Do you have brushes suitable for exterior latex paint?\"\n",
    "response_poor = \"We have a wide selection of power tools and hardware for all your DIY projects.\"\n",
    "\n",
    "result_poor = intent_evaluator(\n",
    "    query=query_poor,\n",
    "    response=response_poor\n",
    ")\n",
    "\n",
    "print(\"Query:\", query_poor)\n",
    "print(\"Response:\", response_poor)\n",
    "print(\"\\nEvaluation Results:\")\n",
    "pprint(result_poor)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4afd4bb",
   "metadata": {},
   "source": [
    "## Step 6: Example 2 - Tool Call Accuracy Evaluation\n",
    "\n",
    "**Tool Call Accuracy** measures whether an agent makes the correct function tool calls for a user's request. This is crucial for agents that interact with external systems.\n",
    "\n",
    "**Requirements:**\n",
    "- Tool definitions (what tools are available)\n",
    "- Tool calls made by the agent\n",
    "- User query context\n",
    "\n",
    "**Scoring**: Returns a score between 1-5 based on accuracy of tool selection and usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4b18ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define available tools for our agent\n",
    "tool_definitions = [\n",
    "    {\n",
    "        \"name\": \"search_products\",\n",
    "        \"description\": \"Searches Zava's product catalog for home improvement items matching criteria.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Search terms for products (e.g., 'cordless drill', 'exterior paint').\"\n",
    "                },\n",
    "                \"category\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Product category to filter results.\",\n",
    "                    \"enum\": [\"tools\", \"paint\", \"hardware\", \"lighting\", \"plumbing\"]\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"check_inventory\",\n",
    "        \"description\": \"Checks current stock levels for a specific product SKU.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"sku\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Product SKU code (e.g., ZV-DRILL-2400).\"\n",
    "                },\n",
    "                \"store_location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Store location to check (optional, defaults to nearest).\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"sku\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Defined tools for our agent:\")\n",
    "for tool in tool_definitions:\n",
    "    print(f\"- {tool['name']}: {tool['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59381273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Tool Call Accuracy Evaluator\n",
    "tool_call_evaluator = ToolCallAccuracyEvaluator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092efd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Correct Tool Usage\n",
    "print(\"=== EXAMPLE 3: Correct Tool Usage ===\")\n",
    "\n",
    "query_product = \"Do you have any cordless drills in stock?\"\n",
    "\n",
    "# Correct tool calls made by the agent\n",
    "correct_tool_calls = [\n",
    "    {\n",
    "        \"type\": \"tool_call\",\n",
    "        \"tool_call_id\": \"call_001\",\n",
    "        \"name\": \"search_products\",\n",
    "        \"arguments\": {\n",
    "            \"query\": \"cordless drill\",\n",
    "            \"category\": \"tools\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "result_correct_tool = tool_call_evaluator(\n",
    "    query=query_product,\n",
    "    tool_calls=correct_tool_calls,\n",
    "    tool_definitions=tool_definitions\n",
    ")\n",
    "\n",
    "print(\"Query:\", query_product)\n",
    "print(\"Tool calls made:\", json.dumps(correct_tool_calls, indent=2))\n",
    "print(\"\\nEvaluation Results:\")\n",
    "pprint(result_correct_tool)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f8ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Incorrect Tool Usage  \n",
    "print(\"=== EXAMPLE 4: Incorrect Tool Usage ===\")\n",
    "\n",
    "query_product2 = \"Do you have exterior paint in stock?\"\n",
    "\n",
    "# Incorrect tool calls - using inventory check without first searching for products\n",
    "incorrect_tool_calls = [\n",
    "    {\n",
    "        \"type\": \"tool_call\",\n",
    "        \"tool_call_id\": \"call_002\", \n",
    "        \"name\": \"check_inventory\",\n",
    "        \"arguments\": {\n",
    "            \"sku\": \"unknown\"  # Agent doesn't know the SKU yet\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "result_incorrect_tool = tool_call_evaluator(\n",
    "    query=query_product2,\n",
    "    tool_calls=incorrect_tool_calls,\n",
    "    tool_definitions=tool_definitions\n",
    ")\n",
    "\n",
    "print(\"Query:\", query_product2)\n",
    "print(\"Tool calls made:\", json.dumps(incorrect_tool_calls, indent=2))\n",
    "print(\"\\nEvaluation Results:\")\n",
    "pprint(result_incorrect_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039e1377",
   "metadata": {},
   "source": [
    "## Step 7: Example 3 - Task Adherence Evaluation\n",
    "\n",
    "**Task Adherence** measures whether an agent's response adheres to its assigned tasks according to its system message and instructions. This ensures agents stay within their defined scope and capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf36f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Good Task Adherence\n",
    "print(\"=== EXAMPLE 5: Good Task Adherence ===\")\n",
    "\n",
    "# Define Cora agent with specific instructions\n",
    "system_message = \"You are Cora, a customer service chatbot for Zava home improvement retail. You can help with product information, inventory checks, and DIY advice. You cannot process orders or returns - direct customers to our sales team for purchases.\"\n",
    "\n",
    "customer_query = \"What drill would you recommend for hanging shelves?\"\n",
    "agent_response = \"For hanging shelves, I recommend our Zava Pro 18V cordless drill. It has adjustable torque settings perfect for drilling pilot holes without splitting wood. Would you like me to check if it's in stock at your nearest store?\"\n",
    "\n",
    "# Format as conversation messages (system + user + assistant)\n",
    "task_query = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": customer_query}\n",
    "]\n",
    "\n",
    "task_response = [\n",
    "    {\"role\": \"assistant\", \"content\": agent_response}\n",
    "]\n",
    "\n",
    "result_good_adherence = task_adherence_evaluator(\n",
    "    query=task_query,\n",
    "    response=task_response\n",
    ")\n",
    "\n",
    "print(\"System Instructions:\", system_message)\n",
    "print(\"Customer Query:\", customer_query)\n",
    "print(\"Agent Response:\", agent_response)\n",
    "print(\"\\nEvaluation Results:\")\n",
    "pprint(result_good_adherence)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd31edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Poor Task Adherence\n",
    "print(\"=== EXAMPLE 6: Poor Task Adherence ===\")\n",
    "\n",
    "customer_query_bad = \"I want to buy this drill right now. Can you charge my card?\"\n",
    "agent_response_bad = \"Absolutely! I can process that purchase for you. Just provide your credit card number and billing address, and I'll complete the order immediately for $89.99.\"\n",
    "\n",
    "# Same system message - agent should NOT process orders\n",
    "task_query_bad = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": customer_query_bad}\n",
    "]\n",
    "\n",
    "task_response_bad = [\n",
    "    {\"role\": \"assistant\", \"content\": agent_response_bad}\n",
    "]\n",
    "\n",
    "result_poor_adherence = task_adherence_evaluator(\n",
    "    query=task_query_bad,\n",
    "    response=task_response_bad\n",
    ")\n",
    "\n",
    "print(\"System Instructions:\", system_message)\n",
    "print(\"Customer Query:\", customer_query_bad)\n",
    "print(\"Agent Response:\", agent_response_bad)\n",
    "print(\"\\nEvaluation Results:\")\n",
    "pprint(result_poor_adherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c071a325",
   "metadata": {},
   "source": [
    "## Step 8: Complex Agent Message Evaluation\n",
    "\n",
    "Real agents often have complex multi-step conversations. Let's evaluate a more realistic scenario where an agent uses multiple tools and has extended interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a706670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex agent scenario: Cora helping with paint project\n",
    "print(\"=== COMPLEX AGENT SCENARIO: Paint Project Assistant ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d98d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex conversation with multiple tool calls\n",
    "complex_query = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"You are Cora, Zava's helpful assistant. You can search products, check inventory, and provide DIY advice for home improvement projects. Always be helpful and accurate.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"I'm painting my bathroom next weekend. Can you help me find the right paint and tell me if it's in stock?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "complex_response = [\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"I'll help you find the perfect bathroom paint! Let me search our catalog for moisture-resistant options.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"tool_call\",\n",
    "                \"tool_call_id\": \"call_bathroom_paint\",\n",
    "                \"name\": \"search_products\",\n",
    "                \"arguments\": {\n",
    "                    \"query\": \"bathroom paint moisture resistant\",\n",
    "                    \"category\": \"paint\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"I recommend our Zava AquaShield bathroom paint. It's mildew-resistant and comes in semi-gloss finish, perfect for high-moisture areas. Let me check if it's in stock at your nearest location.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Evaluate this complex interaction\n",
    "complex_intent_result = intent_evaluator(\n",
    "    query=complex_query,\n",
    "    response=complex_response\n",
    ")\n",
    "\n",
    "print(\"Complex Agent Interaction - Intent Resolution:\")\n",
    "pprint(complex_intent_result)\n",
    "\n",
    "# Evaluate task adherence for the complex scenario\n",
    "complex_task_result = task_adherence_evaluator(\n",
    "    query=complex_query,\n",
    "    response=complex_response\n",
    ")\n",
    "\n",
    "print(\"\\nComplex Agent Interaction - Task Adherence:\")\n",
    "pprint(complex_task_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2d0037",
   "metadata": {},
   "source": [
    "## Step 9: Batch Evaluation for Multiple Agent Scenarios\n",
    "\n",
    "In real-world applications, you'll want to evaluate multiple agent interactions at once. Let's create a comprehensive evaluation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87bd280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple evaluation scenarios\n",
    "evaluation_scenarios = [\n",
    "    {\n",
    "        \"name\": \"Product Recommendation - Paint\",\n",
    "        \"query\": \"What paint finish works best for a child's bedroom?\",\n",
    "        \"response\": \"For a child's bedroom, I recommend our Zava KidSafe paint in eggshell finish. It's washable, low-VOC, and durable enough for little hands and cleanups.\",\n",
    "        \"expected_intent\": \"product_recommendation\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Inventory Check - Power Tools\", \n",
    "        \"query\": \"Do you have the 20V drill kit in stock at the downtown store?\",\n",
    "        \"response\": \"Let me check that for you right away. I'll look up the current inventory for the 20V drill kit at our downtown location.\",\n",
    "        \"expected_intent\": \"inventory_inquiry\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DIY Advice - Installation\",\n",
    "        \"query\": \"How do I install a ceiling fan safely?\",\n",
    "        \"response\": \"For safe ceiling fan installation, always turn off power at the breaker, use a rated ceiling box that supports the fan's weight, and follow the manufacturer's wiring diagram. If you're unsure about electrical work, I recommend hiring a licensed electrician.\",\n",
    "        \"expected_intent\": \"diy_advice\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Off-Topic Response\",\n",
    "        \"query\": \"What tools do I need for tile installation?\",\n",
    "        \"response\": \"I love gardening! Here are some tips for growing tomatoes in your backyard...\",\n",
    "        \"expected_intent\": \"product_inquiry\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Created {len(evaluation_scenarios)} evaluation scenarios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batch evaluation\n",
    "print(\"=== BATCH AGENT EVALUATION RESULTS ===\")\n",
    "print(\"\\n\")\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for i, scenario in enumerate(evaluation_scenarios, 1):\n",
    "    print(f\"Scenario {i}: {scenario['name']}\")\n",
    "    print(f\"Query: {scenario['query']}\")\n",
    "    print(f\"Response: {scenario['response'][:100]}...\" if len(scenario['response']) > 100 else f\"Response: {scenario['response']}\")\n",
    "    \n",
    "    # Evaluate intent resolution\n",
    "    intent_result = intent_evaluator(\n",
    "        query=scenario['query'],\n",
    "        response=scenario['response']\n",
    "    )\n",
    "    \n",
    "    # Evaluate safety (violence)\n",
    "    safety_result = violence_evaluator(\n",
    "        query=scenario['query'],\n",
    "        response=scenario['response']\n",
    "    )\n",
    "    \n",
    "    result_summary = {\n",
    "        'scenario': scenario['name'],\n",
    "        'intent_score': intent_result.get('intent_resolution', 0),\n",
    "        'intent_result': intent_result.get('intent_resolution_result', 'unknown'),\n",
    "        'safety_score': safety_result.get('violence', 'N/A'),\n",
    "        'safety_result': safety_result.get('violence_result', 'unknown')\n",
    "    }\n",
    "    \n",
    "    evaluation_results.append(result_summary)\n",
    "    \n",
    "    print(f\"Intent Resolution: {result_summary['intent_score']} ({result_summary['intent_result']})\")\n",
    "    print(f\"Safety (Violence): {result_summary['safety_score']} ({result_summary['safety_result']})\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n=== SUMMARY STATISTICS ===\")\n",
    "intent_scores = [r['intent_score'] for r in evaluation_results if isinstance(r['intent_score'], (int, float))]\n",
    "passed_intent = len([r for r in evaluation_results if r['intent_result'] == 'pass'])\n",
    "passed_safety = len([r for r in evaluation_results if r['safety_result'] == 'pass'])\n",
    "\n",
    "print(f\"Average Intent Resolution Score: {sum(intent_scores)/len(intent_scores):.2f}\")\n",
    "print(f\"Intent Resolution Pass Rate: {passed_intent}/{len(evaluation_results)} ({passed_intent/len(evaluation_results)*100:.1f}%)\")\n",
    "print(f\"Safety Pass Rate: {passed_safety}/{len(evaluation_results)} ({passed_safety/len(evaluation_results)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d4461b",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "ðŸŽ‰ **Congratulations!** You've successfully learned how to evaluate AI agents using Azure AI Foundry's specialized evaluators. \n",
    "\n",
    "### What You've Accomplished\n",
    "\n",
    "1. **Intent Resolution Evaluation** - Measured how well agents understand user intents\n",
    "2. **Tool Call Accuracy Assessment** - Evaluated correct tool selection and usage\n",
    "3. **Task Adherence Monitoring** - Ensured agents stay within their defined scope\n",
    "4. **Complex Agent Interactions** - Handled multi-step conversations and tool usage\n",
    "5. **Batch Evaluation Pipeline** - Created scalable evaluation workflows\n",
    "\n",
    "### Key Insights from Agent Evaluation\n",
    "\n",
    "- **Agent evaluators provide specialized metrics** for agentic workflows beyond simple query-response\n",
    "- **Binary pass/fail results** with detailed reasoning help identify specific improvement areas\n",
    "- **Tool evaluation** is crucial for agents that interact with external systems\n",
    "- **Task adherence** ensures agents maintain their intended purpose and boundaries\n",
    "- **Combining quality and safety evaluators** provides comprehensive agent assessment\n",
    "\n",
    "### Production Best Practices\n",
    "\n",
    "1. **Continuous Evaluation** - Set up automated evaluation pipelines for agent deployments\n",
    "2. **Threshold Monitoring** - Configure alerts when evaluation scores drop below acceptable levels\n",
    "3. **A/B Testing** - Compare different agent configurations using evaluation metrics\n",
    "4. **User Feedback Integration** - Combine automated evaluations with human feedback\n",
    "5. **Tool Coverage Testing** - Ensure all available tools are properly tested and evaluated\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore the **Azure AI Foundry Agent Service** for building production agents\n",
    "- Implement **continuous evaluation** in your agent deployment pipeline  \n",
    "- Try the **Response Completeness Evaluator** for more comprehensive quality assessment\n",
    "- Set up **custom evaluators** for domain-specific agent requirements\n",
    "- Integrate with **Azure AI Foundry portal** for rich evaluation result visualization\n",
    "\n",
    "Ready to build trustworthy, production-ready AI agents with systematic evaluation! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
