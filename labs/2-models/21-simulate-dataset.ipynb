{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ðŸ›ï¸ | Cora-For-Zava: Simulate Test Datasets\n",
    "\n",
    "Welcome! This notebook will walk you through generating synthetic datasets using Azure AI Search and the Azure AI Evaluation Simulator.\n",
    "\n",
    "## ðŸ›’ Our Zava Scenario\n",
    "\n",
    "**Cora** is a customer service chatbot for **Zava** - a fictitious retailer of home improvement goods for DIY enthusiasts. To ensure Cora provides accurate and helpful responses about hardware and home improvement products, you need quality test data. This notebook helps you generate synthetic query-response pairs based on your product catalog, creating a robust evaluation dataset to measure Cora's performance before deployment.\n",
    "\n",
    "## ðŸŽ¯ What You'll Build\n",
    "\n",
    "By the end of this notebook, you'll have:\n",
    "- âœ… Generated synthetic query-response pairs from your product catalog\n",
    "- âœ… Created a dataset in JSON Lines format for evaluation\n",
    "- âœ… Learned how to use the Azure AI Evaluation Simulator\n",
    "- âœ… Saved your dataset for use in future evaluation exercises\n",
    "\n",
    "## ðŸ’¡ What You'll Learn\n",
    "\n",
    "- How to configure the Azure AI Evaluation Simulator\n",
    "- How to connect to Azure AI Search to retrieve content\n",
    "- How to create a RAG application callback\n",
    "- How to generate synthetic query-response pairs\n",
    "- How to save and review the generated dataset\n",
    "\n",
    "Ready to generate your evaluation dataset? Let's get started! ðŸš€\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify Environment Variables\n",
    "\n",
    "The following environment variables should already be configured in your `.env` file from the earlier setup steps:\n",
    "\n",
    "- **AZURE_OPENAI_API_KEY**: Your Azure OpenAI API key\n",
    "- **AZURE_OPENAI_ENDPOINT**: Your Azure OpenAI service endpoint\n",
    "- **AZURE_OPENAI_API_VERSION**: The API version to use\n",
    "- **AZURE_OPENAI_DEPLOYMENT**: The name of your deployed chat model\n",
    "- **AZURE_SEARCH_ENDPOINT**: Your Azure AI Search service endpoint\n",
    "- **AZURE_SEARCH_API_KEY**: Your Azure AI Search API key\n",
    "- **AZURE_SEARCH_INDEX_NAME**: The name of your search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify all required Azure service credentials are available\n",
    "required_vars = [\n",
    "    \"AZURE_OPENAI_API_KEY\",\n",
    "    \"AZURE_OPENAI_ENDPOINT\", \n",
    "    \"AZURE_OPENAI_API_VERSION\",\n",
    "    \"AZURE_OPENAI_DEPLOYMENT\",\n",
    "    \"AZURE_SEARCH_ENDPOINT\",\n",
    "    \"AZURE_SEARCH_API_KEY\",\n",
    "    \"AZURE_SEARCH_INDEX_NAME\"\n",
    "]\n",
    "\n",
    "missing_vars = [var for var in required_vars if not os.environ.get(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    raise EnvironmentError(f\"âŒ Missing environment variables: {', '.join(missing_vars)}\")\n",
    "\n",
    "print(\"âœ… All environment variables configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize Azure AI Search connection parameters\n",
    "search_endpoint = os.environ.get(\"AZURE_SEARCH_ENDPOINT\")\n",
    "search_api_key = os.environ.get(\"AZURE_SEARCH_API_KEY\") \n",
    "search_index_name = os.environ.get(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "\n",
    "print(f\"âœ… Azure AI Search configured with index: {search_index_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure Azure OpenAI Model\n",
    "\n",
    "Now let's create the model configuration that the simulator will use to generate synthetic queries and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration\n",
    "\n",
    "# Create model configuration using environment variables\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"), \n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "\n",
    "print(f\"âœ… Model configuration created for: {model_config['azure_deployment']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize the Simulator\n",
    "\n",
    "Let's create the Azure AI Evaluation Simulator instance using the model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Initialize the Azure AI Evaluation Simulator\n",
    "from azure.ai.evaluation.simulator import Simulator\n",
    "\n",
    "# Create simulator instance with the configured model\n",
    "simulator = Simulator(model_config=model_config)\n",
    "print(\"ðŸ“Š Simulator initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 4: Define Search Function\n",
    "\n",
    "This function will search the Azure AI Search index for relevant content based on queries. It will be used by the RAG application callback to retrieve context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Define function to search Azure AI Search index and retrieve content\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def search_index_for_content(query: str, top_k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Search the Azure AI Search index for relevant content based on a query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Search query to find relevant content\n",
    "        top_k (int): Number of top results to retrieve (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        str: Combined content from search results, truncated to 1000 characters\n",
    "    \"\"\"\n",
    "    # Construct the search API endpoint\n",
    "    search_url = f\"{search_endpoint}/indexes/{search_index_name}/docs/search?api-version=2023-11-01\"\n",
    "    \n",
    "    # Set up request headers with API key authentication\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": search_api_key\n",
    "    }\n",
    "    \n",
    "    # Define the search query payload\n",
    "    search_payload = {\n",
    "        \"search\": query,\n",
    "        \"top\": top_k,\n",
    "        \"select\": \"content,title\"  # Updated to match available fields\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Execute the search request\n",
    "        response = requests.post(url=search_url, headers=headers, json=search_payload)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP error codes\n",
    "        \n",
    "        # Parse the search results\n",
    "        search_results = response.json()\n",
    "        combined_content = \"\"\n",
    "        \n",
    "        # Extract and combine content from search results\n",
    "        for result in search_results.get(\"value\", []):\n",
    "            # Prioritize 'content' field, fall back to 'title'\n",
    "            content = result.get(\"content\") or result.get(\"title\", \"\")\n",
    "            if content:\n",
    "                combined_content += content + \" \"\n",
    "        \n",
    "        # Limit content length to prevent token limits and improve performance\n",
    "        return combined_content[:1000].strip()\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"âŒ Error searching index: {e}\")\n",
    "        return f\"Error retrieving content for query: {query}\"\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"âŒ Error parsing search response: {e}\")\n",
    "        return f\"Error processing search results for query: {query}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test the Search Function\n",
    "\n",
    "Let's verify that the search function works correctly by testing it with a sample query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the search function with a sample query\n",
    "test_query = \"eggshell paint\"\n",
    "retrieved_content = search_index_for_content(test_query)\n",
    "\n",
    "print(f\"âœ… Search test successful!\")\n",
    "print(f\"ðŸ“„ Retrieved response: {retrieved_content} for query: '{test_query}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create RAG Application Callback\n",
    "\n",
    "This callback function simulates a RAG (Retrieval-Augmented Generation) application that:\n",
    "- Extracts user queries from messages\n",
    "- Searches the index for relevant content\n",
    "- Generates responses using Azure OpenAI\n",
    "- Returns formatted responses for the simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Define the application callback function for the simulator\n",
    "from typing import Dict, Any, Optional\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "async def rag_application_callback(\n",
    "    messages: Dict,\n",
    "    stream: bool = False,\n",
    "    session_state: Any = None,\n",
    "    context: Optional[Dict[str, Any]] = None,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Callback function that simulates a RAG (Retrieval-Augmented Generation) application.\n",
    "    \n",
    "    This function:\n",
    "    1. Extracts the user query from the message\n",
    "    2. Searches the Azure AI Search index for relevant content\n",
    "    3. Uses Azure OpenAI to generate a response based on the retrieved content\n",
    "    4. Returns the response in the expected format\n",
    "    \n",
    "    Args:\n",
    "        messages (Dict): Message history containing user queries\n",
    "        stream (bool): Whether to stream the response (not used in this implementation)\n",
    "        session_state (Any): Session state information\n",
    "        context (Optional[Dict[str, Any]]): Additional context information\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Response containing the generated message and metadata\n",
    "    \"\"\"\n",
    "    # Extract the user's query from the latest message\n",
    "    messages_list = messages[\"messages\"]\n",
    "    user_query = messages_list[-1][\"content\"]\n",
    "    \n",
    "    # Initialize Azure OpenAI client\n",
    "    openai_client = AzureOpenAI(\n",
    "        azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    )\n",
    "    \n",
    "    # Retrieve relevant content from the search index\n",
    "    retrieved_context = search_index_for_content(user_query)\n",
    "    \n",
    "    # Create a system prompt that instructs the model to use the retrieved context\n",
    "    system_prompt = \"\"\"You are a polite and helpful assistant that answers questions based on the provided context. \n",
    "Use the context information to provide accurate and relevant responses. If the context doesn't contain \n",
    "enough information to answer the question, say so politely. If the context mentions a product by name, reference it in the response.\"\"\"\n",
    "    \n",
    "    # Generate response using Azure OpenAI\n",
    "    try:\n",
    "        completion = openai_client.chat.completions.create(\n",
    "            model=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Context: {retrieved_context}\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Question: {user_query}\"}\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        \n",
    "        # Extract the generated response\n",
    "        ai_response = completion.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        ai_response = f\"Sorry, I encountered an error while generating a response: {str(e)}\"\n",
    "    \n",
    "    # Format the response according to the expected structure\n",
    "    response_message = {\n",
    "        \"content\": ai_response,\n",
    "        \"role\": \"assistant\",\n",
    "        \"context\": retrieved_context,\n",
    "    }\n",
    "    \n",
    "    # Add the response to the message history\n",
    "    messages[\"messages\"].append(response_message)\n",
    "    \n",
    "    # Return the complete response structure\n",
    "    return {\n",
    "        \"messages\": messages[\"messages\"], \n",
    "        \"stream\": stream, \n",
    "        \"session_state\": session_state, \n",
    "        \"context\": retrieved_context\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate Synthetic Dataset\n",
    "\n",
    "Now we'll use the simulator to generate synthetic query-response pairs based on content from the search index. This will create 5 query-response pairs for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the retrieved content is ready for simulation\n",
    "if not retrieved_content:\n",
    "    raise ValueError(\"No content retrieved from search. Please run Step 5 again with a valid query.\")\n",
    "\n",
    "print(f\"âœ… Ready to simulate with {len(retrieved_content)} characters of content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Run the simulator to generate synthetic data\n",
    "# The simulator needs a list of query-response dictionaries or plain text\n",
    "synthetic_outputs = await simulator(\n",
    "    target=rag_application_callback,\n",
    "    text=retrieved_content,\n",
    "    num_queries=10,\n",
    "    max_conversation_turns=1,\n",
    ")\n",
    "\n",
    "print(f\"âœ… Generated {len(synthetic_outputs)} synthetic query-response pairs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save the Dataset to File\n",
    "\n",
    "Let's save the generated dataset to a JSONL file for use in evaluation workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = Path(\"21-simulate-datasets.jsonl\")\n",
    "\n",
    "# Write each output as a JSON line to the file\n",
    "with output_file.open(\"w\") as f:\n",
    "    for output in synthetic_outputs:\n",
    "        f.write(output.to_eval_qr_json_lines())\n",
    "\n",
    "print(f\"âœ… Dataset saved to: {output_file.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Preview the Dataset\n",
    "\n",
    "Let's load and display a preview of the generated dataset to verify its structure and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and display the dataset\n",
    "dataset_df = pd.read_json(output_file, lines=True)\n",
    "\n",
    "print(f\"âœ… Dataset loaded: {dataset_df.shape[0]} rows, {dataset_df.shape[1]} columns\")\n",
    "print(f\"ðŸ“‹ Columns: {list(dataset_df.columns)}\")\n",
    "print()\n",
    "\n",
    "# Show first few records\n",
    "dataset_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Review the Dataset File\n",
    "\n",
    "Open the `21-simulate-datasets.jsonl` file in VS Code to examine the generated query-response-context triples and verify the data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "You've successfully generated a synthetic evaluation dataset! You can now use this dataset to:\n",
    "- Evaluate your RAG application's retrieval quality\n",
    "- Test and fine-tune your system prompts\n",
    "- Benchmark your application's performance\n",
    "- Create test cases for development\n",
    "\n",
    "Great work! ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
