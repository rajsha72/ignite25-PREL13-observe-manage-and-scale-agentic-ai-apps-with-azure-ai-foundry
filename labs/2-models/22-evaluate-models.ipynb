{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "745e6fbf",
   "metadata": {},
   "source": [
    "# üõçÔ∏è | Cora-For-Zava: Model Selection \n",
    "\n",
    "Welcome! This notebook will walk you through evaluating multiple AI models using a standardized test dataset and the Azure AI Evaluation SDK.\n",
    "\n",
    "## üõí Our Zava Scenario\n",
    "\n",
    "**Cora** is a customer service chatbot for **Zava** - a fictitious retailer of home improvement goods for DIY enthusiasts. To ensure Cora provides the best customer experience, you need to select the right foundation model. With multiple Azure OpenAI models available (GPT-4o, GPT-4o-mini, GPT-4), you need to evaluate which model delivers the best balance of quality, safety, and performance for your retail use case.\n",
    "\n",
    "## üéØ What You'll Build\n",
    "\n",
    "By the end of this notebook, you'll have:\n",
    "- ‚úÖ Configured multiple Azure OpenAI models for comparison\n",
    "- ‚úÖ Loaded standardized test datasets for evaluation\n",
    "- ‚úÖ Run evaluations across models using built-in evaluators\n",
    "- ‚úÖ Analyzed performance metrics (quality, safety, latency)\n",
    "- ‚úÖ Compared model results to make informed selection decisions\n",
    "\n",
    "## üí° What You'll Learn\n",
    "\n",
    "- How to configure multiple models for evaluation\n",
    "- How to load test datasets for evaluation\n",
    "- How to run evaluations with built-in evaluators\n",
    "- How to analyze and compare model performance\n",
    "- How to use Azure AI Foundry model leaderboards\n",
    "\n",
    "> **Note**: This demonstrates pre-production evaluation, which is essential before deploying AI applications.\n",
    "\n",
    "Ready to compare models? Let's get started! üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ecf1c6",
   "metadata": {},
   "source": [
    "## Step 1: Verify Environment Variables\n",
    "\n",
    "The following environment variables should already be configured in your `.env` file from the earlier setup steps:\n",
    "\n",
    "- **AZURE_OPENAI_API_KEY**: Your Azure OpenAI API key\n",
    "- **AZURE_OPENAI_ENDPOINT**: Your Azure OpenAI service endpoint\n",
    "- **AZURE_OPENAI_API_VERSION**: The API version to use\n",
    "- **AZURE_SUBSCRIPTION_ID**: Your Azure subscription ID\n",
    "- **AZURE_RESOURCE_GROUP**: Your Azure resource group name\n",
    "- **AZURE_AI_PROJECT_NAME**: Your Azure AI Foundry project name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa6c36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "# Use override=True to reload any changes made to .env\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Verify all required Azure service credentials are available\n",
    "required_vars = [\n",
    "    \"AZURE_OPENAI_API_KEY\",\n",
    "    \"AZURE_OPENAI_ENDPOINT\", \n",
    "    \"AZURE_OPENAI_API_VERSION\",\n",
    "    \"AZURE_SUBSCRIPTION_ID\",\n",
    "    \"AZURE_RESOURCE_GROUP\",\n",
    "    \"AZURE_AI_PROJECT_NAME\",\n",
    "    \"AZURE_AI_FOUNDRY_NAME\"\n",
    "]\n",
    "\n",
    "missing_vars = [var for var in required_vars if not os.environ.get(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    raise EnvironmentError(f\"‚ùå Missing environment variables: {', '.join(missing_vars)}\")\n",
    "\n",
    "print(\"‚úÖ All environment variables configured!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e63ed",
   "metadata": {},
   "source": [
    "## Step 2: Define Models to Evaluate\n",
    "\n",
    "Configure the array of model deployment names you want to evaluate. You can add or remove models from this list based on what's deployed in your Azure OpenAI resource.\n",
    "\n",
    "> **Tip**: Use [Azure AI Foundry Model Leaderboards](https://learn.microsoft.com/azure/ai-foundry/how-to/benchmark-model-in-catalog) to compare models on quality, safety, cost, and performance before deploying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51526b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models to evaluate\n",
    "# Add or remove model deployment names as needed\n",
    "models_to_evaluate = [\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4.1\"\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Configured {len(models_to_evaluate)} models for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12afa1f",
   "metadata": {},
   "source": [
    "## üí° Model Selection with Leaderboards\n",
    "\n",
    "Before or after running custom evaluations, you can use Azure AI Foundry Model Leaderboards to help select the best models:\n",
    "\n",
    "**How to Access Leaderboards:**\n",
    "1. Go to [Azure AI Foundry portal](https://ai.azure.com)\n",
    "2. Select **Model catalog** from the left pane\n",
    "3. Click **Browse leaderboards** in the Model leaderboards section\n",
    "\n",
    "**What You Can Compare:**\n",
    "- **Quality Leaderboard**: Models ranked by accuracy on reasoning, Q&A, coding, and math tasks\n",
    "- **Safety Leaderboard**: Models ranked by resistance to harmful content\n",
    "- **Cost Leaderboard**: Models ranked by cost-effectiveness\n",
    "- **Performance Leaderboard**: Models ranked by throughput and latency\n",
    "- **Trade-off Charts**: Quality vs. Cost, Quality vs. Safety, Quality vs. Throughput\n",
    "- **Scenario-Specific**: Find models best for your use case (chatbots, code generation, etc.)\n",
    "\n",
    "This can help you narrow down which models to include in your `models_to_evaluate` list!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87373185",
   "metadata": {},
   "source": [
    "## Step 3: Load Test Dataset\n",
    "\n",
    "Load the evaluation dataset with test queries and expected responses. This will be used as the test input for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae061aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the evaluation dataset\n",
    "dataset_path = \"22-evaluate-models.jsonl\"\n",
    "test_data = []\n",
    "\n",
    "with open(dataset_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line))\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(test_data)} test examples from {dataset_path}\\n\")\n",
    "\n",
    "# Display as DataFrame for easy viewing\n",
    "df_test_data = pd.DataFrame(test_data)\n",
    "print(\"üìä Test Dataset Preview:\")\n",
    "display(df_test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09901a6b",
   "metadata": {},
   "source": [
    "## Step 4: Configure Azure AI Project\n",
    "\n",
    "Set up the Azure AI project connection for running evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fb3075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Create Azure AI project configuration\n",
    "subscription_id = os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\n",
    "resource_group_name = os.environ.get(\"AZURE_RESOURCE_GROUP\")\n",
    "project_name = os.environ.get(\"AZURE_AI_PROJECT_NAME\")\n",
    "azure_ai_foundry_name = os.environ.get(\"AZURE_AI_FOUNDRY_NAME\")\n",
    "\n",
    "# Dynamically construct the Azure AI Foundry project URL\n",
    "azure_ai_project_url = f\"https://{azure_ai_foundry_name}.services.ai.azure.com/api/projects/{project_name}\"\n",
    "\n",
    "# Initialize and verify credential\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Try to get a token to verify authentication\n",
    "    token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "    print(\"‚úÖ Azure credentials verified successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Azure credentials not found or expired!\")\n",
    "    print(\"Please run 'az login' in the terminal to authenticate with Azure.\")\n",
    "    raise\n",
    "\n",
    "print(f\"‚úÖ Azure AI Project configured\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54f11c7",
   "metadata": {},
   "source": [
    "## Step 5: Create Model Configurations\n",
    "\n",
    "Create model configuration objects for each model we want to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3addb372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model configurations for all models\n",
    "model_configs = {}\n",
    "\n",
    "for model_name in models_to_evaluate:\n",
    "    model_configs[model_name] = AzureOpenAIModelConfiguration(\n",
    "        azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        azure_deployment=model_name,\n",
    "        api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "        api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    )\n",
    "    \n",
    "print(f\"‚úÖ Created configurations for {len(model_configs)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3af989",
   "metadata": {},
   "source": [
    "## Step 6: Define Target Function\n",
    "\n",
    "Create a function that takes a query and returns a response from a specific model. This will be used by the evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23fa929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "def create_target_function(model_name):\n",
    "    \"\"\"Create a target function for a specific model\"\"\"\n",
    "    \n",
    "    def target_function(query: str, ground_truth: str = None, response: str = None) -> dict:\n",
    "        \"\"\"Generate response from the model\"\"\"\n",
    "        client = AzureOpenAI(\n",
    "            api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "            api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "            azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        )\n",
    "        \n",
    "        # Call the model with the query\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions accurately and concisely.\"},\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=800\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"response\": response.choices[0].message.content\n",
    "        }\n",
    "    \n",
    "    return target_function\n",
    "\n",
    "print(\"‚úÖ Target function factory created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10314e6e",
   "metadata": {},
   "source": [
    "## Step 7: Configure Evaluators\n",
    "\n",
    "Set up the evaluators we'll use to assess model performance. We'll use [built-in evaluators](https://learn.microsoft.com/azure/ai-foundry/concepts/observability#what-are-evaluators) for quality and safety metrics.\n",
    "\n",
    "**Quality Evaluators** (AI-assisted):\n",
    "- **Relevance**: Evaluates how pertinent responses are to the given questions (scale 1-5)\n",
    "- **Coherence**: Evaluates how well the output flows smoothly and reads naturally (scale 1-5)\n",
    "- **Fluency**: Evaluates language proficiency and grammatical correctness (scale 1-5)\n",
    "\n",
    "**Safety Evaluators** (Content safety):\n",
    "- **Violence**: Detects violent content in responses (scale 0-7, lower is safer)\n",
    "- **Hate/Unfairness**: Detects hateful or unfair content (scale 0-7, lower is safer)\n",
    "- **Self-Harm**: Detects self-harm related content (scale 0-7, lower is safer)\n",
    "- **Sexual**: Detects sexual content (scale 0-7, lower is safer)\n",
    "\n",
    "> **Note**: We're using Relevance, Coherence, and Fluency evaluators which don't require context or ground truth. Groundedness evaluator has been removed as it requires additional context that our simple dataset doesn't provide.\n",
    "\n",
    "> Learn more about [evaluation metrics](https://learn.microsoft.com/azure/machine-learning/prompt-flow/concept-model-monitoring-generative-ai-evaluation-metrics) and their use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388785fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import (\n",
    "    GroundednessEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    ViolenceEvaluator,\n",
    "    HateUnfairnessEvaluator,\n",
    "    SelfHarmEvaluator,\n",
    "    SexualEvaluator\n",
    ")\n",
    "\n",
    "# Create a judge model configuration for evaluators\n",
    "judge_model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=\"gpt-4o\",  # Use a capable model as judge\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "\n",
    "# Initialize quality evaluators\n",
    "groundedness_eval = GroundednessEvaluator(model_config=judge_model_config)\n",
    "relevance_eval = RelevanceEvaluator(model_config=judge_model_config)\n",
    "coherence_eval = CoherenceEvaluator(model_config=judge_model_config)\n",
    "fluency_eval = FluencyEvaluator(model_config=judge_model_config)\n",
    "\n",
    "# Initialize safety evaluators (using azure_ai_project_url instead of dictionary)\n",
    "violence_eval = ViolenceEvaluator(azure_ai_project=azure_ai_project_url, credential=credential)\n",
    "hate_unfairness_eval = HateUnfairnessEvaluator(azure_ai_project=azure_ai_project_url, credential=credential)\n",
    "self_harm_eval = SelfHarmEvaluator(azure_ai_project=azure_ai_project_url, credential=credential)\n",
    "sexual_eval = SexualEvaluator(azure_ai_project=azure_ai_project_url, credential=credential)\n",
    "\n",
    "print(\"‚úÖ Evaluators configured:\")\n",
    "print(\"   Quality: Groundedness, Relevance, Coherence, Fluency\")\n",
    "print(\"   Safety: Violence, Hate/Unfairness, Self-Harm, Sexual\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688f8077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a single model and single prompt\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üß™ Running configuration test...\", flush=True)\n",
    "print(\"=\" * 60, flush=True)\n",
    "\n",
    "# Select first model for testing\n",
    "test_model = models_to_evaluate[0]\n",
    "print(f\"\\nüìã Test Model: {test_model}\", flush=True)\n",
    "\n",
    "# Create a simple test dataset with one example\n",
    "test_example = {\n",
    "    \"query\": \"What is the capital of France?\",\n",
    "    \"ground_truth\": \"Paris\",\n",
    "    \"response\": \"Paris\"\n",
    "}\n",
    "\n",
    "# Save test example to a temporary file\n",
    "import tempfile\n",
    "test_file = tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False)\n",
    "test_file.write(json.dumps(test_example) + '\\n')\n",
    "test_file.close()\n",
    "\n",
    "print(f\"üìù Test Query: {test_example['query']}\", flush=True)\n",
    "\n",
    "try:\n",
    "    # Create target function for test model\n",
    "    test_target_fn = create_target_function(test_model)\n",
    "    \n",
    "    # Test the target function\n",
    "    print(\"\\n1Ô∏è‚É£ Testing target function...\", flush=True)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    test_result = test_target_fn(**test_example)\n",
    "    print(f\"   ‚úÖ Target function returned: {test_result['response'][:100]}...\", flush=True)\n",
    "    \n",
    "    # Test evaluation with minimal evaluators AND portal publishing\n",
    "    print(\"\\n2Ô∏è‚É£ Testing evaluation pipeline with portal publishing...\", flush=True)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    from azure.ai.evaluation import evaluate\n",
    "    \n",
    "    test_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    test_eval_result = evaluate(\n",
    "        data=test_file.name,\n",
    "        target=test_target_fn,\n",
    "        evaluators={\n",
    "            \"relevance\": relevance_eval,\n",
    "            \"coherence\": coherence_eval,\n",
    "        },\n",
    "        evaluator_config={\n",
    "            \"default\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${target.response}\",\n",
    "            }\n",
    "        },\n",
    "        # Publish to portal for verification (using URL format)\n",
    "        azure_ai_project=azure_ai_project_url,\n",
    "        evaluation_name=f\"22-evaluate-models-TEST_{test_model}_{test_timestamp}\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n   ‚úÖ Evaluation completed successfully!\", flush=True)\n",
    "    print(f\"   üìä Sample metrics:\", flush=True)\n",
    "    print(f\"      - Relevance: {test_eval_result['metrics'].get('relevance', 'N/A')}\", flush=True)\n",
    "    print(f\"      - Coherence: {test_eval_result['metrics'].get('coherence', 'N/A')}\", flush=True)\n",
    "    \n",
    "    # Show portal URL if available\n",
    "    if test_eval_result.get('studio_url'):\n",
    "        print(f\"\\n   üåê View test results in portal:\", flush=True)\n",
    "        print(f\"      {test_eval_result['studio_url']}\", flush=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60, flush=True)\n",
    "    print(\"‚úÖ Configuration test PASSED! Ready to run full evaluation.\", flush=True)\n",
    "    print(\"=\" * 60, flush=True)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Configuration test FAILED!\", flush=True)\n",
    "    print(f\"Error: {str(e)}\", flush=True)\n",
    "    print(\"\\nPlease fix the configuration before proceeding to Step 8.\", flush=True)\n",
    "    print(\"=\" * 60, flush=True)\n",
    "    sys.stdout.flush()\n",
    "    raise\n",
    "finally:\n",
    "    # Clean up temporary file\n",
    "    import os\n",
    "    if os.path.exists(test_file.name):\n",
    "        os.unlink(test_file.name)\n",
    "    print(\"\\nüßπ Temporary test file cleaned up.\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf3efc5",
   "metadata": {},
   "source": [
    "## Step 8: Run Evaluations\n",
    "\n",
    "Now we'll evaluate each model using the test dataset and the [`evaluate()` function](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/evaluate-sdk#local-evaluation-on-test-datasets-using-evaluate). This will generate comprehensive metrics for performance, quality, and safety.\n",
    "\n",
    "Each evaluation:\n",
    "- Tests the model with all examples from the dataset\n",
    "- Calculates quality metrics using a judge model\n",
    "- Assesses safety using Azure AI Content Safety\n",
    "- Tracks evaluation time (as a proxy for latency)\n",
    "- **Publishes results to Azure AI Foundry portal** for visualization\n",
    "- **Saves detailed results locally** for offline analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa09801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Create output directory for evaluation results\n",
    "output_dir = \"./22-evaluate-models-results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Store results for each model\n",
    "evaluation_results = {}\n",
    "\n",
    "print(f\"üöÄ Starting evaluation of {len(models_to_evaluate)} models...\")\n",
    "print(f\"   Test dataset size: {len(test_data)} examples\")\n",
    "print(f\"   Output directory: {output_dir}\\n\")\n",
    "\n",
    "for model_name in models_to_evaluate:\n",
    "    print(f\"üìä Evaluating model: {model_name}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Create target function for this model\n",
    "        target_fn = create_target_function(model_name)\n",
    "        \n",
    "        # Create output path for this evaluation\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        output_path = os.path.join(output_dir, f\"22-evaluate-models_{model_name}_{timestamp}\")\n",
    "        \n",
    "        # Run evaluation with both portal publishing and local output\n",
    "        result = evaluate(\n",
    "            data=dataset_path,\n",
    "            target=target_fn,\n",
    "            evaluators={\n",
    "                \"relevance\": relevance_eval,\n",
    "                \"coherence\": coherence_eval,\n",
    "                \"fluency\": fluency_eval,\n",
    "                \"violence\": violence_eval,\n",
    "                \"hate_unfairness\": hate_unfairness_eval,\n",
    "                \"self_harm\": self_harm_eval,\n",
    "                \"sexual\": sexual_eval,\n",
    "            },\n",
    "            evaluator_config={\n",
    "                \"default\": {\n",
    "                    \"query\": \"${data.query}\",\n",
    "                    \"response\": \"${target.response}\",\n",
    "                }\n",
    "            },\n",
    "            # Publish to Azure AI Foundry portal for visualization (using URL format)\n",
    "            azure_ai_project=azure_ai_project_url,\n",
    "            # Save detailed results locally\n",
    "            output_path=output_path,\n",
    "            # Optional: provide evaluation name for easier tracking in portal\n",
    "            evaluation_name=f\"22-evaluate-models_{model_name}_{timestamp}\"\n",
    "        )\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Store results with both portal and local file information\n",
    "        evaluation_results[model_name] = {\n",
    "            \"metrics\": result[\"metrics\"],\n",
    "            \"evaluation_time\": elapsed_time,\n",
    "            \"studio_url\": result.get(\"studio_url\"),\n",
    "            \"output_path\": output_path\n",
    "        }\n",
    "        \n",
    "        print(f\"   ‚úÖ Completed in {elapsed_time:.2f} seconds\")\n",
    "        print(f\"   üìä Portal URL: {result.get('studio_url', 'N/A')}\")\n",
    "        print(f\"   üíæ Local results: {output_path}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error evaluating {model_name}: {str(e)}\\n\")\n",
    "        evaluation_results[model_name] = {\n",
    "            \"error\": str(e),\n",
    "            \"evaluation_time\": time.time() - start_time\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ All evaluations complete!\")\n",
    "print(f\"\\nüìÅ All results saved to: {output_dir}/\")\n",
    "print(f\"üåê View results in Azure AI Foundry portal using the URLs above\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc2431",
   "metadata": {},
   "source": [
    "## Step 9: Analyze Results\n",
    "\n",
    "Let's create a summary comparison of all models across key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375046ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prepare data for comparison\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, results in evaluation_results.items():\n",
    "    if \"error\" in results:\n",
    "        print(f\"‚ö†Ô∏è  {model_name}: Evaluation failed - {results['error']}\\n\")\n",
    "        continue\n",
    "    \n",
    "    metrics = results[\"metrics\"]\n",
    "    \n",
    "    # The metrics are stored with keys like \"relevance.relevance\", \"coherence.coherence\", etc.\n",
    "    row = {\n",
    "        \"Model\": model_name,\n",
    "        \"Eval Time (s)\": results['evaluation_time'],\n",
    "        \"Relevance\": metrics.get('relevance.relevance', metrics.get('relevance', 0)),\n",
    "        \"Coherence\": metrics.get('coherence.coherence', metrics.get('coherence', 0)),\n",
    "        \"Fluency\": metrics.get('fluency.fluency', metrics.get('fluency', 0)),\n",
    "        \"Violence\": metrics.get('violence.violence_defect_rate', metrics.get('violence', 0)),\n",
    "        \"Hate/Unfairness\": metrics.get('hate_unfairness.hate_unfairness_defect_rate', metrics.get('hate_unfairness', 0)),\n",
    "        \"Self-Harm\": metrics.get('self_harm.self_harm_defect_rate', metrics.get('self_harm', 0)),\n",
    "        \"Sexual\": metrics.get('sexual.sexual_defect_rate', metrics.get('sexual', 0)),\n",
    "    }\n",
    "    \n",
    "    comparison_data.append(row)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"üìä Model Evaluation Comparison\\n\")\n",
    "display(df_comparison)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d088ee16",
   "metadata": {},
   "source": [
    "## Step 10: Performance Summary\n",
    "\n",
    "Analyze the evaluation results to identify the best performing models across different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d089ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, HTML\n",
    "import pandas as pd\n",
    "\n",
    "# Check for successful evaluations\n",
    "successful_models = {name: results for name, results in evaluation_results.items() if 'error' not in results}\n",
    "failed_models = {name: results for name, results in evaluation_results.items() if 'error' in results}\n",
    "\n",
    "if successful_models:\n",
    "    display(Markdown(\"## üèÜ Best Performing Models by Metric\"))\n",
    "    \n",
    "    # Define evaluator metrics and their optimization direction\n",
    "    evaluator_metrics = [\n",
    "        ('relevance.relevance', 'Relevance', True),\n",
    "        ('coherence.coherence', 'Coherence', True),\n",
    "        ('fluency.fluency', 'Fluency', True),\n",
    "        ('violence.violence_defect_rate', 'Violence Safety', False),\n",
    "        ('hate_unfairness.hate_unfairness_defect_rate', 'Hate/Unfairness Safety', False),\n",
    "        ('self_harm.self_harm_defect_rate', 'Self-Harm Safety', False),\n",
    "        ('sexual.sexual_defect_rate', 'Sexual Safety', False),\n",
    "    ]\n",
    "    \n",
    "    # Create a dataframe for best models\n",
    "    best_models_data = []\n",
    "    \n",
    "    for metric_key, display_name, higher_is_better in evaluator_metrics:\n",
    "        valid_models = {}\n",
    "        \n",
    "        # Collect scores for this metric from all successful models\n",
    "        for model_name, results in successful_models.items():\n",
    "            metrics = results['metrics']\n",
    "            score = metrics.get(metric_key)\n",
    "            if score is not None:\n",
    "                valid_models[model_name] = score\n",
    "        \n",
    "        if valid_models:\n",
    "            # Find best model based on optimization direction\n",
    "            if higher_is_better:\n",
    "                best_model_name = max(valid_models, key=valid_models.get)\n",
    "                best_score = valid_models[best_model_name]\n",
    "                direction = \"‚Üë Higher is Better\"\n",
    "            else:\n",
    "                best_model_name = min(valid_models, key=valid_models.get)\n",
    "                best_score = valid_models[best_model_name]\n",
    "                direction = \"‚Üì Lower is Better\"\n",
    "            \n",
    "            best_models_data.append({\n",
    "                \"Metric\": display_name,\n",
    "                \"Best Model\": best_model_name,\n",
    "                \"Score\": f\"{best_score:.3f}\",\n",
    "                \"Direction\": direction\n",
    "            })\n",
    "    \n",
    "    df_best = pd.DataFrame(best_models_data)\n",
    "    display(df_best)\n",
    "    \n",
    "    # Calculate overall best model (based on quality metrics average)\n",
    "    display(Markdown(\"---\"))\n",
    "    display(Markdown(\"## üåü Overall Best Model (Quality Metrics Average)\"))\n",
    "    \n",
    "    quality_metric_keys = ['relevance.relevance', 'coherence.coherence', 'fluency.fluency']\n",
    "    model_quality_scores = {}\n",
    "    \n",
    "    for model_name, results in successful_models.items():\n",
    "        metrics = results['metrics']\n",
    "        scores = []\n",
    "        for metric_key in quality_metric_keys:\n",
    "            score = metrics.get(metric_key)\n",
    "            if score is not None:\n",
    "                scores.append(score)\n",
    "        \n",
    "        if scores:\n",
    "            avg_score = sum(scores) / len(scores)\n",
    "            model_quality_scores[model_name] = {\n",
    "                'avg_quality': avg_score,\n",
    "                'eval_time': results['evaluation_time']\n",
    "            }\n",
    "    \n",
    "    if model_quality_scores:\n",
    "        best_overall = max(model_quality_scores, key=lambda x: model_quality_scores[x]['avg_quality'])\n",
    "        best_data = model_quality_scores[best_overall]\n",
    "        \n",
    "        display(Markdown(f\"ü•á **{best_overall}** - Quality: {best_data['avg_quality']:.3f} | Time: {best_data['eval_time']:.2f}s\"))\n",
    "        \n",
    "        # Show all model quality scores for comparison\n",
    "        display(Markdown(\"\"))\n",
    "        \n",
    "        ranking_data = []\n",
    "        sorted_models = sorted(model_quality_scores.items(), key=lambda x: x[1]['avg_quality'], reverse=True)\n",
    "        \n",
    "        for rank, (model_name, data) in enumerate(sorted_models, 1):\n",
    "            medal = \"ü•á\" if rank == 1 else \"ü•à\" if rank == 2 else \"ü•â\" if rank == 3 else \"\"\n",
    "            ranking_data.append({\n",
    "                \"Rank\": f\"{medal} {rank}\",\n",
    "                \"Model\": model_name,\n",
    "                \"Avg Quality Score\": f\"{data['avg_quality']:.3f}\",\n",
    "                \"Eval Time (s)\": f\"{data['eval_time']:.2f}\"\n",
    "            })\n",
    "        \n",
    "        df_ranking = pd.DataFrame(ranking_data)\n",
    "        display(df_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e5dd01",
   "metadata": {},
   "source": [
    "## Step 11: Next Steps\n",
    "\n",
    "You've successfully evaluated multiple models! Here are some next steps to consider:\n",
    "\n",
    "### üìä View Results in Two Places\n",
    "\n",
    "- **Azure AI Foundry Portal**: Interactive visualizations with [detailed charts and comparisons](https://learn.microsoft.com/azure/ai-foundry/how-to/evaluate-results)\n",
    "- **Portal URLs**: Each evaluation includes a studio URL for easy access and team sharing\n",
    "- **Local Files**: All results saved in `./22-evaluate-models-results/` for offline analysis\n",
    "- **Version Control**: Commit JSON files for reproducibility and tracking over time\n",
    "\n",
    "### üèÜ Use Model Leaderboards for Selection\n",
    "\n",
    "- **Browse Leaderboards**: Compare models by [Quality, Safety, Cost, and Performance](https://learn.microsoft.com/azure/ai-foundry/how-to/benchmark-model-in-catalog)\n",
    "- **Trade-off Analysis**: View quality vs. cost, quality vs. safety charts\n",
    "- **Scenario Filtering**: Find models best suited for your use case (Q&A, coding, reasoning)\n",
    "- **Access Portal**: [Azure AI Foundry Model Catalog ‚Üí Browse Leaderboards](https://aka.ms/model-leaderboards)\n",
    "\n",
    "---\n",
    "\n",
    "**Great work! You now have comprehensive evaluation metrics for multiple models.** üéâ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
