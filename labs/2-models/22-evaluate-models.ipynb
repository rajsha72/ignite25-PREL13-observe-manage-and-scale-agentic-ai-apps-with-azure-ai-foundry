{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "745e6fbf",
   "metadata": {},
   "source": [
    "# Model Selection By Evaluation\n",
    "\n",
    "Welcome! This notebook will walk you through evaluating multiple AI models using a standardized test dataset and the [Azure AI Evaluation SDK](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/evaluate-sdk).\n",
    "\n",
    "## What You'll Learn\n",
    "- How to configure multiple models for evaluation\n",
    "- How to load test datasets for evaluation\n",
    "- How to run evaluations across multiple models using built-in evaluators\n",
    "- How to analyze performance metrics (quality, safety, latency)\n",
    "- How to compare model results\n",
    "- How to use Azure AI Foundry model leaderboards for model selection\n",
    "\n",
    "> **Note**: This notebook demonstrates [pre-production evaluation](https://learn.microsoft.com/azure/ai-foundry/concepts/observability#the-three-stages-of-genaiops-evaluation), which is essential before deploying AI applications to production.\n",
    "\n",
    "Let's get started! üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ecf1c6",
   "metadata": {},
   "source": [
    "## Step 1: Verify Environment Variables\n",
    "\n",
    "The following environment variables should already be configured in your `.env` file from the earlier setup steps:\n",
    "\n",
    "- **AZURE_OPENAI_API_KEY**: Your Azure OpenAI API key\n",
    "- **AZURE_OPENAI_ENDPOINT**: Your Azure OpenAI service endpoint\n",
    "- **AZURE_OPENAI_API_VERSION**: The API version to use\n",
    "- **AZURE_SUBSCRIPTION_ID**: Your Azure subscription ID\n",
    "- **AZURE_RESOURCE_GROUP**: Your Azure resource group name\n",
    "- **AZURE_AI_PROJECT_NAME**: Your Azure AI Foundry project name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aa6c36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All environment variables configured!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "# Use override=True to reload any changes made to .env\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Verify all required Azure service credentials are available\n",
    "required_vars = [\n",
    "    \"AZURE_OPENAI_API_KEY\",\n",
    "    \"AZURE_OPENAI_ENDPOINT\", \n",
    "    \"AZURE_OPENAI_API_VERSION\",\n",
    "    \"AZURE_SUBSCRIPTION_ID\",\n",
    "    \"AZURE_RESOURCE_GROUP\",\n",
    "    \"AZURE_AI_PROJECT_NAME\",\n",
    "    \"AZURE_AI_FOUNDRY_NAME\"\n",
    "]\n",
    "\n",
    "missing_vars = [var for var in required_vars if not os.environ.get(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    raise EnvironmentError(f\"‚ùå Missing environment variables: {', '.join(missing_vars)}\")\n",
    "\n",
    "print(\"‚úÖ All environment variables configured!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e63ed",
   "metadata": {},
   "source": [
    "## Step 2: Define Models to Evaluate\n",
    "\n",
    "Configure the array of model deployment names you want to evaluate. You can add or remove models from this list based on what's deployed in your Azure OpenAI resource.\n",
    "\n",
    "> **Tip**: Use [Azure AI Foundry Model Leaderboards](https://learn.microsoft.com/azure/ai-foundry/how-to/benchmark-model-in-catalog) to compare models on quality, safety, cost, and performance before deploying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e51526b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configured 4 models for evaluation\n"
     ]
    }
   ],
   "source": [
    "# Define the models to evaluate\n",
    "# Add or remove model deployment names as needed\n",
    "models_to_evaluate = [\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4.1\"\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Configured {len(models_to_evaluate)} models for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12afa1f",
   "metadata": {},
   "source": [
    "## üí° Model Selection with Leaderboards\n",
    "\n",
    "Before or after running custom evaluations, you can use Azure AI Foundry Model Leaderboards to help select the best models:\n",
    "\n",
    "**How to Access Leaderboards:**\n",
    "1. Go to [Azure AI Foundry portal](https://ai.azure.com)\n",
    "2. Select **Model catalog** from the left pane\n",
    "3. Click **Browse leaderboards** in the Model leaderboards section\n",
    "\n",
    "**What You Can Compare:**\n",
    "- **Quality Leaderboard**: Models ranked by accuracy on reasoning, Q&A, coding, and math tasks\n",
    "- **Safety Leaderboard**: Models ranked by resistance to harmful content\n",
    "- **Cost Leaderboard**: Models ranked by cost-effectiveness\n",
    "- **Performance Leaderboard**: Models ranked by throughput and latency\n",
    "- **Trade-off Charts**: Quality vs. Cost, Quality vs. Safety, Quality vs. Throughput\n",
    "- **Scenario-Specific**: Find models best for your use case (chatbots, code generation, etc.)\n",
    "\n",
    "This can help you narrow down which models to include in your `models_to_evaluate` list!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87373185",
   "metadata": {},
   "source": [
    "## Step 3: Load Test Dataset\n",
    "\n",
    "Load the evaluation dataset with test queries and expected responses. This will be used as the test input for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae061aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 5 test examples from 22-evaluate-models.jsonl\n",
      "\n",
      "üìä Test Dataset Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When was United States found ?</td>\n",
       "      <td>1776</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What type of finish does the durable eggshell ...</td>\n",
       "      <td>The durable eggshell finish paint has a subtle...</td>\n",
       "      <td>The durable eggshell finish paint has a subtle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What product fits standard paint trays for qui...</td>\n",
       "      <td>Disposable plastic liners that fit standard pa...</td>\n",
       "      <td>The product that fits standard paint trays for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which paint is recommended for kitchens, bathr...</td>\n",
       "      <td>Washable semi-gloss interior paint for kitchen...</td>\n",
       "      <td>The washable semi-gloss interior paint is reco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0                     When was United States found ?   \n",
       "1                     What is the capital of France?   \n",
       "2  What type of finish does the durable eggshell ...   \n",
       "3  What product fits standard paint trays for qui...   \n",
       "4  Which paint is recommended for kitchens, bathr...   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0                                               1776   \n",
       "1                                              Paris   \n",
       "2  The durable eggshell finish paint has a subtle...   \n",
       "3  Disposable plastic liners that fit standard pa...   \n",
       "4  Washable semi-gloss interior paint for kitchen...   \n",
       "\n",
       "                                            response  \n",
       "0                                               1600  \n",
       "1                                              Paris  \n",
       "2  The durable eggshell finish paint has a subtle...  \n",
       "3  The product that fits standard paint trays for...  \n",
       "4  The washable semi-gloss interior paint is reco...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the evaluation dataset\n",
    "dataset_path = \"22-evaluate-models.jsonl\"\n",
    "test_data = []\n",
    "\n",
    "with open(dataset_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line))\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(test_data)} test examples from {dataset_path}\\n\")\n",
    "\n",
    "# Display as DataFrame for easy viewing\n",
    "df_test_data = pd.DataFrame(test_data)\n",
    "print(\"üìä Test Dataset Preview:\")\n",
    "display(df_test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09901a6b",
   "metadata": {},
   "source": [
    "## Step 4: Configure Azure AI Project\n",
    "\n",
    "Set up the Azure AI project connection for running evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05fb3075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure credentials verified successfully!\n",
      "‚úÖ Azure AI Project configured\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Create Azure AI project configuration\n",
    "subscription_id = os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\n",
    "resource_group_name = os.environ.get(\"AZURE_RESOURCE_GROUP\")\n",
    "project_name = os.environ.get(\"AZURE_AI_PROJECT_NAME\")\n",
    "azure_ai_foundry_name = os.environ.get(\"AZURE_AI_FOUNDRY_NAME\")\n",
    "\n",
    "# Dynamically construct the Azure AI Foundry project URL\n",
    "azure_ai_project_url = f\"https://{azure_ai_foundry_name}.services.ai.azure.com/api/projects/{project_name}\"\n",
    "\n",
    "# Initialize and verify credential\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Try to get a token to verify authentication\n",
    "    token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "    print(\"‚úÖ Azure credentials verified successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Azure credentials not found or expired!\")\n",
    "    print(\"Please run 'az login' in the terminal to authenticate with Azure.\")\n",
    "    raise\n",
    "\n",
    "print(f\"‚úÖ Azure AI Project configured\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54f11c7",
   "metadata": {},
   "source": [
    "## Step 5: Create Model Configurations\n",
    "\n",
    "Create model configuration objects for each model we want to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3addb372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created configurations for 4 models\n"
     ]
    }
   ],
   "source": [
    "# Create model configurations for all models\n",
    "model_configs = {}\n",
    "\n",
    "for model_name in models_to_evaluate:\n",
    "    model_configs[model_name] = AzureOpenAIModelConfiguration(\n",
    "        azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        azure_deployment=model_name,\n",
    "        api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "        api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    )\n",
    "    \n",
    "print(f\"‚úÖ Created configurations for {len(model_configs)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3af989",
   "metadata": {},
   "source": [
    "## Step 6: Define Target Function\n",
    "\n",
    "Create a function that takes a query and returns a response from a specific model. This will be used by the evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e23fa929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Target function factory created\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "def create_target_function(model_name):\n",
    "    \"\"\"Create a target function for a specific model\"\"\"\n",
    "    \n",
    "    def target_function(query: str, ground_truth: str = None, response: str = None) -> dict:\n",
    "        \"\"\"Generate response from the model\"\"\"\n",
    "        client = AzureOpenAI(\n",
    "            api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "            api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "            azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        )\n",
    "        \n",
    "        # Call the model with the query\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions accurately and concisely.\"},\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=800\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"response\": response.choices[0].message.content\n",
    "        }\n",
    "    \n",
    "    return target_function\n",
    "\n",
    "print(\"‚úÖ Target function factory created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10314e6e",
   "metadata": {},
   "source": [
    "## Step 7: Configure Evaluators\n",
    "\n",
    "Set up the evaluators we'll use to assess model performance. We'll use [built-in evaluators](https://learn.microsoft.com/azure/ai-foundry/concepts/observability#what-are-evaluators) for quality and safety metrics.\n",
    "\n",
    "**Quality Evaluators** (AI-assisted):\n",
    "- **Relevance**: Evaluates how pertinent responses are to the given questions (scale 1-5)\n",
    "- **Coherence**: Evaluates how well the output flows smoothly and reads naturally (scale 1-5)\n",
    "- **Fluency**: Evaluates language proficiency and grammatical correctness (scale 1-5)\n",
    "\n",
    "**Safety Evaluators** (Content safety):\n",
    "- **Violence**: Detects violent content in responses (scale 0-7, lower is safer)\n",
    "- **Hate/Unfairness**: Detects hateful or unfair content (scale 0-7, lower is safer)\n",
    "- **Self-Harm**: Detects self-harm related content (scale 0-7, lower is safer)\n",
    "- **Sexual**: Detects sexual content (scale 0-7, lower is safer)\n",
    "\n",
    "> **Note**: We're using Relevance, Coherence, and Fluency evaluators which don't require context or ground truth. Groundedness evaluator has been removed as it requires additional context that our simple dataset doesn't provide.\n",
    "\n",
    "> Learn more about [evaluation metrics](https://learn.microsoft.com/azure/machine-learning/prompt-flow/concept-model-monitoring-generative-ai-evaluation-metrics) and their use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "388785fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class ViolenceEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class HateUnfairnessEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class HateUnfairnessEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class SelfHarmEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class SexualEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class SelfHarmEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class SexualEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluators configured:\n",
      "   Quality: Groundedness, Relevance, Coherence, Fluency\n",
      "   Safety: Violence, Hate/Unfairness, Self-Harm, Sexual\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import (\n",
    "    GroundednessEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    ViolenceEvaluator,\n",
    "    HateUnfairnessEvaluator,\n",
    "    SelfHarmEvaluator,\n",
    "    SexualEvaluator\n",
    ")\n",
    "\n",
    "# Create a judge model configuration for evaluators\n",
    "judge_model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=\"gpt-4o\",  # Use a capable model as judge\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "\n",
    "# Initialize quality evaluators\n",
    "groundedness_eval = GroundednessEvaluator(model_config=judge_model_config)\n",
    "relevance_eval = RelevanceEvaluator(model_config=judge_model_config)\n",
    "coherence_eval = CoherenceEvaluator(model_config=judge_model_config)\n",
    "fluency_eval = FluencyEvaluator(model_config=judge_model_config)\n",
    "\n",
    "# Initialize safety evaluators (using azure_ai_project_url instead of dictionary)\n",
    "violence_eval = ViolenceEvaluator(azure_ai_project=azure_ai_project_url, credential=credential)\n",
    "hate_unfairness_eval = HateUnfairnessEvaluator(azure_ai_project=azure_ai_project_url, credential=credential)\n",
    "self_harm_eval = SelfHarmEvaluator(azure_ai_project=azure_ai_project_url, credential=credential)\n",
    "sexual_eval = SexualEvaluator(azure_ai_project=azure_ai_project_url, credential=credential)\n",
    "\n",
    "print(\"‚úÖ Evaluators configured:\")\n",
    "print(\"   Quality: Groundedness, Relevance, Coherence, Fluency\")\n",
    "print(\"   Safety: Violence, Hate/Unfairness, Self-Harm, Sexual\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688f8077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a single model and single prompt\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üß™ Running configuration test...\", flush=True)\n",
    "print(\"=\" * 60, flush=True)\n",
    "\n",
    "# Select first model for testing\n",
    "test_model = models_to_evaluate[0]\n",
    "print(f\"\\nüìã Test Model: {test_model}\", flush=True)\n",
    "\n",
    "# Create a simple test dataset with one example\n",
    "test_example = {\n",
    "    \"query\": \"What is the capital of France?\",\n",
    "    \"ground_truth\": \"Paris\",\n",
    "    \"response\": \"Paris\"\n",
    "}\n",
    "\n",
    "# Save test example to a temporary file\n",
    "import tempfile\n",
    "test_file = tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False)\n",
    "test_file.write(json.dumps(test_example) + '\\n')\n",
    "test_file.close()\n",
    "\n",
    "print(f\"üìù Test Query: {test_example['query']}\", flush=True)\n",
    "\n",
    "try:\n",
    "    # Create target function for test model\n",
    "    test_target_fn = create_target_function(test_model)\n",
    "    \n",
    "    # Test the target function\n",
    "    print(\"\\n1Ô∏è‚É£ Testing target function...\", flush=True)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    test_result = test_target_fn(**test_example)\n",
    "    print(f\"   ‚úÖ Target function returned: {test_result['response'][:100]}...\", flush=True)\n",
    "    \n",
    "    # Test evaluation with minimal evaluators AND portal publishing\n",
    "    print(\"\\n2Ô∏è‚É£ Testing evaluation pipeline with portal publishing...\", flush=True)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    from azure.ai.evaluation import evaluate\n",
    "    \n",
    "    test_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    test_eval_result = evaluate(\n",
    "        data=test_file.name,\n",
    "        target=test_target_fn,\n",
    "        evaluators={\n",
    "            \"relevance\": relevance_eval,\n",
    "            \"coherence\": coherence_eval,\n",
    "        },\n",
    "        evaluator_config={\n",
    "            \"default\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${target.response}\",\n",
    "            }\n",
    "        },\n",
    "        # Publish to portal for verification (using URL format)\n",
    "        azure_ai_project=azure_ai_project_url,\n",
    "        evaluation_name=f\"22-evaluate-models-TEST_{test_model}_{test_timestamp}\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n   ‚úÖ Evaluation completed successfully!\", flush=True)\n",
    "    print(f\"   üìä Sample metrics:\", flush=True)\n",
    "    print(f\"      - Relevance: {test_eval_result['metrics'].get('relevance', 'N/A')}\", flush=True)\n",
    "    print(f\"      - Coherence: {test_eval_result['metrics'].get('coherence', 'N/A')}\", flush=True)\n",
    "    \n",
    "    # Show portal URL if available\n",
    "    if test_eval_result.get('studio_url'):\n",
    "        print(f\"\\n   üåê View test results in portal:\", flush=True)\n",
    "        print(f\"      {test_eval_result['studio_url']}\", flush=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60, flush=True)\n",
    "    print(\"‚úÖ Configuration test PASSED! Ready to run full evaluation.\", flush=True)\n",
    "    print(\"=\" * 60, flush=True)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Configuration test FAILED!\", flush=True)\n",
    "    print(f\"Error: {str(e)}\", flush=True)\n",
    "    print(\"\\nPlease fix the configuration before proceeding to Step 8.\", flush=True)\n",
    "    print(\"=\" * 60, flush=True)\n",
    "    sys.stdout.flush()\n",
    "    raise\n",
    "finally:\n",
    "    # Clean up temporary file\n",
    "    import os\n",
    "    if os.path.exists(test_file.name):\n",
    "        os.unlink(test_file.name)\n",
    "    print(\"\\nüßπ Temporary test file cleaned up.\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf3efc5",
   "metadata": {},
   "source": [
    "## Step 8: Run Evaluations\n",
    "\n",
    "Now we'll evaluate each model using the test dataset and the [`evaluate()` function](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/evaluate-sdk#local-evaluation-on-test-datasets-using-evaluate). This will generate comprehensive metrics for performance, quality, and safety.\n",
    "\n",
    "Each evaluation:\n",
    "- Tests the model with all examples from the dataset\n",
    "- Calculates quality metrics using a judge model\n",
    "- Assesses safety using Azure AI Content Safety\n",
    "- Tracks evaluation time (as a proxy for latency)\n",
    "- **Publishes results to Azure AI Foundry portal** for visualization\n",
    "- **Saves detailed results locally** for offline analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7aa09801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Create output directory for evaluation results\n",
    "output_dir = \"./22-evaluate-models-results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Store results for each model\n",
    "evaluation_results = {}\n",
    "\n",
    "print(f\"üöÄ Starting evaluation of {len(models_to_evaluate)} models...\")\n",
    "print(f\"   Test dataset size: {len(test_data)} examples\")\n",
    "print(f\"   Output directory: {output_dir}\\n\")\n",
    "\n",
    "for model_name in models_to_evaluate:\n",
    "    print(f\"üìä Evaluating model: {model_name}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Create target function for this model\n",
    "        target_fn = create_target_function(model_name)\n",
    "        \n",
    "        # Create output path for this evaluation\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        output_path = os.path.join(output_dir, f\"22-evaluate-models_{model_name}_{timestamp}\")\n",
    "        \n",
    "        # Run evaluation with both portal publishing and local output\n",
    "        result = evaluate(\n",
    "            data=dataset_path,\n",
    "            target=target_fn,\n",
    "            evaluators={\n",
    "                \"relevance\": relevance_eval,\n",
    "                \"coherence\": coherence_eval,\n",
    "                \"fluency\": fluency_eval,\n",
    "                \"violence\": violence_eval,\n",
    "                \"hate_unfairness\": hate_unfairness_eval,\n",
    "                \"self_harm\": self_harm_eval,\n",
    "                \"sexual\": sexual_eval,\n",
    "            },\n",
    "            evaluator_config={\n",
    "                \"default\": {\n",
    "                    \"query\": \"${data.query}\",\n",
    "                    \"response\": \"${target.response}\",\n",
    "                }\n",
    "            },\n",
    "            # Publish to Azure AI Foundry portal for visualization (using URL format)\n",
    "            azure_ai_project=azure_ai_project_url,\n",
    "            # Save detailed results locally\n",
    "            output_path=output_path,\n",
    "            # Optional: provide evaluation name for easier tracking in portal\n",
    "            evaluation_name=f\"22-evaluate-models_{model_name}_{timestamp}\"\n",
    "        )\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Store results with both portal and local file information\n",
    "        evaluation_results[model_name] = {\n",
    "            \"metrics\": result[\"metrics\"],\n",
    "            \"evaluation_time\": elapsed_time,\n",
    "            \"studio_url\": result.get(\"studio_url\"),\n",
    "            \"output_path\": output_path\n",
    "        }\n",
    "        \n",
    "        print(f\"   ‚úÖ Completed in {elapsed_time:.2f} seconds\")\n",
    "        print(f\"   üìä Portal URL: {result.get('studio_url', 'N/A')}\")\n",
    "        print(f\"   üíæ Local results: {output_path}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error evaluating {model_name}: {str(e)}\\n\")\n",
    "        evaluation_results[model_name] = {\n",
    "            \"error\": str(e),\n",
    "            \"evaluation_time\": time.time() - start_time\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ All evaluations complete!\")\n",
    "print(f\"\\nüìÅ All results saved to: {output_dir}/\")\n",
    "print(f\"üåê View results in Azure AI Foundry portal using the URLs above\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc2431",
   "metadata": {},
   "source": [
    "## Step 9: Analyze Results\n",
    "\n",
    "Let's create a summary comparison of all models across key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "375046ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Eval Time (s)</th>\n",
       "      <th>Relevance</th>\n",
       "      <th>Coherence</th>\n",
       "      <th>Fluency</th>\n",
       "      <th>Violence</th>\n",
       "      <th>Hate/Unfairness</th>\n",
       "      <th>Self-Harm</th>\n",
       "      <th>Sexual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>47.382301</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>60.539374</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>40.326455</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model  Eval Time (s)  Relevance  Coherence  Fluency  Violence  \\\n",
       "0  gpt-4o-mini      47.382301        4.6        4.0      3.6       0.0   \n",
       "1       gpt-4o      60.539374        4.8        4.0      3.6       0.0   \n",
       "2      gpt-4.1      40.326455        4.4        4.2      3.6       0.0   \n",
       "\n",
       "   Hate/Unfairness  Self-Harm  Sexual  \n",
       "0              0.0        0.0     0.0  \n",
       "1              0.0        0.0     0.0  \n",
       "2              0.0        0.0     0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prepare data for comparison\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, results in evaluation_results.items():\n",
    "    if \"error\" in results:\n",
    "        print(f\"‚ö†Ô∏è  {model_name}: Evaluation failed - {results['error']}\\n\")\n",
    "        continue\n",
    "    \n",
    "    metrics = results[\"metrics\"]\n",
    "    \n",
    "    # The metrics are stored with keys like \"relevance.relevance\", \"coherence.coherence\", etc.\n",
    "    row = {\n",
    "        \"Model\": model_name,\n",
    "        \"Eval Time (s)\": results['evaluation_time'],\n",
    "        \"Relevance\": metrics.get('relevance.relevance', metrics.get('relevance', 0)),\n",
    "        \"Coherence\": metrics.get('coherence.coherence', metrics.get('coherence', 0)),\n",
    "        \"Fluency\": metrics.get('fluency.fluency', metrics.get('fluency', 0)),\n",
    "        \"Violence\": metrics.get('violence.violence_defect_rate', metrics.get('violence', 0)),\n",
    "        \"Hate/Unfairness\": metrics.get('hate_unfairness.hate_unfairness_defect_rate', metrics.get('hate_unfairness', 0)),\n",
    "        \"Self-Harm\": metrics.get('self_harm.self_harm_defect_rate', metrics.get('self_harm', 0)),\n",
    "        \"Sexual\": metrics.get('sexual.sexual_defect_rate', metrics.get('sexual', 0)),\n",
    "    }\n",
    "    \n",
    "    comparison_data.append(row)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"üìä Model Evaluation Comparison\\n\")\n",
    "display(df_comparison)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d088ee16",
   "metadata": {},
   "source": [
    "## Step 10: Performance Summary\n",
    "\n",
    "Analyze the evaluation results to identify the best performing models across different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86d089ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## üèÜ Best Performing Models by Metric"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Best Model</th>\n",
       "      <th>Score</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Relevance</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>4.800</td>\n",
       "      <td>‚Üë Higher is Better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coherence</td>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>4.200</td>\n",
       "      <td>‚Üë Higher is Better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fluency</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>3.600</td>\n",
       "      <td>‚Üë Higher is Better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Violence Safety</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.000</td>\n",
       "      <td>‚Üì Lower is Better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hate/Unfairness Safety</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.000</td>\n",
       "      <td>‚Üì Lower is Better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Self-Harm Safety</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.000</td>\n",
       "      <td>‚Üì Lower is Better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sexual Safety</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.000</td>\n",
       "      <td>‚Üì Lower is Better</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Metric   Best Model  Score           Direction\n",
       "0               Relevance       gpt-4o  4.800  ‚Üë Higher is Better\n",
       "1               Coherence      gpt-4.1  4.200  ‚Üë Higher is Better\n",
       "2                 Fluency  gpt-4o-mini  3.600  ‚Üë Higher is Better\n",
       "3         Violence Safety  gpt-4o-mini  0.000   ‚Üì Lower is Better\n",
       "4  Hate/Unfairness Safety  gpt-4o-mini  0.000   ‚Üì Lower is Better\n",
       "5        Self-Harm Safety  gpt-4o-mini  0.000   ‚Üì Lower is Better\n",
       "6           Sexual Safety  gpt-4o-mini  0.000   ‚Üì Lower is Better"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## üåü Overall Best Model (Quality Metrics Average)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ü•á **gpt-4o** - Quality: 4.133 | Time: 60.54s"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Model</th>\n",
       "      <th>Avg Quality Score</th>\n",
       "      <th>Eval Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ü•á 1</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>4.133</td>\n",
       "      <td>60.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ü•à 2</td>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>4.067</td>\n",
       "      <td>40.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ü•â 3</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>4.067</td>\n",
       "      <td>47.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rank        Model Avg Quality Score Eval Time (s)\n",
       "0  ü•á 1       gpt-4o             4.133         60.54\n",
       "1  ü•à 2      gpt-4.1             4.067         40.33\n",
       "2  ü•â 3  gpt-4o-mini             4.067         47.38"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, HTML\n",
    "import pandas as pd\n",
    "\n",
    "# Check for successful evaluations\n",
    "successful_models = {name: results for name, results in evaluation_results.items() if 'error' not in results}\n",
    "failed_models = {name: results for name, results in evaluation_results.items() if 'error' in results}\n",
    "\n",
    "if successful_models:\n",
    "    display(Markdown(\"## üèÜ Best Performing Models by Metric\"))\n",
    "    \n",
    "    # Define evaluator metrics and their optimization direction\n",
    "    evaluator_metrics = [\n",
    "        ('relevance.relevance', 'Relevance', True),\n",
    "        ('coherence.coherence', 'Coherence', True),\n",
    "        ('fluency.fluency', 'Fluency', True),\n",
    "        ('violence.violence_defect_rate', 'Violence Safety', False),\n",
    "        ('hate_unfairness.hate_unfairness_defect_rate', 'Hate/Unfairness Safety', False),\n",
    "        ('self_harm.self_harm_defect_rate', 'Self-Harm Safety', False),\n",
    "        ('sexual.sexual_defect_rate', 'Sexual Safety', False),\n",
    "    ]\n",
    "    \n",
    "    # Create a dataframe for best models\n",
    "    best_models_data = []\n",
    "    \n",
    "    for metric_key, display_name, higher_is_better in evaluator_metrics:\n",
    "        valid_models = {}\n",
    "        \n",
    "        # Collect scores for this metric from all successful models\n",
    "        for model_name, results in successful_models.items():\n",
    "            metrics = results['metrics']\n",
    "            score = metrics.get(metric_key)\n",
    "            if score is not None:\n",
    "                valid_models[model_name] = score\n",
    "        \n",
    "        if valid_models:\n",
    "            # Find best model based on optimization direction\n",
    "            if higher_is_better:\n",
    "                best_model_name = max(valid_models, key=valid_models.get)\n",
    "                best_score = valid_models[best_model_name]\n",
    "                direction = \"‚Üë Higher is Better\"\n",
    "            else:\n",
    "                best_model_name = min(valid_models, key=valid_models.get)\n",
    "                best_score = valid_models[best_model_name]\n",
    "                direction = \"‚Üì Lower is Better\"\n",
    "            \n",
    "            best_models_data.append({\n",
    "                \"Metric\": display_name,\n",
    "                \"Best Model\": best_model_name,\n",
    "                \"Score\": f\"{best_score:.3f}\",\n",
    "                \"Direction\": direction\n",
    "            })\n",
    "    \n",
    "    df_best = pd.DataFrame(best_models_data)\n",
    "    display(df_best)\n",
    "    \n",
    "    # Calculate overall best model (based on quality metrics average)\n",
    "    display(Markdown(\"---\"))\n",
    "    display(Markdown(\"## üåü Overall Best Model (Quality Metrics Average)\"))\n",
    "    \n",
    "    quality_metric_keys = ['relevance.relevance', 'coherence.coherence', 'fluency.fluency']\n",
    "    model_quality_scores = {}\n",
    "    \n",
    "    for model_name, results in successful_models.items():\n",
    "        metrics = results['metrics']\n",
    "        scores = []\n",
    "        for metric_key in quality_metric_keys:\n",
    "            score = metrics.get(metric_key)\n",
    "            if score is not None:\n",
    "                scores.append(score)\n",
    "        \n",
    "        if scores:\n",
    "            avg_score = sum(scores) / len(scores)\n",
    "            model_quality_scores[model_name] = {\n",
    "                'avg_quality': avg_score,\n",
    "                'eval_time': results['evaluation_time']\n",
    "            }\n",
    "    \n",
    "    if model_quality_scores:\n",
    "        best_overall = max(model_quality_scores, key=lambda x: model_quality_scores[x]['avg_quality'])\n",
    "        best_data = model_quality_scores[best_overall]\n",
    "        \n",
    "        display(Markdown(f\"ü•á **{best_overall}** - Quality: {best_data['avg_quality']:.3f} | Time: {best_data['eval_time']:.2f}s\"))\n",
    "        \n",
    "        # Show all model quality scores for comparison\n",
    "        display(Markdown(\"\"))\n",
    "        \n",
    "        ranking_data = []\n",
    "        sorted_models = sorted(model_quality_scores.items(), key=lambda x: x[1]['avg_quality'], reverse=True)\n",
    "        \n",
    "        for rank, (model_name, data) in enumerate(sorted_models, 1):\n",
    "            medal = \"ü•á\" if rank == 1 else \"ü•à\" if rank == 2 else \"ü•â\" if rank == 3 else \"\"\n",
    "            ranking_data.append({\n",
    "                \"Rank\": f\"{medal} {rank}\",\n",
    "                \"Model\": model_name,\n",
    "                \"Avg Quality Score\": f\"{data['avg_quality']:.3f}\",\n",
    "                \"Eval Time (s)\": f\"{data['eval_time']:.2f}\"\n",
    "            })\n",
    "        \n",
    "        df_ranking = pd.DataFrame(ranking_data)\n",
    "        display(df_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e5dd01",
   "metadata": {},
   "source": [
    "## Step 11: Next Steps\n",
    "\n",
    "You've successfully evaluated multiple models! Here are some next steps to consider:\n",
    "\n",
    "### üìä View Results in Two Places\n",
    "\n",
    "- **Azure AI Foundry Portal**: Interactive visualizations with [detailed charts and comparisons](https://learn.microsoft.com/azure/ai-foundry/how-to/evaluate-results)\n",
    "- **Portal URLs**: Each evaluation includes a studio URL for easy access and team sharing\n",
    "- **Local Files**: All results saved in `./22-evaluate-models-results/` for offline analysis\n",
    "- **Version Control**: Commit JSON files for reproducibility and tracking over time\n",
    "\n",
    "### üèÜ Use Model Leaderboards for Selection\n",
    "\n",
    "- **Browse Leaderboards**: Compare models by [Quality, Safety, Cost, and Performance](https://learn.microsoft.com/azure/ai-foundry/how-to/benchmark-model-in-catalog)\n",
    "- **Trade-off Analysis**: View quality vs. cost, quality vs. safety charts\n",
    "- **Scenario Filtering**: Find models best suited for your use case (Q&A, coding, reasoning)\n",
    "- **Access Portal**: [Azure AI Foundry Model Catalog ‚Üí Browse Leaderboards](https://aka.ms/model-leaderboards)\n",
    "\n",
    "---\n",
    "\n",
    "**Great work! You now have comprehensive evaluation metrics for multiple models.** üéâ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
